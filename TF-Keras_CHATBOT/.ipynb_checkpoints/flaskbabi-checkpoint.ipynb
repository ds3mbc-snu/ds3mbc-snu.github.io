{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 바로 아래를 먼저 실행시키고, 바로 아래의 주소를 클릭 !!\n",
    "\n",
    "# 여기 주소를 클릭  :  http://localhost:5000  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python flaskbabi.py \n",
    "\n",
    "\n",
    "# localhost:5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학기말 과제: QA 질문답변 등등 교육용 챗봇을 설계하자 !!\n",
    "\n",
    "<p> &nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI 융합수학교육 (수학교육 응용 + 데이터 수집 + 선처리 +  AI 코딩) \n",
    "\n",
    "\n",
    "<p> &nbsp;\n",
    "    \n",
    "    \n",
    "# 학기말 과제 힌트 : 0.ipynb 와 여기의 web_babi.ipynb 그리고 \n",
    "    \n",
    "# 4.ipynb 코드를 기반으로,  교육용 데이터가 중심인 챗봇 설계 !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras 코딩 : 손글씨 숫자 알아내기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18000 samples, validate on 42000 samples\n",
      "Epoch 1/5\n",
      "18000/18000 [==============================] - 3s 181us/step - loss: 1.2557 - accuracy: 0.6849 - val_loss: 0.7199 - val_accuracy: 0.8213\n",
      "Epoch 2/5\n",
      "18000/18000 [==============================] - 3s 188us/step - loss: 0.5625 - accuracy: 0.8538 - val_loss: 0.5100 - val_accuracy: 0.8621\n",
      "Epoch 3/5\n",
      "18000/18000 [==============================] - 3s 194us/step - loss: 0.4365 - accuracy: 0.8809 - val_loss: 0.4307 - val_accuracy: 0.8814\n",
      "Epoch 4/5\n",
      "18000/18000 [==============================] - 4s 197us/step - loss: 0.3812 - accuracy: 0.8936 - val_loss: 0.3899 - val_accuracy: 0.8899\n",
      "Epoch 5/5\n",
      "18000/18000 [==============================] - 3s 192us/step - loss: 0.3482 - accuracy: 0.9028 - val_loss: 0.3667 - val_accuracy: 0.8963\n",
      "10000/10000 [==============================] - 0s 44us/step\n",
      "\n",
      "loss_and_metrics : [0.33985268929600715, 0.90420001745224]\n"
     ]
    }
   ],
   "source": [
    "# 0. 사용할 패키지 불러오기\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "import numpy as np\n",
    "from numpy import argmax\n",
    "\n",
    "# 1. 데이터셋 생성하기\n",
    "\n",
    "# 훈련셋과 시험셋 불러오기\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# 데이터셋 전처리\n",
    "x_train = x_train.reshape(60000, 784).astype('float32') / 255.0\n",
    "x_test = x_test.reshape(10000, 784).astype('float32') / 255.0\n",
    "\n",
    "# 원핫인코딩 (one-hot encoding) 처리\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "# 훈련셋과 검증셋 분리\n",
    "x_val = x_train[:42000]   # 데이터셋의 70%를 훈련셋/학습셋으로 사용\n",
    "x_train = x_train[42000:] # 데이터셋의 30%를 검증셋으로 사용\n",
    "y_val = y_train[:42000]   # 데이터셋의 70%를 훈련셋/학습셋으로 사용\n",
    "y_train = y_train[42000:] # 데이터셋의 30%를 검증셋으로 사용\n",
    "\n",
    "# 2. 모델 구성하기\n",
    "model = Sequential()\n",
    "model.add(Dense(units=64, input_dim=28*28, activation='relu'))\n",
    "model.add(Dense(units=10, activation='softmax'))\n",
    "\n",
    "# 3. 모델 학습과정 설정하기\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "# 4. 모델 학습시키기\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=32, validation_data=(x_val, y_val))\n",
    "\n",
    "# 5. 모델 평가하기\n",
    "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=32)\n",
    "print('')\n",
    "print('loss_and_metrics : ' + str(loss_and_metrics))\n",
    "\n",
    "# 6. 모델 저장하기\n",
    "from keras.models import load_model\n",
    "model.save('./data_babi/mnist_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "[4 4 4 9 9]\n",
      "Train on 18000 samples, validate on 42000 samples\n",
      "Epoch 1/5\n",
      "18000/18000 [==============================] - 4s 231us/step - loss: 0.2602 - accuracy: 0.9263 - val_loss: 0.2959 - val_accuracy: 0.9174\n",
      "Epoch 2/5\n",
      "18000/18000 [==============================] - 4s 211us/step - loss: 0.2515 - accuracy: 0.9291 - val_loss: 0.2856 - val_accuracy: 0.9195\n",
      "Epoch 3/5\n",
      "18000/18000 [==============================] - 4s 237us/step - loss: 0.2435 - accuracy: 0.9322 - val_loss: 0.2832 - val_accuracy: 0.9203\n",
      "Epoch 4/5\n",
      "18000/18000 [==============================] - 4s 232us/step - loss: 0.2363 - accuracy: 0.9334 - val_loss: 0.2748 - val_accuracy: 0.9220\n",
      "Epoch 5/5\n",
      "18000/18000 [==============================] - 4s 228us/step - loss: 0.2293 - accuracy: 0.9357 - val_loss: 0.2695 - val_accuracy: 0.9242\n",
      "10000/10000 [==============================] - 0s 38us/step\n",
      "\n",
      "loss_and_metrics : [0.2469834449470043, 0.9290000200271606]\n",
      "True : 4, Predict : 4\n",
      "True : 4, Predict : 4\n",
      "True : 4, Predict : 4\n",
      "True : 9, Predict : 9\n",
      "True : 9, Predict : 9\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "import numpy as np\n",
    "from numpy import argmax\n",
    "\n",
    "\n",
    "# 2-0. 실무에 사용할 데이터 준비하기\n",
    "\n",
    "# 훈련셋과 시험셋 불러오기\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# 데이터셋 전처리\n",
    "x_train = x_train.reshape(60000, 784).astype('float32') / 255.0\n",
    "x_test = x_test.reshape(10000, 784).astype('float32') / 255.0\n",
    "\n",
    "# 원핫인코딩 (one-hot encoding) 처리\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "# 훈련셋과 검증셋 분리\n",
    "x_val = x_train[:42000]   # 데이터셋의 70%를 훈련셋/학습셋으로 사용\n",
    "x_train = x_train[42000:] # 데이터셋의 30%를 검증셋으로 사용\n",
    "y_val = y_train[:42000]   # 데이터셋의 70%를 훈련셋/학습셋으로 사용\n",
    "y_train = y_train[42000:] # 데이터셋의 30%를 검증셋으로 사용\n",
    "\n",
    "\n",
    "# 2-1. 모델 불러오기\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('./data_babi/mnist_model_15.h5')\n",
    "\n",
    "# 2-2. 모델 사용하기\n",
    "\n",
    "xhat_idx = np.random.choice(x_test.shape[0], 5)\n",
    "xhat = x_test[xhat_idx]\n",
    "zhat = y_test[xhat_idx]\n",
    "yhat = model.predict_classes(xhat)\n",
    "print( zhat )\n",
    "print( yhat )\n",
    "\n",
    "\n",
    "# 2-3. 모델 학습과정 설정하기\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "# 2-4. 모델 학습시키기\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=32, validation_data=(x_val, y_val))\n",
    "\n",
    "# 2-5. 모델 평가하기\n",
    "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=32)\n",
    "print('')\n",
    "print('loss_and_metrics : ' + str(loss_and_metrics))\n",
    "\n",
    "# 2-6. 모델 저장하기\n",
    "from keras.models import load_model\n",
    "model.save('./data_babi/mnist_model.h5')\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    print('True : ' + str(argmax(y_test[xhat_idx[i]])) + ', Predict : ' + str(yhat[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==============================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras 코딩 : Babi 프로젝트 20개 문제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Trains a memory network on the facebook bAbI dataset for Question/Answering System.\n",
    "\n",
    "'''\n",
    "\n",
    "# import the packages\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Input, Activation, Dense, Permute, Dropout\n",
    "from keras.layers import add, dot, concatenate\n",
    "from keras.layers import LSTM, GRU\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import backend as K\n",
    "\n",
    "from functools import reduce\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting stories for the challenge: single_supporting_fact_10k\n",
      "-\n",
      "Vocab size: 22 unique words\n",
      "Story max length: 68 words\n",
      "Query max length: 4 words\n",
      "Number of training stories: 10000\n",
      "Number of test stories: 1000\n",
      "-\n",
      "Here's what a \"story\" tuple looks like (input, query, answer):\n",
      "(['Mary', 'moved', 'to', 'the', 'bathroom', '.', 'John', 'went', 'to', 'the', 'hallway', '.'], ['Where', 'is', 'Mary', '?'], 'bathroom')\n",
      "-\n",
      "Vectorizing the word sequences...\n",
      "-\n",
      "inputs: integer tensor of shape (samples, max_length)\n",
      "inputs_train shape: (10000, 68)\n",
      "inputs_test shape: (1000, 68)\n",
      "-\n",
      "queries: integer tensor of shape (samples, max_length)\n",
      "queries_train shape: (10000, 4)\n",
      "queries_test shape: (1000, 4)\n",
      "-\n",
      "answers: binary (1 or 0) tensor of shape (samples, vocab_size)\n",
      "answers_train shape: (10000, 22)\n",
      "answers_test shape: (1000, 22)\n",
      "-\n",
      "Compiling...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def tokenize(sent):\n",
    "    '''Return the tokens of a sentence including punctuation.\n",
    "    >>> tokenize('Bob dropped the apple. Where is the apple?')\n",
    "    ['Bob', 'dropped', 'the', 'apple', '.', 'Where', 'is', 'the', 'apple', '?']\n",
    "    '''\n",
    "    return [x.strip() for x in re.split('(\\W+)+', sent) if x.strip()]\n",
    "\n",
    "\n",
    "def parse_stories(lines, only_supporting=False):\n",
    "    '''Parse stories provided in the bAbi tasks format\n",
    "    If only_supporting is true, only the sentences\n",
    "    that support the answer are kept.\n",
    "    '''\n",
    "    data = []\n",
    "    story = []\n",
    "    for line in lines:\n",
    "        line = line.decode('utf-8').strip()\n",
    "        nid, line = line.split(' ', 1)\n",
    "        nid = int(nid)\n",
    "        if nid == 1:\n",
    "            story = []\n",
    "        if '\\t' in line:\n",
    "            q, a, supporting = line.split('\\t')\n",
    "            q = tokenize(q)\n",
    "            substory = None\n",
    "            if only_supporting:\n",
    "                # Only select the related substory\n",
    "                supporting = map(int, supporting.split())\n",
    "                substory = [story[i - 1] for i in supporting]\n",
    "            else:\n",
    "                # Provide all the substories\n",
    "                substory = [x for x in story if x]\n",
    "            data.append((substory, q, a))\n",
    "            story.append('')\n",
    "        else:\n",
    "            sent = tokenize(line)\n",
    "            story.append(sent)\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_stories(f, only_supporting=False, max_length=None):\n",
    "    '''Given a file name, read the file,\n",
    "    retrieve the stories,\n",
    "    and then convert the sentences into a single story.\n",
    "    If max_length is supplied,\n",
    "    any stories longer than max_length tokens will be discarded.\n",
    "    '''\n",
    "    data = parse_stories(f.readlines(), only_supporting=only_supporting)\n",
    "    flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
    "    data = [(flatten(story), q, answer) for story, q, answer in data if not max_length or len(flatten(story)) < max_length]\n",
    "    return data\n",
    "\n",
    "\n",
    "def vectorize_stories(data, word_idx, story_maxlen, query_maxlen):\n",
    "    X = []\n",
    "    Xq = []\n",
    "    Y = []\n",
    "    for story, query, answer in data:\n",
    "        x = [word_idx[w] for w in story]\n",
    "        xq = [word_idx[w] for w in query]\n",
    "        # let's not forget that index 0 is reserved\n",
    "        y = np.zeros(len(word_idx) + 1)\n",
    "        y[word_idx[answer]] = 1\n",
    "        X.append(x)\n",
    "        Xq.append(xq)\n",
    "        Y.append(y)\n",
    "    return (pad_sequences(X, maxlen=story_maxlen),\n",
    "            pad_sequences(Xq, maxlen=query_maxlen), np.array(Y))\n",
    "\n",
    "try:\n",
    "    path = get_file('babi-tasks-v1-2.tar.gz', origin='https://s3.amazonaws.com/text-datasets/babi_tasks_1-20_v1-2.tar.gz')\n",
    "except:\n",
    "    print('Error downloading dataset, please download it manually:\\n'\n",
    "          '$ wget http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2.tar.gz\\n'\n",
    "          '$ mv tasks_1-20_v1-2.tar.gz ~/.keras/datasets/babi-tasks-v1-2.tar.gz')\n",
    "    raise\n",
    "tar = tarfile.open(path)\n",
    "\n",
    "challenges = {\n",
    "    # QA1 with 10,000 samples\n",
    "    'single_supporting_fact_10k': 'tasks_1-20_v1-2/en-10k/qa1_single-supporting-fact_{}.txt',\n",
    "    # QA2 with 10,000 samples\n",
    "    'two_supporting_facts_10k': 'tasks_1-20_v1-2/en-10k/qa2_two-supporting-facts_{}.txt',\n",
    "}\n",
    "challenge_type = 'single_supporting_fact_10k'\n",
    "challenge = challenges[challenge_type]\n",
    "\n",
    "print('Extracting stories for the challenge:', challenge_type)\n",
    "train_stories = get_stories(tar.extractfile(challenge.format('train')))\n",
    "test_stories = get_stories(tar.extractfile(challenge.format('test')))\n",
    "\n",
    "vocab = set()\n",
    "for story, q, answer in train_stories + test_stories:\n",
    "    vocab |= set(story + q + [answer])\n",
    "vocab = sorted(vocab)\n",
    "\n",
    "# Reserve 0 for masking via pad_sequences\n",
    "vocab_size = len(vocab) + 1\n",
    "story_maxlen = max(map(len, (x for x, _, _ in train_stories + test_stories)))\n",
    "query_maxlen = max(map(len, (x for _, x, _ in train_stories + test_stories)))\n",
    "\n",
    "print('-')\n",
    "print('Vocab size:', vocab_size, 'unique words')\n",
    "print('Story max length:', story_maxlen, 'words')\n",
    "print('Query max length:', query_maxlen, 'words')\n",
    "print('Number of training stories:', len(train_stories))\n",
    "print('Number of test stories:', len(test_stories))\n",
    "print('-')\n",
    "print('Here\\'s what a \"story\" tuple looks like (input, query, answer):')\n",
    "print(train_stories[0])\n",
    "print('-')\n",
    "print('Vectorizing the word sequences...')\n",
    "\n",
    "word_idx = dict((c, i + 1) for i, c in enumerate(vocab))\n",
    "idx_word = dict((i+1, c) for i,c in enumerate(vocab))\n",
    "inputs_train, queries_train, answers_train = vectorize_stories(train_stories,\n",
    "                                                               word_idx,\n",
    "                                                               story_maxlen,\n",
    "                                                               query_maxlen)\n",
    "inputs_test, queries_test, answers_test = vectorize_stories(test_stories,\n",
    "                                                            word_idx,\n",
    "                                                            story_maxlen,\n",
    "                                                            query_maxlen)\n",
    "\n",
    "print('-')\n",
    "print('inputs: integer tensor of shape (samples, max_length)')\n",
    "print('inputs_train shape:', inputs_train.shape)\n",
    "print('inputs_test shape:', inputs_test.shape)\n",
    "print('-')\n",
    "print('queries: integer tensor of shape (samples, max_length)')\n",
    "print('queries_train shape:', queries_train.shape)\n",
    "print('queries_test shape:', queries_test.shape)\n",
    "print('-')\n",
    "print('answers: binary (1 or 0) tensor of shape (samples, vocab_size)')\n",
    "print('answers_train shape:', answers_train.shape)\n",
    "print('answers_test shape:', answers_test.shape)\n",
    "print('-')\n",
    "print('Compiling...')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence: Tensor(\"input_3:0\", shape=(None, 68), dtype=float32)\n",
      "Question: Tensor(\"input_4:0\", shape=(None, 4), dtype=float32)\n",
      "Input encoded m Tensor(\"sequential_6/dropout_5/cond/Identity:0\", shape=(None, 68, 64), dtype=float32)\n",
      "Input encoded c Tensor(\"sequential_7/dropout_6/cond/Identity:0\", shape=(None, 68, 4), dtype=float32)\n",
      "Question encoded Tensor(\"sequential_8/dropout_7/cond/Identity:0\", shape=(None, 4, 64), dtype=float32)\n",
      "(None, 68, 4)\n",
      "Match shape Tensor(\"activation_3/truediv:0\", shape=(None, 68, 4), dtype=float32)\n",
      "Response shape Tensor(\"permute_2/transpose:0\", shape=(None, 4, 68), dtype=float32)\n",
      "Answer shape Tensor(\"concatenate_2/concat:0\", shape=(None, 4, 132), dtype=float32)\n",
      "-------------Model Summary------------\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 68)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 4)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_6 (Sequential)       multiple             1408        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_8 (Sequential)       (None, 4, 64)        1408        input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_2 (Dot)                     (None, 68, 4)        0           sequential_6[1][0]               \n",
      "                                                                 sequential_8[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 68, 4)        0           dot_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "sequential_7 (Sequential)       multiple             88          input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 68, 4)        0           activation_3[0][0]               \n",
      "                                                                 sequential_7[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "permute_2 (Permute)             (None, 4, 68)        0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 4, 132)       0           permute_2[0][0]                  \n",
      "                                                                 sequential_8[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 64)           50432       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 64)           0           lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 22)           1430        dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 22)           0           dense_6[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 54,766\n",
      "Trainable params: 54,766\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"1180pt\" viewBox=\"0.00 0.00 783.50 885.00\" width=\"1045pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1.33333 1.33333) rotate(0) translate(4 881)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-881 779.5,-881 779.5,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 2474150934232 -->\n",
       "<g class=\"node\" id=\"node1\"><title>2474150934232</title>\n",
       "<polygon fill=\"none\" points=\"109,-830.5 109,-876.5 368,-876.5 368,-830.5 109,-830.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"172\" y=\"-849.8\">input_3: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"235,-830.5 235,-876.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"263\" y=\"-861.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"235,-853.5 291,-853.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"263\" y=\"-838.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"291,-830.5 291,-876.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"329.5\" y=\"-861.3\">(None, 68)</text>\n",
       "<polyline fill=\"none\" points=\"291,-853.5 368,-853.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"329.5\" y=\"-838.3\">(None, 68)</text>\n",
       "</g>\n",
       "<!-- 2473991970768 -->\n",
       "<g class=\"node\" id=\"node3\"><title>2473991970768</title>\n",
       "<polygon fill=\"none\" points=\"191,-747.5 191,-793.5 456,-793.5 456,-747.5 191,-747.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"265.5\" y=\"-766.8\">sequential_6: Sequential</text>\n",
       "<polyline fill=\"none\" points=\"340,-747.5 340,-793.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"368\" y=\"-778.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"340,-770.5 396,-770.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"368\" y=\"-755.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"396,-747.5 396,-793.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"426\" y=\"-778.3\">multiple</text>\n",
       "<polyline fill=\"none\" points=\"396,-770.5 456,-770.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"426\" y=\"-755.3\">multiple</text>\n",
       "</g>\n",
       "<!-- 2474150934232&#45;&gt;2473991970768 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>2474150934232-&gt;2473991970768</title>\n",
       "<path d=\"M261.727,-830.366C271.378,-821.169 282.711,-810.369 293.017,-800.548\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"295.477,-803.039 300.301,-793.607 290.648,-797.972 295.477,-803.039\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2475896430944 -->\n",
       "<g class=\"node\" id=\"node7\"><title>2475896430944</title>\n",
       "<polygon fill=\"none\" points=\"0,-664.5 0,-710.5 265,-710.5 265,-664.5 0,-664.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"74.5\" y=\"-683.8\">sequential_7: Sequential</text>\n",
       "<polyline fill=\"none\" points=\"149,-664.5 149,-710.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"177\" y=\"-695.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"149,-687.5 205,-687.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"177\" y=\"-672.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"205,-664.5 205,-710.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"235\" y=\"-695.3\">multiple</text>\n",
       "<polyline fill=\"none\" points=\"205,-687.5 265,-687.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"235\" y=\"-672.3\">multiple</text>\n",
       "</g>\n",
       "<!-- 2474150934232&#45;&gt;2475896430944 -->\n",
       "<g class=\"edge\" id=\"edge6\"><title>2474150934232-&gt;2475896430944</title>\n",
       "<path d=\"M213.527,-830.481C202.767,-820.136 190.595,-807.192 181.5,-794 165.595,-770.931 152.537,-741.877 143.912,-720.103\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"147.132,-718.727 140.266,-710.658 140.602,-721.248 147.132,-718.727\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2474150932608 -->\n",
       "<g class=\"node\" id=\"node2\"><title>2474150932608</title>\n",
       "<polygon fill=\"none\" points=\"501.5,-830.5 501.5,-876.5 753.5,-876.5 753.5,-830.5 501.5,-830.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"564.5\" y=\"-849.8\">input_4: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"627.5,-830.5 627.5,-876.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"655.5\" y=\"-861.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"627.5,-853.5 683.5,-853.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"655.5\" y=\"-838.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"683.5,-830.5 683.5,-876.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"718.5\" y=\"-861.3\">(None, 4)</text>\n",
       "<polyline fill=\"none\" points=\"683.5,-853.5 753.5,-853.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"718.5\" y=\"-838.3\">(None, 4)</text>\n",
       "</g>\n",
       "<!-- 2474001616064 -->\n",
       "<g class=\"node\" id=\"node4\"><title>2474001616064</title>\n",
       "<polygon fill=\"none\" points=\"479.5,-747.5 479.5,-793.5 775.5,-793.5 775.5,-747.5 479.5,-747.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"554\" y=\"-766.8\">sequential_8: Sequential</text>\n",
       "<polyline fill=\"none\" points=\"628.5,-747.5 628.5,-793.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"656.5\" y=\"-778.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"628.5,-770.5 684.5,-770.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"656.5\" y=\"-755.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"684.5,-747.5 684.5,-793.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"730\" y=\"-778.3\">(None, 4)</text>\n",
       "<polyline fill=\"none\" points=\"684.5,-770.5 775.5,-770.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"730\" y=\"-755.3\">(None, 4, 64)</text>\n",
       "</g>\n",
       "<!-- 2474150932608&#45;&gt;2474001616064 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>2474150932608-&gt;2474001616064</title>\n",
       "<path d=\"M627.5,-830.366C627.5,-822.152 627.5,-812.658 627.5,-803.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"631,-803.607 627.5,-793.607 624,-803.607 631,-803.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2474111867144 -->\n",
       "<g class=\"node\" id=\"node5\"><title>2474111867144</title>\n",
       "<polygon fill=\"none\" points=\"283,-664.5 283,-710.5 604,-710.5 604,-664.5 283,-664.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"321.5\" y=\"-683.8\">dot_2: Dot</text>\n",
       "<polyline fill=\"none\" points=\"360,-664.5 360,-710.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"388\" y=\"-695.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"360,-687.5 416,-687.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"388\" y=\"-672.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"416,-664.5 416,-710.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"510\" y=\"-695.3\">[(None, 68, 64), (None, 4, 64)]</text>\n",
       "<polyline fill=\"none\" points=\"416,-687.5 604,-687.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"510\" y=\"-672.3\">(None, 68, 4)</text>\n",
       "</g>\n",
       "<!-- 2473991970768&#45;&gt;2474111867144 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>2473991970768-&gt;2474111867144</title>\n",
       "<path d=\"M356.291,-747.366C370.445,-737.812 387.162,-726.528 402.154,-716.409\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"404.419,-719.103 410.749,-710.607 400.502,-713.301 404.419,-719.103\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2474001616064&#45;&gt;2474111867144 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>2474001616064-&gt;2474111867144</title>\n",
       "<path d=\"M577.221,-747.366C554.301,-737.277 526.998,-725.257 503.056,-714.718\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"504.281,-711.433 493.718,-710.607 501.46,-717.839 504.281,-711.433\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2474002732424 -->\n",
       "<g class=\"node\" id=\"node10\"><title>2474002732424</title>\n",
       "<polygon fill=\"none\" points=\"338,-332.5 338,-378.5 749,-378.5 749,-332.5 338,-332.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"424.5\" y=\"-351.8\">concatenate_2: Concatenate</text>\n",
       "<polyline fill=\"none\" points=\"511,-332.5 511,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"539\" y=\"-363.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"511,-355.5 567,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"539\" y=\"-340.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"567,-332.5 567,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"658\" y=\"-363.3\">[(None, 4, 68), (None, 4, 64)]</text>\n",
       "<polyline fill=\"none\" points=\"567,-355.5 749,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"658\" y=\"-340.3\">(None, 4, 132)</text>\n",
       "</g>\n",
       "<!-- 2474001616064&#45;&gt;2474002732424 -->\n",
       "<g class=\"edge\" id=\"edge11\"><title>2474001616064-&gt;2474002732424</title>\n",
       "<path d=\"M628.597,-747.489C630.069,-716.019 632.5,-656.347 632.5,-605.5 632.5,-605.5 632.5,-605.5 632.5,-520.5 632.5,-471.752 627.284,-456.977 602.5,-415 596.198,-404.326 587.478,-394.225 578.674,-385.517\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"580.979,-382.879 571.31,-378.547 576.167,-387.963 580.979,-382.879\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2474002981832 -->\n",
       "<g class=\"node\" id=\"node6\"><title>2474002981832</title>\n",
       "<polygon fill=\"none\" points=\"296,-581.5 296,-627.5 591,-627.5 591,-581.5 296,-581.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"370\" y=\"-600.8\">activation_3: Activation</text>\n",
       "<polyline fill=\"none\" points=\"444,-581.5 444,-627.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"472\" y=\"-612.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"444,-604.5 500,-604.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"472\" y=\"-589.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"500,-581.5 500,-627.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"545.5\" y=\"-612.3\">(None, 68, 4)</text>\n",
       "<polyline fill=\"none\" points=\"500,-604.5 591,-604.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"545.5\" y=\"-589.3\">(None, 68, 4)</text>\n",
       "</g>\n",
       "<!-- 2474111867144&#45;&gt;2474002981832 -->\n",
       "<g class=\"edge\" id=\"edge5\"><title>2474111867144-&gt;2474002981832</title>\n",
       "<path d=\"M443.5,-664.366C443.5,-656.152 443.5,-646.658 443.5,-637.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"447,-637.607 443.5,-627.607 440,-637.607 447,-637.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2473985593072 -->\n",
       "<g class=\"node\" id=\"node8\"><title>2473985593072</title>\n",
       "<polygon fill=\"none\" points=\"283,-498.5 283,-544.5 604,-544.5 604,-498.5 283,-498.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"324.5\" y=\"-517.8\">add_2: Add</text>\n",
       "<polyline fill=\"none\" points=\"366,-498.5 366,-544.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"394\" y=\"-529.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"366,-521.5 422,-521.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"394\" y=\"-506.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"422,-498.5 422,-544.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"513\" y=\"-529.3\">[(None, 68, 4), (None, 68, 4)]</text>\n",
       "<polyline fill=\"none\" points=\"422,-521.5 604,-521.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"513\" y=\"-506.3\">(None, 68, 4)</text>\n",
       "</g>\n",
       "<!-- 2474002981832&#45;&gt;2473985593072 -->\n",
       "<g class=\"edge\" id=\"edge7\"><title>2474002981832-&gt;2473985593072</title>\n",
       "<path d=\"M443.5,-581.366C443.5,-573.152 443.5,-563.658 443.5,-554.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"447,-554.607 443.5,-544.607 440,-554.607 447,-554.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2475896430944&#45;&gt;2473985593072 -->\n",
       "<g class=\"edge\" id=\"edge8\"><title>2475896430944-&gt;2473985593072</title>\n",
       "<path d=\"M160.141,-664.479C189.999,-641.449 239.611,-605.331 286.5,-581 310.768,-568.408 338.379,-557.176 363.608,-548.003\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"364.912,-551.254 373.149,-544.59 362.554,-544.663 364.912,-551.254\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2474001315152 -->\n",
       "<g class=\"node\" id=\"node9\"><title>2474001315152</title>\n",
       "<polygon fill=\"none\" points=\"318,-415.5 318,-461.5 593,-461.5 593,-415.5 318,-415.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"382\" y=\"-434.8\">permute_2: Permute</text>\n",
       "<polyline fill=\"none\" points=\"446,-415.5 446,-461.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"474\" y=\"-446.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"446,-438.5 502,-438.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"474\" y=\"-423.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"502,-415.5 502,-461.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"547.5\" y=\"-446.3\">(None, 68, 4)</text>\n",
       "<polyline fill=\"none\" points=\"502,-438.5 593,-438.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"547.5\" y=\"-423.3\">(None, 4, 68)</text>\n",
       "</g>\n",
       "<!-- 2473985593072&#45;&gt;2474001315152 -->\n",
       "<g class=\"edge\" id=\"edge9\"><title>2473985593072-&gt;2474001315152</title>\n",
       "<path d=\"M446.779,-498.366C447.996,-490.152 449.403,-480.658 450.726,-471.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"454.222,-472.012 452.225,-461.607 447.297,-470.986 454.222,-472.012\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2474001315152&#45;&gt;2474002732424 -->\n",
       "<g class=\"edge\" id=\"edge10\"><title>2474001315152-&gt;2474002732424</title>\n",
       "<path d=\"M479.546,-415.366C489.538,-406.169 501.272,-395.369 511.941,-385.548\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"514.495,-387.954 519.483,-378.607 509.755,-382.804 514.495,-387.954\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2474002734496 -->\n",
       "<g class=\"node\" id=\"node11\"><title>2474002734496</title>\n",
       "<polygon fill=\"none\" points=\"417.5,-249.5 417.5,-295.5 669.5,-295.5 669.5,-249.5 417.5,-249.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"466.5\" y=\"-268.8\">lstm_2: LSTM</text>\n",
       "<polyline fill=\"none\" points=\"515.5,-249.5 515.5,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"543.5\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"515.5,-272.5 571.5,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"543.5\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"571.5,-249.5 571.5,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"620.5\" y=\"-280.3\">(None, 4, 132)</text>\n",
       "<polyline fill=\"none\" points=\"571.5,-272.5 669.5,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"620.5\" y=\"-257.3\">(None, 64)</text>\n",
       "</g>\n",
       "<!-- 2474002732424&#45;&gt;2474002734496 -->\n",
       "<g class=\"edge\" id=\"edge12\"><title>2474002732424-&gt;2474002734496</title>\n",
       "<path d=\"M543.5,-332.366C543.5,-324.152 543.5,-314.658 543.5,-305.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"547,-305.607 543.5,-295.607 540,-305.607 547,-305.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2474002734216 -->\n",
       "<g class=\"node\" id=\"node12\"><title>2474002734216</title>\n",
       "<polygon fill=\"none\" points=\"412,-166.5 412,-212.5 675,-212.5 675,-166.5 412,-166.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"477\" y=\"-185.8\">dropout_8: Dropout</text>\n",
       "<polyline fill=\"none\" points=\"542,-166.5 542,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"570\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"542,-189.5 598,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"570\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"598,-166.5 598,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"636.5\" y=\"-197.3\">(None, 64)</text>\n",
       "<polyline fill=\"none\" points=\"598,-189.5 675,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"636.5\" y=\"-174.3\">(None, 64)</text>\n",
       "</g>\n",
       "<!-- 2474002734496&#45;&gt;2474002734216 -->\n",
       "<g class=\"edge\" id=\"edge13\"><title>2474002734496-&gt;2474002734216</title>\n",
       "<path d=\"M543.5,-249.366C543.5,-241.152 543.5,-231.658 543.5,-222.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"547,-222.607 543.5,-212.607 540,-222.607 547,-222.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2474002734944 -->\n",
       "<g class=\"node\" id=\"node13\"><title>2474002734944</title>\n",
       "<polygon fill=\"none\" points=\"425,-83.5 425,-129.5 662,-129.5 662,-83.5 425,-83.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"477\" y=\"-102.8\">dense_6: Dense</text>\n",
       "<polyline fill=\"none\" points=\"529,-83.5 529,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"557\" y=\"-114.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"529,-106.5 585,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"557\" y=\"-91.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"585,-83.5 585,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"623.5\" y=\"-114.3\">(None, 64)</text>\n",
       "<polyline fill=\"none\" points=\"585,-106.5 662,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"623.5\" y=\"-91.3\">(None, 22)</text>\n",
       "</g>\n",
       "<!-- 2474002734216&#45;&gt;2474002734944 -->\n",
       "<g class=\"edge\" id=\"edge14\"><title>2474002734216-&gt;2474002734944</title>\n",
       "<path d=\"M543.5,-166.366C543.5,-158.152 543.5,-148.658 543.5,-139.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"547,-139.607 543.5,-129.607 540,-139.607 547,-139.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2474002733712 -->\n",
       "<g class=\"node\" id=\"node14\"><title>2474002733712</title>\n",
       "<polygon fill=\"none\" points=\"403,-0.5 403,-46.5 684,-46.5 684,-0.5 403,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"477\" y=\"-19.8\">activation_4: Activation</text>\n",
       "<polyline fill=\"none\" points=\"551,-0.5 551,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"579\" y=\"-31.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"551,-23.5 607,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"579\" y=\"-8.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"607,-0.5 607,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"645.5\" y=\"-31.3\">(None, 22)</text>\n",
       "<polyline fill=\"none\" points=\"607,-23.5 684,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"645.5\" y=\"-8.3\">(None, 22)</text>\n",
       "</g>\n",
       "<!-- 2474002734944&#45;&gt;2474002733712 -->\n",
       "<g class=\"edge\" id=\"edge15\"><title>2474002734944-&gt;2474002733712</title>\n",
       "<path d=\"M543.5,-83.3664C543.5,-75.1516 543.5,-65.6579 543.5,-56.7252\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"547,-56.6068 543.5,-46.6068 540,-56.6069 547,-56.6068\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# placeholders\n",
    "\n",
    "input_sequence = Input((story_maxlen,))\n",
    "question = Input((query_maxlen,))\n",
    "\n",
    "print('Input sequence:', input_sequence)\n",
    "print('Question:', question)\n",
    "\n",
    "# encoders\n",
    "# embed the input sequence into a sequence of vectors\n",
    "input_encoder_m = Sequential()\n",
    "input_encoder_m.add(Embedding(input_dim=vocab_size,\n",
    "                              output_dim=64))\n",
    "input_encoder_m.add(Dropout(0.3))\n",
    "# output: (samples, story_maxlen, embedding_dim)\n",
    "\n",
    "# embed the input into a sequence of vectors of size query_maxlen\n",
    "input_encoder_c = Sequential()\n",
    "input_encoder_c.add(Embedding(input_dim=vocab_size,\n",
    "                              output_dim=query_maxlen))\n",
    "input_encoder_c.add(Dropout(0.3))\n",
    "# output: (samples, story_maxlen, query_maxlen)\n",
    "\n",
    "# embed the question into a sequence of vectors\n",
    "question_encoder = Sequential()\n",
    "question_encoder.add(Embedding(input_dim=vocab_size,\n",
    "                               output_dim=64,\n",
    "                               input_length=query_maxlen))\n",
    "question_encoder.add(Dropout(0.3))\n",
    "# output: (samples, query_maxlen, embedding_dim)\n",
    "\n",
    "# encode input sequence and questions (which are indices)\n",
    "# to sequences of dense vectors\n",
    "input_encoded_m = input_encoder_m(input_sequence)\n",
    "print('Input encoded m', input_encoded_m)\n",
    "input_encoded_c = input_encoder_c(input_sequence)\n",
    "print('Input encoded c', input_encoded_c)\n",
    "question_encoded = question_encoder(question)\n",
    "print('Question encoded', question_encoded)\n",
    "\n",
    "\n",
    "# compute a 'match' between the first input vector sequence\n",
    "# and the question vector sequence\n",
    "# shape: `(samples, story_maxlen, query_maxlen)\n",
    "match = dot([input_encoded_m, question_encoded], axes=(2, 2))\n",
    "print(match.shape)\n",
    "match = Activation('softmax')(match)\n",
    "print('Match shape', match)\n",
    "\n",
    "# add the match matrix with the second input vector sequence\n",
    "response = add([match, input_encoded_c])  # (samples, story_maxlen, query_maxlen)\n",
    "response = Permute((2, 1))(response)  # (samples, query_maxlen, story_maxlen)\n",
    "print('Response shape', response)\n",
    "\n",
    "# concatenate the response vector with the question vector sequence\n",
    "answer = concatenate([response, question_encoded])\n",
    "print('Answer shape', answer)\n",
    "\n",
    "#answer = LSTM(lstm_size, return_sequences=True)(answer)  # Generate tensors of shape 32\n",
    "#answer = Dropout(0.3)(answer)\n",
    "answer = LSTM(lstm_size)(answer)  # Generate tensors of shape 32\n",
    "answer = Dropout(0.3)(answer)\n",
    "answer = Dense(vocab_size)(answer)  # (samples, vocab_size)\n",
    "# we output a probability distribution over the vocabulary\n",
    "answer = Activation('softmax')(answer)\n",
    "# build the final model\n",
    "model = Model([input_sequence, question], answer)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(\"-------------Model Summary------------\")\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig the model\n",
      "Train on 10000 samples, validate on 1000 samples\n",
      "Epoch 1/100\n",
      "10000/10000 [==============================] - 3s 349us/step - loss: 0.0251 - accuracy: 0.9928 - val_loss: 0.0788 - val_accuracy: 0.9810\n",
      "Epoch 2/100\n",
      "10000/10000 [==============================] - 3s 325us/step - loss: 0.0358 - accuracy: 0.9898 - val_loss: 0.0585 - val_accuracy: 0.9820\n",
      "Epoch 3/100\n",
      "10000/10000 [==============================] - 3s 327us/step - loss: 0.0243 - accuracy: 0.9922 - val_loss: 0.0759 - val_accuracy: 0.9800\n",
      "Epoch 4/100\n",
      "10000/10000 [==============================] - 3s 337us/step - loss: 0.0276 - accuracy: 0.9926 - val_loss: 0.0593 - val_accuracy: 0.9820\n",
      "Epoch 5/100\n",
      "10000/10000 [==============================] - 3s 321us/step - loss: 0.0219 - accuracy: 0.9937 - val_loss: 0.0601 - val_accuracy: 0.9820\n",
      "Epoch 6/100\n",
      "10000/10000 [==============================] - 3s 318us/step - loss: 0.0162 - accuracy: 0.9949 - val_loss: 0.0454 - val_accuracy: 0.9880\n",
      "Epoch 7/100\n",
      "10000/10000 [==============================] - 3s 321us/step - loss: 0.0254 - accuracy: 0.9914 - val_loss: 0.0501 - val_accuracy: 0.9860\n",
      "Epoch 8/100\n",
      "10000/10000 [==============================] - 3s 328us/step - loss: 0.0315 - accuracy: 0.9911 - val_loss: 0.0566 - val_accuracy: 0.9830\n",
      "Epoch 9/100\n",
      "10000/10000 [==============================] - 3s 329us/step - loss: 0.0273 - accuracy: 0.9924 - val_loss: 0.0637 - val_accuracy: 0.9820\n",
      "Epoch 10/100\n",
      "10000/10000 [==============================] - 3s 324us/step - loss: 0.0219 - accuracy: 0.9929 - val_loss: 0.0450 - val_accuracy: 0.9870\n",
      "Epoch 11/100\n",
      "10000/10000 [==============================] - 3s 326us/step - loss: 0.0242 - accuracy: 0.9926 - val_loss: 0.0701 - val_accuracy: 0.9830\n",
      "Epoch 12/100\n",
      "10000/10000 [==============================] - 3s 333us/step - loss: 0.0220 - accuracy: 0.9928 - val_loss: 0.0430 - val_accuracy: 0.9900\n",
      "Epoch 13/100\n",
      "10000/10000 [==============================] - 3s 325us/step - loss: 0.0285 - accuracy: 0.9907 - val_loss: 0.0691 - val_accuracy: 0.9820\n",
      "Epoch 14/100\n",
      "10000/10000 [==============================] - 3s 322us/step - loss: 0.0171 - accuracy: 0.9957 - val_loss: 0.0499 - val_accuracy: 0.9860\n",
      "Epoch 15/100\n",
      "10000/10000 [==============================] - 3s 330us/step - loss: 0.0250 - accuracy: 0.9923 - val_loss: 0.0624 - val_accuracy: 0.9840\n",
      "Epoch 16/100\n",
      "10000/10000 [==============================] - 3s 323us/step - loss: 0.0241 - accuracy: 0.9923 - val_loss: 0.0565 - val_accuracy: 0.9820\n",
      "Epoch 17/100\n",
      "10000/10000 [==============================] - 3s 320us/step - loss: 0.0181 - accuracy: 0.9952 - val_loss: 0.0484 - val_accuracy: 0.9890\n",
      "Epoch 18/100\n",
      "10000/10000 [==============================] - 3s 324us/step - loss: 0.0233 - accuracy: 0.9932 - val_loss: 0.0463 - val_accuracy: 0.9880\n",
      "Epoch 19/100\n",
      "10000/10000 [==============================] - 3s 329us/step - loss: 0.0233 - accuracy: 0.9917 - val_loss: 0.0628 - val_accuracy: 0.9840\n",
      "Epoch 20/100\n",
      "10000/10000 [==============================] - 3s 326us/step - loss: 0.0200 - accuracy: 0.9931 - val_loss: 0.0480 - val_accuracy: 0.9870\n",
      "Epoch 21/100\n",
      "10000/10000 [==============================] - 3s 315us/step - loss: 0.0191 - accuracy: 0.9949 - val_loss: 0.0544 - val_accuracy: 0.9850\n",
      "Epoch 22/100\n",
      "10000/10000 [==============================] - 3s 323us/step - loss: 0.0263 - accuracy: 0.9906 - val_loss: 0.0753 - val_accuracy: 0.9830\n",
      "Epoch 23/100\n",
      "10000/10000 [==============================] - 3s 328us/step - loss: 0.0199 - accuracy: 0.9936 - val_loss: 0.0321 - val_accuracy: 0.9870\n",
      "Epoch 24/100\n",
      "10000/10000 [==============================] - 3s 331us/step - loss: 0.0216 - accuracy: 0.9939 - val_loss: 0.0383 - val_accuracy: 0.9880\n",
      "Epoch 25/100\n",
      "10000/10000 [==============================] - 3s 319us/step - loss: 0.0207 - accuracy: 0.9933 - val_loss: 0.0506 - val_accuracy: 0.9850\n",
      "Epoch 26/100\n",
      "10000/10000 [==============================] - 3s 324us/step - loss: 0.0182 - accuracy: 0.9942 - val_loss: 0.0459 - val_accuracy: 0.9870\n",
      "Epoch 27/100\n",
      "10000/10000 [==============================] - 3s 318us/step - loss: 0.0169 - accuracy: 0.9944 - val_loss: 0.0673 - val_accuracy: 0.9830\n",
      "Epoch 28/100\n",
      "10000/10000 [==============================] - 3s 328us/step - loss: 0.0269 - accuracy: 0.9923 - val_loss: 0.0546 - val_accuracy: 0.9830\n",
      "Epoch 29/100\n",
      "10000/10000 [==============================] - 3s 328us/step - loss: 0.0207 - accuracy: 0.9930 - val_loss: 0.0489 - val_accuracy: 0.9870\n",
      "Epoch 30/100\n",
      "10000/10000 [==============================] - 3s 323us/step - loss: 0.0163 - accuracy: 0.9947 - val_loss: 0.0480 - val_accuracy: 0.9870\n",
      "Epoch 31/100\n",
      "10000/10000 [==============================] - 3s 327us/step - loss: 0.0221 - accuracy: 0.9922 - val_loss: 0.0423 - val_accuracy: 0.9890\n",
      "Epoch 32/100\n",
      "10000/10000 [==============================] - 4s 374us/step - loss: 0.0199 - accuracy: 0.9935 - val_loss: 0.0375 - val_accuracy: 0.9890\n",
      "Epoch 33/100\n",
      "10000/10000 [==============================] - 3s 330us/step - loss: 0.0217 - accuracy: 0.9930 - val_loss: 0.0420 - val_accuracy: 0.9880\n",
      "Epoch 34/100\n",
      "10000/10000 [==============================] - 3s 323us/step - loss: 0.0197 - accuracy: 0.9943 - val_loss: 0.0547 - val_accuracy: 0.9860\n",
      "Epoch 35/100\n",
      "10000/10000 [==============================] - 3s 330us/step - loss: 0.0249 - accuracy: 0.9927 - val_loss: 0.0431 - val_accuracy: 0.9900\n",
      "Epoch 36/100\n",
      "10000/10000 [==============================] - 3s 321us/step - loss: 0.0247 - accuracy: 0.9926 - val_loss: 0.0308 - val_accuracy: 0.9900\n",
      "Epoch 37/100\n",
      "10000/10000 [==============================] - 3s 321us/step - loss: 0.0194 - accuracy: 0.9927 - val_loss: 0.0431 - val_accuracy: 0.9850\n",
      "Epoch 38/100\n",
      "10000/10000 [==============================] - 3s 339us/step - loss: 0.0166 - accuracy: 0.9955 - val_loss: 0.0370 - val_accuracy: 0.9900\n",
      "Epoch 39/100\n",
      "10000/10000 [==============================] - 3s 329us/step - loss: 0.0172 - accuracy: 0.9947 - val_loss: 0.0574 - val_accuracy: 0.9860\n",
      "Epoch 40/100\n",
      "10000/10000 [==============================] - 3s 324us/step - loss: 0.0197 - accuracy: 0.9941 - val_loss: 0.0640 - val_accuracy: 0.9860\n",
      "Epoch 41/100\n",
      "10000/10000 [==============================] - 3s 324us/step - loss: 0.0284 - accuracy: 0.9922 - val_loss: 0.0355 - val_accuracy: 0.9840\n",
      "Epoch 42/100\n",
      "10000/10000 [==============================] - 3s 319us/step - loss: 0.0148 - accuracy: 0.9960 - val_loss: 0.0370 - val_accuracy: 0.9870\n",
      "Epoch 43/100\n",
      "10000/10000 [==============================] - 3s 322us/step - loss: 0.0190 - accuracy: 0.9940 - val_loss: 0.0262 - val_accuracy: 0.9910\n",
      "Epoch 44/100\n",
      "10000/10000 [==============================] - 3s 337us/step - loss: 0.0184 - accuracy: 0.9950 - val_loss: 0.0406 - val_accuracy: 0.9890\n",
      "Epoch 45/100\n",
      "10000/10000 [==============================] - 4s 359us/step - loss: 0.0202 - accuracy: 0.9938 - val_loss: 0.0197 - val_accuracy: 0.9940\n",
      "Epoch 46/100\n",
      "10000/10000 [==============================] - 3s 344us/step - loss: 0.0204 - accuracy: 0.9933 - val_loss: 0.0361 - val_accuracy: 0.9900\n",
      "Epoch 47/100\n",
      "10000/10000 [==============================] - 3s 325us/step - loss: 0.0188 - accuracy: 0.9942 - val_loss: 0.0427 - val_accuracy: 0.9860\n",
      "Epoch 48/100\n",
      "10000/10000 [==============================] - 3s 320us/step - loss: 0.0196 - accuracy: 0.9936 - val_loss: 0.0396 - val_accuracy: 0.9860\n",
      "Epoch 49/100\n",
      "10000/10000 [==============================] - 3s 323us/step - loss: 0.0187 - accuracy: 0.9939 - val_loss: 0.0353 - val_accuracy: 0.9900\n",
      "Epoch 50/100\n",
      "10000/10000 [==============================] - 3s 329us/step - loss: 0.0145 - accuracy: 0.9956 - val_loss: 0.0308 - val_accuracy: 0.9910\n",
      "Epoch 51/100\n",
      "10000/10000 [==============================] - 3s 319us/step - loss: 0.0174 - accuracy: 0.9946 - val_loss: 0.0456 - val_accuracy: 0.9890\n",
      "Epoch 52/100\n",
      "10000/10000 [==============================] - 3s 322us/step - loss: 0.0194 - accuracy: 0.9935 - val_loss: 0.0478 - val_accuracy: 0.9840\n",
      "Epoch 53/100\n",
      "10000/10000 [==============================] - 3s 324us/step - loss: 0.0218 - accuracy: 0.9947 - val_loss: 0.0308 - val_accuracy: 0.9900\n",
      "Epoch 54/100\n",
      "10000/10000 [==============================] - 3s 324us/step - loss: 0.0144 - accuracy: 0.9953 - val_loss: 0.0291 - val_accuracy: 0.9910\n",
      "Epoch 55/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 3s 346us/step - loss: 0.0177 - accuracy: 0.9949 - val_loss: 0.0227 - val_accuracy: 0.9930\n",
      "Epoch 56/100\n",
      "10000/10000 [==============================] - 3s 316us/step - loss: 0.0192 - accuracy: 0.9940 - val_loss: 0.0391 - val_accuracy: 0.9880\n",
      "Epoch 57/100\n",
      "10000/10000 [==============================] - 3s 325us/step - loss: 0.0151 - accuracy: 0.9961 - val_loss: 0.0314 - val_accuracy: 0.9900\n",
      "Epoch 58/100\n",
      "10000/10000 [==============================] - 3s 323us/step - loss: 0.0249 - accuracy: 0.9923 - val_loss: 0.0511 - val_accuracy: 0.9830\n",
      "Epoch 59/100\n",
      "10000/10000 [==============================] - 3s 327us/step - loss: 0.0168 - accuracy: 0.9947 - val_loss: 0.0284 - val_accuracy: 0.9930\n",
      "Epoch 60/100\n",
      "10000/10000 [==============================] - 3s 326us/step - loss: 0.0105 - accuracy: 0.9968 - val_loss: 0.0355 - val_accuracy: 0.9900\n",
      "Epoch 61/100\n",
      "10000/10000 [==============================] - 3s 322us/step - loss: 0.0194 - accuracy: 0.9946 - val_loss: 0.0323 - val_accuracy: 0.9910\n",
      "Epoch 62/100\n",
      "10000/10000 [==============================] - 3s 317us/step - loss: 0.0144 - accuracy: 0.9958 - val_loss: 0.0337 - val_accuracy: 0.9900\n",
      "Epoch 63/100\n",
      "10000/10000 [==============================] - 3s 329us/step - loss: 0.0176 - accuracy: 0.9953 - val_loss: 0.0206 - val_accuracy: 0.9910\n",
      "Epoch 64/100\n",
      "10000/10000 [==============================] - 3s 320us/step - loss: 0.0148 - accuracy: 0.9950 - val_loss: 0.0324 - val_accuracy: 0.9900\n",
      "Epoch 65/100\n",
      "10000/10000 [==============================] - 3s 328us/step - loss: 0.0106 - accuracy: 0.9972 - val_loss: 0.0443 - val_accuracy: 0.9890\n",
      "Epoch 66/100\n",
      "10000/10000 [==============================] - 3s 333us/step - loss: 0.0171 - accuracy: 0.9939 - val_loss: 0.0311 - val_accuracy: 0.9910\n",
      "Epoch 67/100\n",
      "10000/10000 [==============================] - 3s 323us/step - loss: 0.0125 - accuracy: 0.9960 - val_loss: 0.0384 - val_accuracy: 0.9900\n",
      "Epoch 68/100\n",
      "10000/10000 [==============================] - 3s 318us/step - loss: 0.0186 - accuracy: 0.9940 - val_loss: 0.0332 - val_accuracy: 0.9890\n",
      "Epoch 69/100\n",
      "10000/10000 [==============================] - 4s 358us/step - loss: 0.0152 - accuracy: 0.9960 - val_loss: 0.0445 - val_accuracy: 0.9900\n",
      "Epoch 70/100\n",
      "10000/10000 [==============================] - 3s 328us/step - loss: 0.0176 - accuracy: 0.9951 - val_loss: 0.0395 - val_accuracy: 0.9880\n",
      "Epoch 71/100\n",
      "10000/10000 [==============================] - 3s 315us/step - loss: 0.0120 - accuracy: 0.9964 - val_loss: 0.0487 - val_accuracy: 0.9840\n",
      "Epoch 72/100\n",
      "10000/10000 [==============================] - 3s 328us/step - loss: 0.0096 - accuracy: 0.9975 - val_loss: 0.0350 - val_accuracy: 0.9900\n",
      "Epoch 73/100\n",
      "10000/10000 [==============================] - 3s 326us/step - loss: 0.0140 - accuracy: 0.9952 - val_loss: 0.0378 - val_accuracy: 0.9870\n",
      "Epoch 74/100\n",
      "10000/10000 [==============================] - 3s 325us/step - loss: 0.0138 - accuracy: 0.9965 - val_loss: 0.0466 - val_accuracy: 0.9880\n",
      "Epoch 75/100\n",
      "10000/10000 [==============================] - 3s 325us/step - loss: 0.0142 - accuracy: 0.9962 - val_loss: 0.0511 - val_accuracy: 0.9870\n",
      "Epoch 76/100\n",
      "10000/10000 [==============================] - 3s 318us/step - loss: 0.0185 - accuracy: 0.9944 - val_loss: 0.0247 - val_accuracy: 0.9940\n",
      "Epoch 77/100\n",
      "10000/10000 [==============================] - 3s 340us/step - loss: 0.0155 - accuracy: 0.9957 - val_loss: 0.0245 - val_accuracy: 0.9950\n",
      "Epoch 78/100\n",
      "10000/10000 [==============================] - 3s 331us/step - loss: 0.0137 - accuracy: 0.9965 - val_loss: 0.0402 - val_accuracy: 0.9880\n",
      "Epoch 79/100\n",
      "10000/10000 [==============================] - 3s 330us/step - loss: 0.0106 - accuracy: 0.9965 - val_loss: 0.0362 - val_accuracy: 0.9910\n",
      "Epoch 80/100\n",
      "10000/10000 [==============================] - 3s 327us/step - loss: 0.0104 - accuracy: 0.9966 - val_loss: 0.0201 - val_accuracy: 0.9950\n",
      "Epoch 81/100\n",
      "10000/10000 [==============================] - 3s 315us/step - loss: 0.0165 - accuracy: 0.9957 - val_loss: 0.0440 - val_accuracy: 0.9860\n",
      "Epoch 82/100\n",
      "10000/10000 [==============================] - 3s 328us/step - loss: 0.0173 - accuracy: 0.9951 - val_loss: 0.0449 - val_accuracy: 0.9890\n",
      "Epoch 83/100\n",
      "10000/10000 [==============================] - 3s 322us/step - loss: 0.0063 - accuracy: 0.9979 - val_loss: 0.0259 - val_accuracy: 0.9910\n",
      "Epoch 84/100\n",
      "10000/10000 [==============================] - 3s 324us/step - loss: 0.0124 - accuracy: 0.9957 - val_loss: 0.0163 - val_accuracy: 0.9950\n",
      "Epoch 85/100\n",
      "10000/10000 [==============================] - 3s 325us/step - loss: 0.0149 - accuracy: 0.9958 - val_loss: 0.0384 - val_accuracy: 0.9900\n",
      "Epoch 86/100\n",
      "10000/10000 [==============================] - 3s 324us/step - loss: 0.0122 - accuracy: 0.9971 - val_loss: 0.0440 - val_accuracy: 0.9890\n",
      "Epoch 87/100\n",
      "10000/10000 [==============================] - 3s 318us/step - loss: 0.0134 - accuracy: 0.9957 - val_loss: 0.0283 - val_accuracy: 0.9910\n",
      "Epoch 88/100\n",
      "10000/10000 [==============================] - 3s 326us/step - loss: 0.0101 - accuracy: 0.9973 - val_loss: 0.0333 - val_accuracy: 0.9920\n",
      "Epoch 89/100\n",
      "10000/10000 [==============================] - 3s 331us/step - loss: 0.0118 - accuracy: 0.9963 - val_loss: 0.0214 - val_accuracy: 0.9920\n",
      "Epoch 90/100\n",
      "10000/10000 [==============================] - 3s 323us/step - loss: 0.0118 - accuracy: 0.9964 - val_loss: 0.0263 - val_accuracy: 0.9930\n",
      "Epoch 91/100\n",
      "10000/10000 [==============================] - 3s 314us/step - loss: 0.0111 - accuracy: 0.9971 - val_loss: 0.0308 - val_accuracy: 0.9920\n",
      "Epoch 92/100\n",
      "10000/10000 [==============================] - 3s 341us/step - loss: 0.0129 - accuracy: 0.9958 - val_loss: 0.0294 - val_accuracy: 0.9950\n",
      "Epoch 93/100\n",
      "10000/10000 [==============================] - 3s 337us/step - loss: 0.0072 - accuracy: 0.9980 - val_loss: 0.0247 - val_accuracy: 0.9930\n",
      "Epoch 94/100\n",
      "10000/10000 [==============================] - 3s 316us/step - loss: 0.0121 - accuracy: 0.9953 - val_loss: 0.0352 - val_accuracy: 0.9910\n",
      "Epoch 95/100\n",
      "10000/10000 [==============================] - 3s 318us/step - loss: 0.0126 - accuracy: 0.9957 - val_loss: 0.0288 - val_accuracy: 0.9910\n",
      "Epoch 96/100\n",
      "10000/10000 [==============================] - 3s 319us/step - loss: 0.0081 - accuracy: 0.9970 - val_loss: 0.0267 - val_accuracy: 0.9930\n",
      "Epoch 97/100\n",
      "10000/10000 [==============================] - 3s 323us/step - loss: 0.0098 - accuracy: 0.9971 - val_loss: 0.0511 - val_accuracy: 0.9860\n",
      "Epoch 98/100\n",
      "10000/10000 [==============================] - 3s 325us/step - loss: 0.0129 - accuracy: 0.9967 - val_loss: 0.0309 - val_accuracy: 0.9910\n",
      "Epoch 99/100\n",
      "10000/10000 [==============================] - 3s 317us/step - loss: 0.0077 - accuracy: 0.9975 - val_loss: 0.0487 - val_accuracy: 0.9900\n",
      "Epoch 100/100\n",
      "10000/10000 [==============================] - 3s 320us/step - loss: 0.0112 - accuracy: 0.9966 - val_loss: 0.0266 - val_accuracy: 0.9900\n",
      "-------------------------------------------------------------------------------------------\n",
      "Qualitative Test Result Analysis\n",
      "John travelled to the hallway . Mary journeyed to the bathroom . Where is John ? | Prediction: hallway | Ground Truth: hallway\n",
      "John travelled to the hallway . Mary journeyed to the bathroom . Daniel went back to the bathroom . John moved to the bedroom . Where is Mary ? | Prediction: bathroom | Ground Truth: bathroom\n",
      "John travelled to the hallway . Mary journeyed to the bathroom . Daniel went back to the bathroom . John moved to the bedroom . John went to the hallway . Sandra journeyed to the kitchen . Where is Sandra ? | Prediction: kitchen | Ground Truth: kitchen\n",
      "John travelled to the hallway . Mary journeyed to the bathroom . Daniel went back to the bathroom . John moved to the bedroom . John went to the hallway . Sandra journeyed to the kitchen . Sandra travelled to the hallway . John went to the garden . Where is Sandra ? | Prediction: hallway | Ground Truth: hallway\n",
      "John travelled to the hallway . Mary journeyed to the bathroom . Daniel went back to the bathroom . John moved to the bedroom . John went to the hallway . Sandra journeyed to the kitchen . Sandra travelled to the hallway . John went to the garden . Sandra went back to the bathroom . Sandra moved to the kitchen . Where is Sandra ? | Prediction: kitchen | Ground Truth: kitchen\n",
      "Sandra travelled to the kitchen . Sandra travelled to the hallway . Where is Sandra ? | Prediction: hallway | Ground Truth: hallway\n",
      "Sandra travelled to the kitchen . Sandra travelled to the hallway . Mary went to the bathroom . Sandra moved to the garden . Where is Sandra ? | Prediction: garden | Ground Truth: garden\n",
      "Sandra travelled to the kitchen . Sandra travelled to the hallway . Mary went to the bathroom . Sandra moved to the garden . Sandra travelled to the office . Daniel journeyed to the hallway . Where is Daniel ? | Prediction: hallway | Ground Truth: hallway\n",
      "Sandra travelled to the kitchen . Sandra travelled to the hallway . Mary went to the bathroom . Sandra moved to the garden . Sandra travelled to the office . Daniel journeyed to the hallway . Daniel journeyed to the office . John moved to the hallway . Where is Sandra ? | Prediction: office | Ground Truth: office\n",
      "Sandra travelled to the kitchen . Sandra travelled to the hallway . Mary went to the bathroom . Sandra moved to the garden . Sandra travelled to the office . Daniel journeyed to the hallway . Daniel journeyed to the office . John moved to the hallway . John travelled to the bathroom . John journeyed to the office . Where is Daniel ? | Prediction: office | Ground Truth: office\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_epochs = 100\n",
    "batch_size = 32\n",
    "lstm_size = 64\n",
    "\n",
    "\n",
    "\n",
    "# train, batch_size = 32 and epochs = 120\n",
    "\n",
    "print(\"Trainig the model\")\n",
    "model.fit([inputs_train, queries_train], answers_train, batch_size, train_epochs,\n",
    "      validation_data=([inputs_test, queries_test], answers_test))\n",
    "\n",
    "\n",
    "model.save('./data_babi/babi_model.h5')\n",
    "\n",
    "print('-------------------------------------------------------------------------------------------')\n",
    "print('Qualitative Test Result Analysis')\n",
    "for i in range(0,10):\n",
    "    current_inp = test_stories[i]\n",
    "    current_story, current_query, current_answer = vectorize_stories([current_inp], word_idx, story_maxlen, query_maxlen)\n",
    "    current_prediction = model.predict([current_story, current_query])\n",
    "    current_prediction = idx_word[np.argmax(current_prediction)]\n",
    "    print(' '.join(current_inp[0]), ' '.join(current_inp[1]), '| Prediction:', current_prediction, '| Ground Truth:', current_inp[2])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# +++++++++++++++++++++++++++\n",
    "\n",
    "\n",
    "# 텐서플로 2.0 버젼으로 바꾸기 ????\n",
    "\n",
    "\n",
    "\n",
    "# +++++++++++++++++++++++++++ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

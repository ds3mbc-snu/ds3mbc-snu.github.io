{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 수학교육에 활용할 수 있는 챗봇을 만들자 !! \n",
    "\n",
    "<p> &nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# +++++++++++++++++++++++++ \n",
    "\n",
    "## 1. 인공지능의 학습과 인간의 학습을  탐구\n",
    "\n",
    "## 2. 구체적으로 수학교육용 인공지능 개발 \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# +++++++++++++++++++++++++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 교재의 챕더 1,2,3 통해서 기본을 학습하고\n",
    "\n",
    "## 챕터 4장 5장에서 텍스트분류와 유사도를 \n",
    "\n",
    "## 팀별 발료로 학습한다.  목표는 챕터 6\n",
    "\n",
    "## 수학교육용 한글 챗봇을 만드는 것이다 \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<p> &nbsp;\n",
    "    \n",
    "# 인공지능 번역과 텐서플로우 2.0 버젼 학습\n",
    "<p>    &nbsp;\n",
    "\n",
    "\n",
    "###################################################################\n",
    "\n",
    "\n",
    "# (목표) Toy 코드와 Attentoin 으로 챗봇을 설계!\n",
    "\n",
    "\n",
    "<p> &nbsp;\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy 코드 (파트A+파트B) 와 어텐션 코드 C\n",
    "\n",
    "# +++++++++++++++++++++++++++++++++\n",
    "\n",
    "# [목표] 파트 A + 파트 B  ++ 어텐션 파트 C \n",
    "\n",
    "<p> &nbsp;\n",
    "\n",
    "#####################################################################\n",
    "<p> &nbsp;\n",
    "\n",
    "### 파트 A와 B는 다음 주소의 설명코드를 ipython 만든 것\n",
    "\n",
    "https://crazia.tistory.com/entry/Deep-Learning-seq2seq-%EB%A5%BC-%EC%9D%B4%EC%9A%A9%ED%95%9C-%EC%B1%97%EB%B4%87-Neural-Machine-Chatbot\n",
    "\n",
    "\n",
    "##########################################################\n",
    "    \n",
    "## 파트 C 의 코드는 Attention matrix 에 대한 내용이다 !!\n",
    "\n",
    "### https://www.tensorflow.org/tutorials/text/nmt_with_attention\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## (참고) \n",
    "\n",
    "### https://www.tensorflow.org/tutorials/text/transformer\n",
    "    \n",
    "    \n",
    "    \n",
    "# 구글의 머신트랜스레이션과 어텐션 활용\n",
    "\n",
    "https://github.com/tensorflow/nmt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 준비  : 다음을 통해 파트 챗봇 데이터를 탐구하자 !! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['금융챗봇이야', '머리가', '당신은요', '못생겼나요', '좋은', '감사합니요다', '껴', '내리네요', '캄보디아의', '챙겨야겠어요']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'어떻게 지내세요?\\n잘 지내고 있어요. 당신은요?\\n저도 잘 지내고 있어요.\\n네, 그럼 안녕히 가세요.\\n계속 연락해요.\\n네, 제가 곧 다시 연락 드릴게요.\\n이메일로 연락주세요.\\n지금 '"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "DATA_PATH=\"./data_nmt/conversation2.txt\" \n",
    "############## 챗봇 데이터 ############\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def Tokenizer( sentence ):\n",
    "    token=[]\n",
    "    for word in sentence.strip().split():\n",
    "        token.extend(re.compile(\"([.,!?\\\"':;)(])\").split(word))\n",
    "        \n",
    "    ret=[t for t in token if t]\n",
    "    return ret             \n",
    "\n",
    "    \n",
    "    \n",
    "wordsk=[]\n",
    "datask=[]\n",
    "\n",
    "\n",
    "\n",
    "with open( DATA_PATH , 'r' , encoding='utf-8' ) as f:\n",
    "    lines=f.read()\n",
    "    datask.append(lines)\n",
    "    wordsk=Tokenizer( lines  )\n",
    "    wordsk=list( set( wordsk ) )\n",
    "\n",
    "    \n",
    "# list <=== datas.shape \n",
    "\n",
    "\n",
    "print( wordsk[0:10] )\n",
    "\n",
    "datask[0][0:100] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['가을', '상영', '좋은', '급', '걱정', '파키스탄', '껴', '내리네요', '헤어', '1,294.67원']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'잘 지내고 있어요. 당신은요?\\n저도 잘 지내고 있어요.\\n네, 그럼 안녕히 가세요.\\n계속 연락해요.\\n네, 제가 곧 다시 연락 드릴게요.\\n이메일로 연락주세요.\\n지금 어디신가요?\\n집이요'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "\n",
    "wordsz=[]\n",
    "datasz=[]\n",
    "\n",
    "with open(  DATA_PATH , 'r', encoding='utf-8') as  content_file :\n",
    "    for con in content_file: \n",
    "        content = content_file.read()\n",
    "        datasz.append( content )\n",
    "        wordsz.extend( okt.morphs(content)   )\n",
    "        wordsz = list(set(wordsz))\n",
    "\n",
    "            \n",
    "print( wordsz[0:10] )\n",
    "\n",
    "datasz[0][0:100] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 쳇봇 데이터 다루기 \n",
    "\n",
    "    f = pd.DataFrame([['1위', '심재철'],\n",
    "                ['2위', '서미경']],\n",
    "               columns=['rank', 'keyword'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>어떻게 지내세요?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>잘 지내고 있어요. 당신은요?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>저도 잘 지내고 있어요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>네, 그럼 안녕히 가세요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>계속 연락해요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>그러게요. 거긴 어때요?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>매우 포근한 날씨네요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>나들이 가기 좋은 날씨죠.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>안녕하세요</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>안녕하세요. 만나서 반가워요.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0\n",
       "0           어떻게 지내세요?\n",
       "1    잘 지내고 있어요. 당신은요?\n",
       "2       저도 잘 지내고 있어요.\n",
       "3      네, 그럼 안녕히 가세요.\n",
       "4            계속 연락해요.\n",
       "..                ...\n",
       "495     그러게요. 거긴 어때요?\n",
       "496      매우 포근한 날씨네요.\n",
       "497    나들이 가기 좋은 날씨죠.\n",
       "498             안녕하세요\n",
       "499  안녕하세요. 만나서 반가워요.\n",
       "\n",
       "[500 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file = open( \"./data_nmt/conversation2.txt\", \"r\" , encoding=\"utf-8\")\n",
    "data = file.readlines()\n",
    "df1 = []\n",
    "for ii in data:\n",
    "    df1.append(ii[:-1])\n",
    "\n",
    "\n",
    "df2 = pd.DataFrame( df1  )\n",
    " \n",
    "df2.to_csv( './data_nmt/conversation2.csv' , index=False, encoding='utf-8')\n",
    "\n",
    "\n",
    "datacsv = pd.read_csv( './data_nmt/conversation2.csv' ,  encoding='utf-8')\n",
    "\n",
    "datacsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  0\n",
      "0         어떻게 지내세요?\n",
      "1  잘 지내고 있어요. 당신은요?\n",
      "2     저도 잘 지내고 있어요.\n",
      "3    네, 그럼 안녕히 가세요.\n",
      "4          계속 연락해요.\n",
      "어떻게 지내세요?\n",
      "잘 지내고 있어요. 당신은요?\n",
      "저도 잘 지내고 있어요.\n",
      "네, 그럼 안녕히 가세요.\n",
      "계속 연락해요.\n",
      "['어떻게 지내세요 ?', '잘 지내고있어요 . 당 신 은 요 ?', '저 도 잘 지내고있어요 .', '네 , 그럼 안녕히가세요 .', '계속 연락 해 요 .']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['어떻게 지내세요 ?',\n",
       " '잘 지내고있어요 . 당 신 은 요 ?',\n",
       " '저 도 잘 지내고있어요 .',\n",
       " '네 , 그럼 안녕히가세요 .',\n",
       " '계속 연락 해 요 .']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print( datacsv[0:5] )\n",
    "\n",
    "result_data=list( )\n",
    "\n",
    "input_data = list( datacsv['0'][0:5] )\n",
    "######################################\n",
    "\n",
    "for seq in input_data :\n",
    "    print( seq )\n",
    "    result = \" \".join( okt.morphs(seq.replace(' ', '')))\n",
    "    result_data.append( result )\n",
    "    \n",
    "print( result_data )\n",
    "\n",
    "datas = []\n",
    "\n",
    "datas.extend( result_data )\n",
    "             \n",
    "datas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['어떻게', '지내세요', '잘', '지내고있어요', '당', '신', '은', '요', '저', '도', '잘', '지내고있어요', '네', '그럼', '안녕히가세요', '계속', '연락', '해', '요']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "\n",
    "FILTERS = \"([~.,!?\\\"':;)(])\"\n",
    "CHANGE_FILTER = re.compile(FILTERS)\n",
    "\n",
    "vocabwords = []\n",
    "for sentence in datas:\n",
    "    # FILTERS = \"([~.,!?\\\"':;)(])\"\n",
    "    # 위 필터와 같은 값들을 정규화 표현식을 \n",
    "    # 통해서 모두 \"\" 으로 변환 해주는 부분이다.\n",
    "    sentence = re.sub(CHANGE_FILTER, \"\", sentence)\n",
    "    for word in sentence.split():\n",
    "        vocabwords.append(word)\n",
    "            \n",
    "print( vocabwords )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    # 판다스를 통해서 데이터를 불러온다.\n",
    "    data_df = pd.read_csv( './data_nmt/conversation2.csv', header=0)\n",
    "    # 질문과 답변 열을 가져와 question과 answer에 넣는다.\n",
    "    # 홓수줄 짝수불 \n",
    "    \n",
    "    question = data_df[0::2]\n",
    "    answer = data_df[1::2]\n",
    "\n",
    "\n",
    "    \n",
    "    # skleran에서 지원하는 함수를 통해서 학습 셋과 \n",
    "    # 테스트 셋을 나눈다.\n",
    "    \n",
    "    train_input, eval_input, train_label, eval_label = train_test_split(question, answer, test_size=0.33, random_state=42)\n",
    "    \n",
    "    # 그 값을 리턴한다.\n",
    "    return train_input, train_label, eval_input, eval_label\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_input, train_label, eval_input, eval_label = load_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "462         핀란드의 수도는?\n",
      "340     단풍 구경하러 가실래요?\n",
      "130          한번 봐야겠어요\n",
      "280    나이에 비해 어려 보여요.\n",
      "248           감사합니요다.\n",
      "Name: 0, dtype: object\n",
      "            0\n",
      "463  헬싱키 입니다.\n",
      "341       좋죠.\n",
      "131   엄청 재밌어요\n",
      "281    감사합니다.\n",
      "249     천만에요.\n"
     ]
    }
   ],
   "source": [
    "print( train_input['0'][0:5] )\n",
    "\n",
    "print( train_label[0:5] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ++++++++++++++++++++++++++++++++++++++++\n",
    "<p> &nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [목표] 챗봇 데이터를 보강한 한글 okt() 챗봇으로 !!\n",
    "\n",
    "<p> &nbsp;\n",
    "\n",
    "# [참고] 5.교재한글CHATBOT.ipynb 코드를 활용 !    \n",
    "    \n",
    "<p> &nbsp;\n",
    "    \n",
    "# ++++++++++++++++++++++++++++++++++++++++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> &nbsp;\n",
    "    \n",
    "# [출발점 코드] 아래는 okt 안쓰는 코드 (okt 로 바꿈)\n",
    "    \n",
    "  \n",
    "    \n",
    "# ++++++++++++++++++++++++++++++++++++++++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [탐구실험 시작]  toy 코드로 파트 A 와 파트 B  시작  !! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\WinPython37F\\python-3.7.2.amd64\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\WinPython37F\\python-3.7.2.amd64\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\WinPython37F\\python-3.7.2.amd64\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\WinPython37F\\python-3.7.2.amd64\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\WinPython37F\\python-3.7.2.amd64\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\WinPython37F\\python-3.7.2.amd64\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\WinPython37F\\python-3.7.2.amd64\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\WinPython37F\\python-3.7.2.amd64\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\WinPython37F\\python-3.7.2.amd64\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\WinPython37F\\python-3.7.2.amd64\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\WinPython37F\\python-3.7.2.amd64\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\WinPython37F\\python-3.7.2.amd64\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow.compat.v1  as tf\n",
    "\n",
    "\n",
    "\n",
    "tf.disable_eager_execution()\n",
    "\n",
    "\n",
    "\n",
    "########### 모델 ###############\n",
    "\n",
    "\n",
    "train_dir=\"./data_nmt/model\" # model directory\n",
    "\n",
    "log_dir=\"./data_nmt/logs\" #log directory\n",
    "\n",
    "ckpt_name=\"conversation.ckpt\" #checkpoint \n",
    "\n",
    "\n",
    "######## 데이터 ##################\n",
    "\n",
    "\n",
    "DATA_PATH=\"./data_nmt/conversation2.txt\" #data\n",
    "\n",
    "VOC_PATH=\"./data_nmt/conversation.voc\" #data.voc\n",
    "\n",
    "#################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-*- coding: utf-8 -*-\n",
    "#import tensorflow.compat.v1  as tf\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import codecs\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "##########################################\n",
    "# 함글 처리를 할 것인가 ??? 학기말 과제 ??\n",
    "\n",
    "\n",
    "# from konlpy.tag import Okt\n",
    "\n",
    "# okt=Okt()\n",
    "\n",
    "\n",
    "########################################\n",
    "# 이걸 쓰는 경우에는 # 을 없앤다 \n",
    "#########################################\n",
    "########################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "PAD=\"_PAD_\"\n",
    "GO=\"_GO_\"\n",
    "EOS=\"_EOS_\"\n",
    "UNK=\"_UNK_\"\n",
    "\n",
    "PAD_ID=0\n",
    "GO_ID=1\n",
    "EOS_ID=2\n",
    "UNK_ID=3\n",
    "ALL=[PAD_ID,GO_ID, EOS_ID,UNK_ID]\n",
    "\n",
    "\n",
    "\n",
    "class Conversation():\n",
    "    \n",
    "    voc_list=[]\n",
    "    voc_dict=[]\n",
    "    voc_size=[]\n",
    "    conversation=[]\n",
    "    ##############\n",
    "    index_in_epoch=0\n",
    "\n",
    "    def Tokens_to_index(self, token):\n",
    "        index=[]\n",
    "        for word in token:\n",
    "            if word in self.voc_dict:\n",
    "                index.append(self.voc_dict[word])\n",
    "            else:\n",
    "                index.append( UNK_ID)\n",
    "        return index\n",
    "\n",
    "    def Index_to_token(self,index):\n",
    "        token=[]\n",
    "        for word in index:\n",
    "            token.append(self.voc_list[word])\n",
    "        return token\n",
    "\n",
    "    def pad(self, seq, max_len, start=None, eos=None):\n",
    "        if start:\n",
    "            padded_seq = [ GO_ID] + seq\n",
    "        elif eos:\n",
    "            padded_seq = seq + [ EOS_ID]\n",
    "        else:\n",
    "            padded_seq = seq\n",
    "\n",
    "        if len(padded_seq) < max_len:\n",
    "            return padded_seq + ([ PAD_ID] * (max_len - len(padded_seq)))\n",
    "        else:\n",
    "            return padded_seq\n",
    "\n",
    "    def pad_left(self, seq, max_len):\n",
    "        if len(seq) < max_len:\n",
    "            return ([ PAD_ID] * (max_len - len(seq))) + seq\n",
    "        else:\n",
    "            return seq\n",
    "\n",
    "\n",
    "    def transform(self, input, output, input_max, output_max):\n",
    "\n",
    "        enc_input = self.pad(input, input_max)\n",
    "        dec_input = self.pad(output, output_max, start=True)\n",
    "        target = self.pad(output, output_max, eos=True)\n",
    "\n",
    "        enc_input.reverse()\n",
    "        enc_input = np.eye(self.voc_size)[enc_input]\n",
    "        dec_input = np.eye(self.voc_size)[dec_input]\n",
    "\n",
    "        return enc_input, dec_input, target\n",
    "\n",
    "    def max_len(self, batch_set):\n",
    "        max_len_input = 0\n",
    "        max_len_output = 0\n",
    "\n",
    "        for i in range(0, len(batch_set), 2):\n",
    "            len_input = len(batch_set[i])\n",
    "            len_output = len(batch_set[i+1])\n",
    "            if len_input > max_len_input:\n",
    "                max_len_input = len_input\n",
    "            if len_output > max_len_output:\n",
    "                max_len_output = len_output\n",
    "\n",
    "        return max_len_input, max_len_output + 1\n",
    "\n",
    "    def decode(self, index,string=False):\n",
    "        token=[[self.voc_list[i] for i in k] for k in index]\n",
    "\n",
    "        if string:\n",
    "            return self.decode_to_string(token[0])\n",
    "        else:\n",
    "            return token\n",
    "\n",
    "    def decode_to_string(self,token):\n",
    "        txt=' '.join(token)\n",
    "        return txt.strip()\n",
    "\n",
    "    def cut_eos(self, indices):\n",
    "        eos_idx = indices.index( EOS_ID)\n",
    "        return indices[:eos_idx]\n",
    "\n",
    "    def is_eos(self, voc_id):\n",
    "        return voc_id == EOS_ID\n",
    "\n",
    "    def is_defined(self, voc_id):\n",
    "        return voc_id in ALL\n",
    "\n",
    "    def Load_conversation(self,data_path):\n",
    "        self.conversation=[]\n",
    "        \n",
    "        with open(data_path,'r',encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                \n",
    "                #token=okt.morphs(line.strip()) # 한글처리\n",
    "                ##########################################\n",
    "                \n",
    "                tokens=self.Tokenizer(line.strip()) \n",
    "                ##########################################\n",
    "                # okt로 token 생성시 이 라인을 주석처리함.\n",
    "                \n",
    "                index=self.Tokens_to_index(tokens)\n",
    "                self.conversation.append(index)\n",
    "                ###############################\n",
    "                \n",
    "\n",
    "    def Load_voc(self, voc_path): \n",
    "        \n",
    "        front=ALL+[]\n",
    "        self.voc_list=front\n",
    "        \n",
    "        with open(voc_path,'r', encoding='utf-8') as f:\n",
    "            for word in f:\n",
    "                self.voc_list.append(word.strip())\n",
    "        self.voc_dict={n: i for i, n in enumerate(self.voc_list)}\n",
    "        self.voc_size= len(self.voc_list)\n",
    "        \n",
    "        \n",
    "    def Tokenizer(self, sentence):\n",
    "        token=[]\n",
    "        for word in sentence.strip().split():\n",
    "            token.extend(re.compile(\"([.,!?\\\"':;)(])\").split(word))\n",
    "        \n",
    "        ret=[t for t in token if t]\n",
    "        return ret             \n",
    "        \n",
    "    def next_batch(self, batch_size):\n",
    "        enc_input = []\n",
    "        dec_input = []\n",
    "        dec_target = []\n",
    "\n",
    "        start = self.index_in_epoch\n",
    "\n",
    "        if self.index_in_epoch + batch_size < len(self.conversation) - 1:\n",
    "            self.index_in_epoch = self.index_in_epoch + batch_size\n",
    "        else:\n",
    "            self.index_in_epoch = 0\n",
    "\n",
    "        batch_set = self.conversation[start:start+batch_size]\n",
    "        batch_set = batch_set+ batch_set[1:] + batch_set[0:1]\n",
    "\n",
    "        max_len_input, max_len_output = self.max_len(batch_set)\n",
    "\n",
    "        for i in range(0, len(batch_set) - 1, 2):\n",
    "            enc, dec, tar = self.transform(batch_set[i], batch_set[i+1],max_len_input, max_len_output)\n",
    "\n",
    "            enc_input.append(enc)\n",
    "            dec_input.append(dec)\n",
    "            dec_target.append(tar)\n",
    "\n",
    "        return enc_input, dec_input, dec_target\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BATCH_SIZE=100\n",
    "EPOCH=2000\n",
    "\n",
    "n_hidden=128\n",
    "n_layer=3\n",
    "learning_rate=0.001\n",
    "max_decode_len=20\n",
    "\n",
    "\n",
    "\n",
    "#############################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class sequence2sequence:\n",
    "\n",
    "    logits=None\n",
    "    outputs=None\n",
    "    train_op=None\n",
    "    cost=None\n",
    "    output_keep_prob=0.5\n",
    "\n",
    "    def __init__(self, voc_size):\n",
    "\n",
    "        self.voc_size=voc_size\n",
    "\n",
    "        self.n_hidden=n_hidden\n",
    "        self.n_layer=n_layer\n",
    "        self.learning_rate=learning_rate\n",
    "\n",
    "        print(\"self.voc_size\",self.voc_size)\n",
    "        print(\"self.n_hidden\",self.n_hidden)\n",
    "        print(\"self.n_layer\",self.n_layer)\n",
    "        print(\"self.learning_rate\",self.learning_rate)\n",
    "\n",
    "        self.enc_input=tf.placeholder(tf.float32,[None,None,self.voc_size])\n",
    "        self.dec_input=tf.placeholder(tf.float32,[None,None,self.voc_size])\n",
    "        self.dec_target=tf.placeholder(tf.int64,[None,None])\n",
    "\n",
    "        self.weights=tf.Variable(tf.ones([self.n_hidden,self.voc_size]),name=\"weights\")\n",
    "        self.bias=tf.Variable(tf.zeros([self.voc_size]),name=\"bias\")\n",
    "        self.global_step=tf.Variable(0,trainable=False,name=\"global_step\")\n",
    "\n",
    "\n",
    "        self.Make_model()\n",
    "        self.saver=tf.train.Saver(tf.global_variables())\n",
    "\n",
    "    def Make_model(self,output_keep_prob=0.5):\n",
    "        \n",
    "        self.enc_input=tf.transpose(self.enc_input,[1,0,2])\n",
    "        self.dec_input=tf.transpose(self.dec_input,[1,0,2])\n",
    "        \n",
    "\n",
    "        with tf.variable_scope('encode'):\n",
    "            enc_cell=tf.nn.rnn_cell.MultiRNNCell([self.cell(self.n_hidden,output_keep_prob) for _ in range(self.n_layer)])\n",
    "            outputs,enc_state=tf.nn.dynamic_rnn(enc_cell,self.enc_input, dtype=tf.float32)\n",
    "            \n",
    "        with tf.variable_scope('decode'):\n",
    "            dec_cell=tf.nn.rnn_cell.MultiRNNCell([self.cell(self.n_hidden,output_keep_prob) for _ in range(self.n_layer)])\n",
    "            outputs,dec_state=tf.nn.dynamic_rnn(dec_cell, self.dec_input, dtype=tf.float32, initial_state=enc_state)\n",
    "            \n",
    "        self.logits, self.cost, self.train_op=self.Make_ops(outputs,self.dec_target)\n",
    "        self.outputs=tf.argmax(self.logits,2)\n",
    "\n",
    "    def cell(self,n_hidden, output_keep_prob):\n",
    "        #############################################################\n",
    "        rnn_cell=tf.nn.rnn_cell.BasicRNNCell(self.n_hidden)\n",
    "        #현재는 BasicRNNCell입니다,  BasicLSTMCell, LSTMCell 로도 구현 가능합니.\n",
    "        #cell 이름만 바꾸어 주면 됩니다.#############################\n",
    "        \n",
    "        rnn_cell=tf.nn.rnn_cell.DropoutWrapper(rnn_cell,output_keep_prob=output_keep_prob)\n",
    "        return rnn_cell\n",
    "    \n",
    "    def Make_ops(self,outputs,dec_target):\n",
    "        time_step=tf.shape(outputs)[1]\n",
    "        outputs=tf.reshape(outputs,[-1, self.n_hidden])\n",
    "        \n",
    "        logits=tf.matmul(outputs, self.weights) + self.bias\n",
    "        logits=tf.reshape(logits,[-1,time_step,self.voc_size])\n",
    "        \n",
    "        cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=dec_target))\n",
    "        train_op= tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(cost,global_step=self.global_step)\n",
    "\n",
    "        tf.summary.scalar('cost',cost)\n",
    "        \n",
    "        return logits, cost,train_op\n",
    "        \n",
    "    def train(self,session, enc_input,dec_input,dec_target):\n",
    "        return session.run([self.train_op,self.cost],\n",
    "                          feed_dict={self.enc_input: enc_input,\n",
    "                                     self.dec_input: dec_input,\n",
    "                                     self.dec_target: dec_target})\n",
    "\n",
    "    def logs(self,session,writer,enc_input,dec_input,dec_target):\n",
    "        merge=tf.summary.merge_all()\n",
    "        summary=session.run(merge,feed_dict={self.enc_input:enc_input,self.dec_input:dec_input,self.dec_target:dec_target})\n",
    "        writer.add_summary(summary,self.global_step.eval())\n",
    "\n",
    "    def predict(self, session, enc_input, dec_input):\n",
    "        return session.run(self.outputs,\n",
    "                           feed_dict={self.enc_input: enc_input,\n",
    "                                      self.dec_input: dec_input})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ++++ 단어장을 이미 만들었으면 다음을 생략  !!\n",
    "\n",
    "\n",
    "## 혹시 데이터를 바꾸었다면 다음 실행으로 단어장 만듦 !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어장이 새로 만들어짐 !!\n"
     ]
    }
   ],
   "source": [
    "# 단어장 만드는 코드 (데이터가 바뀐 경우만 새러 실행시킨다)\n",
    "### 단어장 만들기 코드  (데이터가 바뀌어 새로 단어장 만들때 사용)\n",
    "####################################################################\n",
    "\n",
    "\n",
    "#from konlpy.tag import Okt\n",
    "#okt = Okt()\n",
    "\n",
    "#import tensorflow as tf\n",
    "#import numpy as np\n",
    "\n",
    "####################################################################\n",
    "#\n",
    "#okt = Okt()  사용시.\n",
    "#\n",
    "#def Make_voc(data_path,voc_path):\n",
    "#    words=[]\n",
    "#\n",
    "#    with open(data_path, 'r', encoding='utf-8') as content_file:\n",
    "#        for con in content_file: \n",
    "#            content = content_file.read()\n",
    "#            words.extend(   okt.morphs(content)   )\n",
    "#        words = list(set(words))\n",
    "#\n",
    "#    with open(voc_path, 'w') as vocab_file:\n",
    "#        for w in words:\n",
    "#            vocab_file.write(w + '\\n')\n",
    "#\n",
    "# --okt 사용시 이 함수 주석을 전부 풀어주고 아래의 함수를 주석처리.\n",
    "#\n",
    "#####################################################################\n",
    "\n",
    "\n",
    "def Make_voc(conv, data_path, voc_path):\n",
    "    words=[]\n",
    "\n",
    "    with open(data_path,'r', encoding='utf-8') as f:\n",
    "        lines=f.read()\n",
    "        words=conv.Tokenizer(lines)\n",
    "        words=list(set(words))\n",
    "\n",
    "    with open(voc_path,'w', encoding='utf-8') as wf:\n",
    "         for w in words:\n",
    "            wf.write(w+\"\\n\")\n",
    "    print(\"단어장이 새로 만들어짐 !!\")\n",
    "    \n",
    "\n",
    "## okt 대신에 토크나이저 쓴 경우######################################\n",
    "\n",
    "\n",
    "conv=Conversation()\n",
    "\n",
    "\n",
    "Make_voc(conv, DATA_PATH,  VOC_PATH)\n",
    "\n",
    "####################################\n",
    "\n",
    "\n",
    "#  Make_voc(config.DATA_PATH, config.VOC_PATH) \n",
    "# <-okt 사용시 위를 주석처리하고 이 라인의 주석을 풀어줌.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "# ++++++++++++++++++++++++++++++++++++++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> &nbsp;\n",
    "\n",
    "\n",
    "# 파트 A : 채팅하기 (파트 B로 미리 훈련시킨 ckpt 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-*- coding: utf-8 -*-\n",
    "\n",
    "#import tensorflow.compat.v1  as tf\n",
    "#tf.disable_eager_execution()\n",
    "\n",
    "#from konlpy.tag import Okt\n",
    "#okt=Okt()\n",
    "\n",
    "\n",
    "#from model import sequence2sequence\n",
    "#from Conversation import Conversation\n",
    "#####################################\n",
    "\n",
    "\n",
    "########### 사용하는 모델의 위치##########\n",
    "\n",
    "\n",
    "#train_dir=\"./data_nmt/model\" # model directory\n",
    "\n",
    "#log_dir=\"./data_nmt/logs\" #log directory\n",
    "\n",
    "#ckpt_name=\"conversation.ckpt\" #checkpoint \n",
    "\n",
    "\n",
    "###########################################\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4\"\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "\n",
    "\n",
    "\n",
    "##########################################\n",
    "\n",
    "class Chat:\n",
    "\n",
    "    def __init__(self, VOC_PATH, train_dir):\n",
    "        self.conv= Conversation()\n",
    "        self.conv.Load_voc(VOC_PATH)\n",
    "        self.model=sequence2sequence(self.conv.voc_size)\n",
    "\n",
    "        self.sess=tf.Session()\n",
    "        \n",
    "        \n",
    "        ckpt=tf.train.get_checkpoint_state(train_dir)\n",
    "        \n",
    "        \n",
    "        self.model.saver.restore(self.sess, ckpt.model_checkpoint_path)\n",
    "        \n",
    "        ###############################################################\n",
    "        # 여기서 훈련시켜 만든 ckpt 모델을 불러서 채팅을 한다 #########\n",
    "        ###############################################################\n",
    "    \n",
    "    \n",
    "    def decode(self,enc_input, dec_input):\n",
    "        if type(dec_input) is np.ndarray:\n",
    "            dec_input=dec_input.tolist()\n",
    "        input_len=int(math.ceil((len(enc_input)+1) * 1.5))\n",
    "        enc_input,dec_input,_=self.conv.transform( enc_input, dec_input,input_len,  max_decode_len)\n",
    "\n",
    "        return self.model.predict(self.sess, [enc_input], [dec_input])\n",
    "\n",
    "    def run(self):\n",
    "        \n",
    "        #sys.stdout.write(\" > \")\n",
    "        print( '끝내려면 :  exit [엔터]')\n",
    "        \n",
    "        #sys.stdout.flush()\n",
    "        line=input( ) # sys.stdin.readline()\n",
    "\n",
    "        while line != 'exit' :\n",
    "            print(self.response(line.strip()))\n",
    "            #sys.stdout.write(\"\\n > \")\n",
    "            #sys.stdout.flush()\n",
    "            line=input() # sys.stdin.readline()\n",
    "\n",
    "    def response(self, ipt):\n",
    "        \n",
    "        #enc_input=okt.morphs(ipt)  # 한글처리 \n",
    "        ########################################\n",
    "        enc_input=self.conv.Tokenizer(ipt) \n",
    "        #######################################\n",
    "        # okt 사용시 위 라인의 주석을 풀고, 이 라인을 주석처리.\n",
    "        \n",
    "        enc_input=self.conv.Tokens_to_index(enc_input)\n",
    "        cur=0\n",
    "        dec_input=[]\n",
    "        for i in range( max_decode_len):\n",
    "            outputs=self.decode(enc_input,dec_input)\n",
    "            if self.conv.is_eos(outputs[0][cur]):\n",
    "                break\n",
    "            elif self.conv.is_defined(outputs[0][cur]) is not True:\n",
    "                dec_input.append(outputs[0][cur])\n",
    "                cur+=1\n",
    "\n",
    "        reply=self.conv.decode([dec_input],True)\n",
    "\n",
    "        return reply\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.voc_size 792\n",
      "self.n_hidden 128\n",
      "self.n_layer 3\n",
      "self.learning_rate 0.001\n",
      "INFO:tensorflow:Restoring parameters from ./data_nmt/model\\conversation.ckpt-16600\n",
      "끝내려면 :  exit [엔터]\n",
      "안녕\n",
      "오늘 걸렸어요 갈\n",
      "서울은 \n",
      "울란바토르 알바하시나요 보세요 .\n",
      "잘있냐\n",
      "무슨 스케줄 있나요 ?\n",
      "없어\n",
      "만나서 포근한 .\n",
      "나도\n",
      "갤럭시 s8이요 있어\n",
      "좋으냐 ?\n",
      "아무것도 입니다 거긴\n",
      "여기도\n",
      "보세요 .\n",
      "무엇을\n",
      "더 인구수는 .\n",
      "무슨 인구수\n",
      "어디서 입니다 ?\n",
      "대한민국\n",
      "전주의 인구수는 얼마인가요 ?\n",
      "나도 몰라\n",
      "생각 잘 해보고요 합격을\n",
      "너도 합격을\n",
      "네\n",
      "고맙다\n",
      "지금 인구수는 해요 ?\n",
      "무슨말이냐\n",
      "오늘 좀 ?\n",
      "공부 재미있냐 ?\n",
      "광주에요 .\n",
      "전주의 인구수는 ?\n",
      "전주의 이야기보따리 기대하고 일어났던데요 운영하나요 일어나야죠\n",
      "광주는 ?\n",
      "아무것도 안해 감사합니다\n",
      "무슨얘기냐 ?\n",
      "아무것도 안해 좋은\n",
      "exit\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "#######################\n",
    "# 없으면 에러를 일으킴 \n",
    "#######################\n",
    "\n",
    "\n",
    "\n",
    "chatbot=Chat( VOC_PATH, train_dir )\n",
    "\n",
    "\n",
    "chatbot.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실행 예\n",
    "\n",
    "    self.voc_size 792\n",
    "    self.n_hidden 128\n",
    "    self.n_layer 3\n",
    "    self.learning_rate 0.001\n",
    "\n",
    "    INFO:tensorflow:Restoring parameters from ./model\\conversation.ckpt-10000\n",
    "    끝내려면 :  exit [엔터]\n",
    "    안녕\n",
    "    헬싱키 수도는 ?\n",
    "    몰라\n",
    "    나들이 수도는 ?\n",
    "    exit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ++++++++++++++++++++++++++++++++++++++++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "\n",
    "\n",
    "# 파트 B : 학습시키기 ( A가 사용하는 훈련된 ckpt 만듦)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='5'\n",
    "\n",
    "\n",
    "import random\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "checkpoint=tf.train.get_checkpoint_state( train_dir)\n",
    "\n",
    "checkpoint_path=os.path.join( train_dir, ckpt_name)\n",
    "\n",
    "#################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train(conv, batch_size, epoch):\n",
    "\n",
    "    model=sequence2sequence(conv.voc_size)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        if checkpoint and tf.train.checkpoint_exists( checkpoint.model_checkpoint_path):\n",
    "            model.saver.restore(sess, checkpoint.model_checkpoint_path)\n",
    "        else:\n",
    "            print(\"모델을 빌드하자 !!\")\n",
    "            sess.run( tf.global_variables_initializer())\n",
    "    \n",
    "        writer= tf.summary.FileWriter( log_dir,sess.graph)\n",
    "        total_batch=int(math.ceil(len(conv.conversation)/float(batch_size)))\n",
    "        \n",
    "        #print( len(conv.conversation)  )\n",
    "        #print( total_batch * EPOCH )\n",
    "    \n",
    "    \n",
    "    \n",
    "        for step in range(total_batch * epoch):\n",
    "            enc_input, dec_input,dec_target= conv.next_batch(batch_size)\n",
    "        \n",
    "            _,loss=model.train(sess, enc_input, dec_input,dec_target)\n",
    "        \n",
    "            if(step+1)% 50 ==0:  # 50 에포크에 한번씩 저장 \n",
    "                model.logs(sess, writer, enc_input, dec_input, dec_target)\n",
    "                model.saver.save(sess, checkpoint_path,global_step= model.global_step)\n",
    "                print('스텝:', '%06d' % model.global_step.eval(),'cost =', '{:.6f}'.format(loss))\n",
    "\n",
    "        model.saver.save(sess, checkpoint_path, global_step= model.global_step)\n",
    "        \n",
    "        print(\"빌드 끝 !! Finished\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 바로 위에서 전에 학습한 checkpoint 파악해서 부름\n",
    "\n",
    "## 바로 아래에서 그 checkpoint 부터 train 시작함 !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size 792\n",
      "500\n",
      "1200\n",
      "self.voc_size 792\n",
      "self.n_hidden 128\n",
      "self.n_layer 3\n",
      "self.learning_rate 0.001\n",
      "WARNING:tensorflow:From <ipython-input-11-12a6fec3db26>:68: BasicRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.SimpleRNNCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-11-12a6fec3db26>:56: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-11-12a6fec3db26>:57: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From C:\\WinPython37F\\python-3.7.2.amd64\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py:455: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From <ipython-input-14-e7be35831dba>:26: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from ./data_nmt/model\\conversation.ckpt-16600\n",
      "스텝: 016650 cost = 3.570256\n",
      "스텝: 016700 cost = 3.004854\n",
      "스텝: 016750 cost = 2.306341\n",
      "스텝: 016800 cost = 1.593279\n",
      "스텝: 016850 cost = 1.728105\n",
      "WARNING:tensorflow:From C:\\WinPython37F\\python-3.7.2.amd64\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "스텝: 016900 cost = 1.597751\n",
      "스텝: 016950 cost = 1.263955\n",
      "스텝: 017000 cost = 0.842512\n",
      "스텝: 017050 cost = 0.987032\n",
      "스텝: 017100 cost = 0.987837\n",
      "스텝: 017150 cost = 0.806400\n",
      "스텝: 017200 cost = 0.572723\n",
      "스텝: 017250 cost = 0.669453\n",
      "스텝: 017300 cost = 0.636452\n",
      "스텝: 017350 cost = 0.577441\n",
      "스텝: 017400 cost = 0.383967\n",
      "스텝: 017450 cost = 0.498806\n",
      "스텝: 017500 cost = 0.437643\n",
      "스텝: 017550 cost = 0.396093\n",
      "스텝: 017600 cost = 0.306179\n",
      "스텝: 017650 cost = 0.367973\n",
      "스텝: 017700 cost = 0.359148\n",
      "스텝: 017750 cost = 0.337580\n",
      "스텝: 017800 cost = 0.235192\n",
      "빌드 끝 !! Finished\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# 바로 앞부터 반복한다\n",
    "# 학습한것 불러와야한다\n",
    "\n",
    "########################\n",
    "# 앞에서 chat 을 한 경우\n",
    "# 이 것 안하면 에러나옴\n",
    "########################\n",
    "\n",
    "\n",
    "BATCH_SIZE=64\n",
    "#############\n",
    "\n",
    "\n",
    "EPOCH=150\n",
    "#############\n",
    "\n",
    "\n",
    "conv=Conversation()\n",
    "conv.Load_voc(VOC_PATH)\n",
    "conv.Load_conversation(DATA_PATH)\n",
    "    \n",
    "print(\"size\",conv.voc_size)\n",
    "# 현재는 792\n",
    "\n",
    "print( len(conv.conversation)  )\n",
    "# 현재는 500\n",
    "\n",
    "total_batch=int(math.ceil(len(conv.conversation)/float(BATCH_SIZE)))\n",
    "# 현재는 약 8\n",
    "\n",
    "\n",
    "print( total_batch * EPOCH )\n",
    "# EPOCH 에 8을 곱한다. 이 값만큼 train 실행 \n",
    "# EPOCH=150 이면 1200 번 train 하고\n",
    "# 50번마다 모델을 save 하게 된다.\n",
    "\n",
    "train(conv, batch_size=BATCH_SIZE, epoch=EPOCH)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실행 예 : 이전 학습된 상태를 이어서 학습\n",
    "<pre>\n",
    "size 792\n",
    "500\n",
    "1200\n",
    "self.voc_size 792\n",
    "self.n_hidden 128\n",
    "self.n_layer 3\n",
    "self.learning_rate 0.001\n",
    "INFO:tensorflow:Restoring parameters from ./data_nmt/model\\conversation.ckpt-15400\n",
    "스텝: 015450 cost = 0.322491\n",
    "스텝: 015500 cost = 0.324007\n",
    "스텝: 015550 cost = 0.279106\n",
    "스텝: 015600 cost = 0.174179\n",
    "스텝: 015650 cost = 0.261228\n",
    "스텝: 015700 cost = 0.290277\n",
    "스텝: 015750 cost = 0.227265\n",
    "스텝: 015800 cost = 0.146770\n",
    "스텝: 015850 cost = 0.206373\n",
    "스텝: 015900 cost = 0.241369\n",
    "스텝: 015950 cost = 0.190909\n",
    "스텝: 016000 cost = 0.132130\n",
    "스텝: 016050 cost = 0.157904\n",
    "스텝: 016100 cost = 0.204415\n",
    "스텝: 016150 cost = 0.141685\n",
    "스텝: 016200 cost = 0.142168\n",
    "스텝: 016250 cost = 0.180591\n",
    "스텝: 016300 cost = 0.200669\n",
    "스텝: 016350 cost = 0.126908\n",
    "스텝: 016400 cost = 0.104360\n",
    "스텝: 016450 cost = 0.146085\n",
    "스텝: 016500 cost = 0.145980\n",
    "스텝: 016550 cost = 0.095379\n",
    "스텝: 016600 cost = 0.077563\n",
    "빌드 끝 !! Finished\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<p>&nbsp;\n",
    "    \n",
    "    \n",
    "# ++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "<p>&nbsp;\n",
    "    \n",
    "    \n",
    "    \n",
    "# 파트 A , 파트 B 내용을 바탕으로 파트 C 탐구 !!\n",
    "\n",
    "<p>&nbsp;\n",
    "    \n",
    "# ====================================\n",
    "\n",
    "\n",
    "<p>&nbsp;\n",
    "\n",
    "# 파트 C : 구글의 머신트랜스레이션과 어텐션 코드\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ===================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "jmjh290raIky"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J0Qjg6vuaHNt"
   },
   "source": [
    "## ============== 텐서플로우 2.0 으로 실행시킨다 =============== \n",
    "\n",
    "# Tensorflow 2 : Neural machine translation with attention\n",
    "\n",
    "\n",
    "## [문제] 앞의 파트 A와 파트 B를 attention 과 연관시키는 방법은 무엇인가 ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AOpGoE2T-YXS"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/text/nmt_with_attention\">\n",
    "    <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />\n",
    "    View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/nmt_with_attention.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
    "    Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/nmt_with_attention.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
    "    View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/text/nmt_with_attention.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CiwtNgENbx2g"
   },
   "source": [
    "This notebook trains a sequence to sequence (seq2seq) model for Spanish to English translation. This is an advanced example that assumes some knowledge of sequence to sequence models.\n",
    "\n",
    "After training the model in this notebook, you will be able to input a Spanish sentence, such as *\"¿todavia estan en casa?\"*, and return the English translation: *\"are you still at home?\"*\n",
    "\n",
    "The translation quality is reasonable for a toy example, but the generated attention plot is perhaps more interesting. This shows which parts of the input sentence has the model's attention while translating:\n",
    "\n",
    "<img src=\"https://tensorflow.org/images/spanish-english.png\" alt=\"spanish-english attention plot\">\n",
    "\n",
    "Note: This example takes approximately 10 mintues to run on a single P100 GPU.\n",
    "\n",
    "### GPU 없는 경우에 CPU 로 작동. GPU를 클라우드에서 사용하여 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tnxXKDjq3jEL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor 2.0 기능을 씀 ==  2.0.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "\n",
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "#################################################\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "\n",
    "print('tensor 2.0 기능을 씀 == ',  tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wfodePkj3jEa"
   },
   "source": [
    "## Download and prepare the dataset\n",
    "\n",
    "We'll use a language dataset provided by http://www.manythings.org/anki/. This dataset contains language translation pairs in the format:\n",
    "\n",
    "```\n",
    "May I borrow this book?\t¿Puedo tomar prestado este libro?\n",
    "```\n",
    "\n",
    "There are a variety of languages available, but we'll use the English-Spanish dataset. For convenience, we've hosted a copy of this dataset on Google Cloud, but you can also download your own copy. After downloading the dataset, here are the steps we'll take to prepare the data:\n",
    "\n",
    "1. Add a *start* and *end* token to each sentence.\n",
    "2. Clean the sentences by removing special characters.\n",
    "3. Create a word index and reverse word index (dictionaries mapping from word → id and id → word).\n",
    "4. Pad each sentence to a maximum length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kRVATYOgJs1b"
   },
   "source": [
    "# Download the file\n",
    "\n",
    "path_to_zip = tf.keras.utils.get_file(\n",
    "    \n",
    "    'spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
    "    extract=True\n",
    "\n",
    ")\n",
    "\n",
    "path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\"\n",
    "\n",
    "\n",
    "# winpython37F 의 settings 의 .keras 의 data 디렉토리 밑에 저장됨 \n",
    "\n",
    "# 있으면 pass. 우리는 data_nmt 에 미리 저장된 spa.txt 파일로 한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = \"./data_nmt/spa-eng/spa.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rd0jw-eC3jEh"
   },
   "outputs": [],
   "source": [
    "# Converts the unicode file to ascii\n",
    "def unicode_to_ascii(s):\n",
    "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "      if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "  w = unicode_to_ascii(w.lower().strip())\n",
    "\n",
    "  # creating a space between a word and the punctuation following it\n",
    "  # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "  # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "  w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "  w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "\n",
    "  w = w.rstrip().strip()\n",
    "\n",
    "  # adding a start and an end token to the sentence\n",
    "  # so that the model know when to start and stop predicting.\n",
    "  w = '<start> ' + w + ' <end>'\n",
    "  return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "opI2GzOt479E"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> may i borrow this book ? <end>\n",
      "b'<start> \\xc2\\xbf puedo tomar prestado este libro ? <end>'\n"
     ]
    }
   ],
   "source": [
    "en_sentence = u\"May I borrow this book?\"\n",
    "sp_sentence = u\"¿Puedo tomar prestado este libro?\"\n",
    "print(preprocess_sentence(en_sentence))\n",
    "print(preprocess_sentence(sp_sentence).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OHn4Dct23jEm"
   },
   "outputs": [],
   "source": [
    "# 1. Remove the accents\n",
    "# 2. Clean the sentences\n",
    "# 3. Return word pairs in the format: [ENGLISH, SPANISH]\n",
    "\n",
    "\n",
    "def create_dataset(path, num_examples):\n",
    "  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "\n",
    "  word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
    "\n",
    "  return zip(*word_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cTbSbBz55QtF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> if you want to sound like a native speaker , you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo . <end>\n",
      "<start> si quieres sonar como un hablante nativo , debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un musico de banjo practica el mismo fraseo una y otra vez hasta que lo puedan tocar correctamente y en el tiempo esperado . <end>\n"
     ]
    }
   ],
   "source": [
    "# path_to_file = \"./data_nmt/spa-eng/spa.txt\"\n",
    "\n",
    "en, sp = create_dataset(path_to_file, None)\n",
    "print(en[-1])\n",
    "print(sp[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OmMZQpdO60dt"
   },
   "outputs": [],
   "source": [
    "def max_length(tensor):\n",
    "  return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bIOn8RCNDJXG"
   },
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "      filters='')\n",
    "  lang_tokenizer.fit_on_texts(lang)\n",
    "\n",
    "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "\n",
    "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
    "                                                         padding='post')\n",
    "\n",
    "  return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eAY9k49G3jE_"
   },
   "outputs": [],
   "source": [
    "def load_dataset(path, num_examples=None):\n",
    "  # creating cleaned input, output pairs\n",
    "  targ_lang, inp_lang = create_dataset(path, num_examples)\n",
    "\n",
    "  input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
    "  target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
    "\n",
    "  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GOi42V79Ydlr"
   },
   "source": [
    "### Limit the size of the dataset to experiment faster (optional)\n",
    "\n",
    "Training on the complete dataset of >100,000 sentences will take a long time. To train faster, we can limit the size of the dataset to 30,000 sentences (of course, translation quality degrades with less data):\n",
    "\n",
    "### num_examples = 30000 으로 제한시켜서 한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cnxC7q-j3jFD"
   },
   "outputs": [],
   "source": [
    "# Try experimenting with the size of that dataset\n",
    "num_examples = 30000\n",
    "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)\n",
    "\n",
    "# Calculate max_length of the target tensors\n",
    "max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4QILQkOs3jFG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24000 24000 6000 6000\n",
      "학습 : 테스트 = 80 : 20\n"
     ]
    }
   ],
   "source": [
    "# Creating training and validation sets using an 80-20 split\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "\n",
    "# Show length\n",
    "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))\n",
    "\n",
    "print( '학습 : 테스트 = 80 : 20' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lJPmLZGMeD5q"
   },
   "outputs": [],
   "source": [
    "def convert(lang, tensor):\n",
    "  for t in tensor:\n",
    "    if t!=0:\n",
    "      print (\"%d ----> %s\" % (t, lang.index_word[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VXukARTDd7MT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Language; index to word mapping\n",
      "1 ----> <start>\n",
      "38 ----> puedo\n",
      "72 ----> ir\n",
      "3 ----> .\n",
      "2 ----> <end>\n",
      "\n",
      "Target Language; index to word mapping\n",
      "1 ----> <start>\n",
      "4 ----> i\n",
      "25 ----> can\n",
      "36 ----> go\n",
      "3 ----> .\n",
      "2 ----> <end>\n"
     ]
    }
   ],
   "source": [
    "print (\"Input Language; index to word mapping\")\n",
    "convert(inp_lang, input_tensor_train[0])\n",
    "print ()\n",
    "print (\"Target Language; index to word mapping\")\n",
    "convert(targ_lang, target_tensor_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rgCLkfv5uO3d"
   },
   "source": [
    "### Create a tf.data dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TqHsArVZ3jFS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 단어의 수 :  9414\n",
      "스페인어의 단어 수 :  4935\n"
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "\n",
    "vocab_inp_size = len(inp_lang.word_index)+1\n",
    "vocab_tar_size = len(targ_lang.word_index)+1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "print( '영어 단어의 수 : ', vocab_inp_size )\n",
    "print( '스페인어의 단어 수 : ', vocab_tar_size )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qc6-NK1GtWQt"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 16]), TensorShape([64, 11]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TNfHIF71ulLu"
   },
   "source": [
    "## Write the encoder and decoder model\n",
    "\n",
    "Implement an encoder-decoder model with attention which you can read about in the TensorFlow [Neural Machine Translation (seq2seq) tutorial](https://github.com/tensorflow/nmt). This example uses a more recent set of APIs. This notebook implements the [attention equations](https://github.com/tensorflow/nmt#background-on-the-attention-mechanism) from the seq2seq tutorial. The following diagram shows that each input words is assigned a weight by the attention mechanism which is then used by the decoder to predict the next word in the sentence. The below picture and formulas are an example of attention mechanism from [Luong's paper](https://arxiv.org/abs/1508.04025v5). \n",
    "\n",
    "<img src=\"https://www.tensorflow.org/images/seq2seq/attention_mechanism.jpg\" width=\"500\" alt=\"attention mechanism\">\n",
    "\n",
    "The input is put through an encoder model which gives us the encoder output of shape *(batch_size, max_length, hidden_size)* and the encoder hidden state of shape *(batch_size, hidden_size)*.\n",
    "\n",
    "Here are the equations that are implemented:\n",
    "\n",
    "<img src=\"https://www.tensorflow.org/images/seq2seq/attention_equation_0.jpg\" alt=\"attention equation 0\" width=\"800\">\n",
    "<img src=\"https://www.tensorflow.org/images/seq2seq/attention_equation_1.jpg\" alt=\"attention equation 1\" width=\"800\">\n",
    "\n",
    "This tutorial uses [Bahdanau attention](https://arxiv.org/pdf/1409.0473.pdf) for the encoder. Let's decide on notation before writing the simplified form:\n",
    "\n",
    "* FC = Fully connected (dense) layer\n",
    "* EO = Encoder output\n",
    "* H = hidden state\n",
    "* X = input to the decoder\n",
    "\n",
    "And the pseudo-code:\n",
    "\n",
    "* `score = FC(tanh(FC(EO) + FC(H)))`\n",
    "* `attention weights = softmax(score, axis = 1)`. Softmax by default is applied on the last axis but here we want to apply it on the *1st axis*, since the shape of score is *(batch_size, max_length, hidden_size)*. `Max_length` is the length of our input. Since we are trying to assign a weight to each input, softmax should be applied on that axis.\n",
    "* `context vector = sum(attention weights * EO, axis = 1)`. Same reason as above for choosing axis as 1.\n",
    "* `embedding output` = The input to the decoder X is passed through an embedding layer.\n",
    "* `merged vector = concat(embedding output, context vector)`\n",
    "* This merged vector is then given to the GRU\n",
    "\n",
    "The shapes of all the vectors at each step have been specified in the comments in the code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 인코더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nZ2rI24i3jFg"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.enc_units = enc_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "\n",
    "  def call(self, x, hidden):\n",
    "    x = self.embedding(x)\n",
    "    output, state = self.gru(x, initial_state = hidden)\n",
    "    return output, state\n",
    "\n",
    "  def initialize_hidden_state(self):\n",
    "    return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "60gSVh05Jl6l"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (64, 16, 1024)\n",
      "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "# sample input\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 어탠션"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "umohpBN2OM94"
   },
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, units):\n",
    "    super(BahdanauAttention, self).__init__()\n",
    "    self.W1 = tf.keras.layers.Dense(units)\n",
    "    self.W2 = tf.keras.layers.Dense(units)\n",
    "    self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "  def call(self, query, values):\n",
    "    # hidden shape == (batch_size, hidden size)\n",
    "    # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "    # we are doing this to perform addition to calculate the score\n",
    "    hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "    # score shape == (batch_size, max_length, 1)\n",
    "    # we get 1 at the last axis because we are applying score to self.V\n",
    "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "    score = self.V(tf.nn.tanh(\n",
    "        self.W1(values) + self.W2(hidden_with_time_axis)))\n",
    "\n",
    "    # attention_weights shape == (batch_size, max_length, 1)\n",
    "    attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "    # context_vector shape after sum == (batch_size, hidden_size)\n",
    "    context_vector = attention_weights * values\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "    return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k534zTHiDjQU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape: (batch size, units) (64, 1024)\n",
      "Attention weights shape: (batch_size, sequence_length, 1) (64, 16, 1)\n"
     ]
    }
   ],
   "source": [
    "attention_layer = BahdanauAttention(10)\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 디코더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yJ_B3mhW3jFk"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.dec_units = dec_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    # used for attention\n",
    "    self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "  def call(self, x, hidden, enc_output):\n",
    "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "\n",
    "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "    x = self.embedding(x)\n",
    "\n",
    "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "    # passing the concatenated vector to the GRU\n",
    "    output, state = self.gru(x)\n",
    "\n",
    "    # output shape == (batch_size * 1, hidden_size)\n",
    "    output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "    # output shape == (batch_size, vocab)\n",
    "    x = self.fc(output)\n",
    "\n",
    "    return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P5UY8wko3jFp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, vocab size) (64, 4935)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                      sample_hidden, sample_output)\n",
    "\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_ch_71VbIRfK"
   },
   "source": [
    "##  옵티마이저와 loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WmTHr5iV3jFr"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DMVWzzsfNl4e"
   },
   "source": [
    "## Checkpoints ( './data_nmt/spa-eng' : Object-based saving)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zj8bXQTgNwrF"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = './data_nmt/spa-eng'\n",
    "#####################################\n",
    "\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"attentionckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hpObfY22IddU"
   },
   "source": [
    "## Training\n",
    "\n",
    "1. Pass the *input* through the *encoder* which return *encoder output* and the *encoder hidden state*.\n",
    "2. The encoder output, encoder hidden state and the decoder input (which is the *start token*) is passed to the decoder.\n",
    "3. The decoder returns the *predictions* and the *decoder hidden state*.\n",
    "4. The decoder hidden state is then passed back into the model and the predictions are used to calculate the loss.\n",
    "5. Use *teacher forcing* to decide the next input to the decoder.\n",
    "6. *Teacher forcing* is the technique where the *target word* is passed as the *next input* to the decoder.\n",
    "7. The final step is to calculate the gradients and apply it to the optimizer and backpropagate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sC9ArXSsVfqn"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "  loss = 0\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "\n",
    "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "    # Teacher forcing - feeding the target as the next input\n",
    "    for t in range(1, targ.shape[1]):\n",
    "      # passing enc_output to the decoder\n",
    "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "\n",
    "      loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "      # using teacher forcing\n",
    "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "  batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "  gradients = tape.gradient(loss, variables)\n",
    "\n",
    "  optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "  return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ddefjBMa3jF0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 1.0408\n",
      "Epoch 1 Batch 100 Loss 1.0504\n",
      "Epoch 1 Batch 200 Loss 0.9364\n",
      "Epoch 1 Batch 300 Loss 1.0437\n",
      "steps_per_epoch =  375 total_loss  =  tf.Tensor(375.38873, shape=(), dtype=float32)\n",
      "Epoch 1 Loss 1.0010\n",
      "Time taken for 1 epoch 1580.1497569084167 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.6826\n",
      "Epoch 2 Batch 100 Loss 0.7167\n",
      "Epoch 2 Batch 200 Loss 0.6485\n",
      "Epoch 2 Batch 300 Loss 0.6492\n",
      "steps_per_epoch =  375 total_loss  =  tf.Tensor(256.7156, shape=(), dtype=float32)\n",
      "Epoch 2 Loss 0.6846\n",
      "Time taken for 1 epoch 1247.8489425182343 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#EPOCHS = 10\n",
    "## 이 곳을 5번 반복해서 실행시켜도 됨\n",
    "\n",
    "EPOCHS = 2\n",
    "\n",
    "##########################\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "\n",
    "  enc_hidden = encoder.initialize_hidden_state()\n",
    "  total_loss = 0\n",
    "\n",
    "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "    batch_loss = train_step(inp, targ, enc_hidden)\n",
    "    total_loss += batch_loss\n",
    "\n",
    "    if batch % 100 == 0:\n",
    "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                   batch,\n",
    "                                                   batch_loss.numpy()))\n",
    "\n",
    "  # saving (checkpoint) the model every 2 epochs\n",
    "  # epoch=1 이면 끝날때만 \n",
    "  if (epoch + 1) % 2 == 0:\n",
    "    checkpoint.save( file_prefix = checkpoint_prefix )\n",
    "\n",
    "  print( 'steps_per_epoch = ', steps_per_epoch , 'total_loss  = ', total_loss)\n",
    "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "Epoch 1 Batch 0 Loss 1.0408\n",
    "Epoch 1 Batch 100 Loss 1.0504\n",
    "Epoch 1 Batch 200 Loss 0.9364\n",
    "Epoch 1 Batch 300 Loss 1.0437\n",
    "steps_per_epoch =  375 total_loss  =  tf.Tensor(375.38873, shape=(), dtype=float32)\n",
    "Epoch 1 Loss 1.0010\n",
    "Time taken for 1 epoch 1580.1497569084167 sec\n",
    "\n",
    "Epoch 2 Batch 0 Loss 0.6826\n",
    "Epoch 2 Batch 100 Loss 0.7167\n",
    "Epoch 2 Batch 200 Loss 0.6485\n",
    "Epoch 2 Batch 300 Loss 0.6492\n",
    "steps_per_epoch =  375 total_loss  =  tf.Tensor(256.7156, shape=(), dtype=float32)\n",
    "Epoch 2 Loss 0.6846\n",
    "Time taken for 1 epoch 1247.8489425182343 sec\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mU3Ce8M6I3rz"
   },
   "source": [
    "## Translate\n",
    "\n",
    "* The evaluate function is similar to the training loop, except we don't use *teacher forcing* here. The input to the decoder at each time step is its previous predictions along with the hidden state and the encoder output.\n",
    "* Stop predicting when the model predicts the *end token*.\n",
    "* And store the *attention weights for every time step*.\n",
    "\n",
    "Note: The encoder output is calculated only once for one input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EbQpyYs13jF_"
   },
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "  attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "\n",
    "  sentence = preprocess_sentence(sentence)\n",
    "\n",
    "  inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
    "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                         maxlen=max_length_inp,\n",
    "                                                         padding='post')\n",
    "  inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "  result = ''\n",
    "\n",
    "  hidden = [tf.zeros((1, units))]\n",
    "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "  dec_hidden = enc_hidden\n",
    "  dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
    "\n",
    "  for t in range(max_length_targ):\n",
    "    predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                         dec_hidden,\n",
    "                                                         enc_out)\n",
    "\n",
    "    # storing the attention weights to plot later on\n",
    "    attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "    attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "    result += targ_lang.index_word[predicted_id] + ' '\n",
    "\n",
    "    if targ_lang.index_word[predicted_id] == '<end>':\n",
    "      return result, sentence, attention_plot\n",
    "\n",
    "    # the predicted ID is fed back into the model\n",
    "    dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "  return result, sentence, attention_plot\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# function for plotting the attention weights\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "  fig = plt.figure(figsize=(10,10))\n",
    "  ax = fig.add_subplot(1, 1, 1)\n",
    "  ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "  fontdict = {'fontsize': 14}\n",
    "\n",
    "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n250XbnjOaqP"
   },
   "source": [
    "## 저장된 최근의 checkpoint 불러서 테스트 !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UJpT9D5_OgP6"
   },
   "outputs": [],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "\n",
    "def translate(sentence):\n",
    "  result, sentence, attention_plot = evaluate(sentence)\n",
    "\n",
    "  print('Input: %s' % (sentence))\n",
    "  print('Predicted translation: {}'.format(result))\n",
    "\n",
    "  attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "  plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WrAM0FDomq3E"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> hace mucho frio aqui . <end>\n",
      "Predicted translation: it s very cold here . <end> \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "translate(u'hace mucho frio aqui.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zSx2iM36EZQZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> esta es mi vida . <end>\n",
      "Predicted translation: this is my life . <end> \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmYAAAJwCAYAAAAjo60MAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nO3de7TlB13f/c83mZAUQqDhGpCbBUS5NoxcpMUALlFUVuWhtMolXB7S5aMVH6q2rC4qpSIFoxYf1BJU7lqQVlEQKQoUKrcGRBBUQO6GAEEkCYEkJN/nj71HDiczcc7JzPy+++T1Wuus2ee399nzPb81M+c9v2t1dwAAWN5xSw8AAMCKMAMAGEKYAQAMIcwAAIYQZgAAQwgzAIAhhBkAwBDCDABgCGEGADCEMAMAGEKYDVRVd6iqN1TVXZeeBQA4doTZTGcmOSPJ4xeeAwA4hspNzGepqkrysSSvT/J9SW7R3VcsOhQAcEzYYjbPA5JcP8mPJvlqkocsOw4AcKwIs3kek+SV3X1Jkt/MarcmAHAtYFfmIFV1vSSfTvI93f2WqrpHkrdltTvzC8tOBwAcbbaYzfJ/Jbmgu9+SJN39niQfSvIvF50KADZIVV2vqh5TVTdYepadEmazPDrJS7cte2nszgSAnXhEkhdk9XN1o9iVOURV3SrJR5N8c3d/aMvyb8jqLM1v6e4PLjQeAGyMqnpTkpsmuaS79y88zo4IMwBgz6iq2yb5YJJ7JXl7ktO7+wNLzrQTdmUOUlW3Xl/H7KDPHet5AGADPTrJW9bHaf9+NuxwIGE2y0eT3GT7wqq60fo5AODqPSbJS9aPX5rkkYfa6DGRMJulkhxs3/LJSb5yjGcBgI1SVd+W5LQkv7Ve9Ook103yHYsNtUP7lh6ApKp+cf2wkzyzqi7Z8vTxWe0nf88xHwwANsuZSV7V3V9Kku6+rKpekeSxWd3qcDxhNsNd179Wkm9OctmW5y5L8u4kZx/roQBgU1TViVldJuMHtj310iSvq6qTu/viYz/Zzjgrc4j1/u9XJHl8d1+09DwAsEmq6sZZ3V/6pd195bbnHpXkD7v7/EWG2wFhNkRVHZ/VcWR336TTegGAI8fB/0N09xVJPp7kOkvPAgAswxazQarqzKz2jT+quy9Yeh4AmK6qPpqDX9HgKrr7G4/yONeYg/9n+fEkt0vy11X1qSRf2vpkd99tkakAYK7nbnl8cpInJ3lnkretl903q6sb/NwxnmtXhNksr1x6AADYJN39d8FVVS9M8qzu/pmtr6mqpyS58zEebVfsygQA9oSqujCre2N+eNvy2yd5d3efssxkh8/B/wDAXvGlJGccZPkZSS45yPJx7MocpKquk+TfZ3UCwK2TnLD1+e4+fom5AGBD/EKSX6qq/Unevl52n6zuCPC0pYbaCWE2y39K8i+SPDOrP1w/keS2Sf5lkqcuNxYAzNfdz66qjyV5UlZ3AUiSP09yZne/YrHBdsAxZoOsT/n9oe7+g6q6KMk9uvuvquqHkjyoux++8IgjVdXj8rWtjF93HbhNODUa9rqqOjXJd+Xgf0efvshQMJQtZrPcLMmBq/5fnOSG68d/kORZi0w0XFX9RJKnJHlekvsn+eUkt18/dn9RWFhV3SfJa5JcmuQmSf46yWnrzz+WRJhxVFTVDbPtWPru/puFxjlsDv6f5RNJbrF+/OEkD14/vm+SLy8y0XxPTHJWdz8lyeVJntvdD83qejW3WXQyIEl+NsnLktwyq9vOPTCrLWfnxn84OcKq6jZV9dqq+kqSzyf53PrjgvWv49liNstvJ3lQVgcsPifJb1bVE7P6B+1nlxxssG/I6kKCySpeD5wK/Zvr5U9cYijg79wtyRO6u6vqiiQndvdHqurfJvmNrKINjpQXZLW36fFJzsth3hFgEmE2yHqrz4HHr6yqTya5X5IPdverl5tstPOT3DirrY0fz2rr4nuy2p25cX8hYQ+6bMvjz2S1JfvPszpc4xYH/QrYvXsluU93/9nSg+yWMBukqu6f5K3d/dUk6e53JHlHVe2rqvt395uXnXCkNyR5aJJ3J/m1JL9QVY9IcnqSjTgDB/a4dyf51iQfTPKmJD9dVTdL8qgk711wLvamjyY5cekhrglnZQ6y3sx/Wnd/dtvyGyX5rOuYXVVVHZfkuAMxW1X/IuutjEme192XLzkfXNutryd1/e5+Y1XdJMmL87W/o4/r7vctOiB7SlU9MMm/S/L/bL/6/6YQZoNU1ZVJbtbdn9u2/I5Jzt2EW0kca1V16ySf7G1/kKuqktyquz+xzGQAHGvrS02dmOT4rM78/erW5zfh56hdmQNU1e+uH3aSl1bVpVuePj7JXZK89ZgPthk+mtWp95/dtvzU9XO2MgJce/zI0gNcU8Jshs+vf60kX8jXXxrjsiT/O8nzj/VQG6Jy8IP8T87q1HzgGFtfLPuwdse4CDRHUne/aOkZrilhNkB3Py5J1reROLu7v7TsRPNV1S+uH3aSZ1bV1pvTHp/VmTnvOeaDAUny3C2PT07y5KwuX/O29bL7ZvV39OeO8VxcC6xPLnl0kn+U5KndfUFV3S/Jed390WWn+/s5xmyQ9YHs6e4r15/fPMn3JvlAd9uVuUVVvXH98Nuz+sd+6yn5l2V1RfGzu/tDx3g0YIuqemFWl/z5mW3Ln5Lkzt39qEUGY0+qqnsm+aOsDmW5c5I7ra+b97Qkd+zuH1xyvsMhzAapqtcm+YPufk5VnZzkL5JcL6v/cT6hu1+86IADVdULkjypuy9cehbgqqrqwiSnbz9Drqpun+Tdm3AwNptj/Z/2N3f3T61PBLj7Oszum+S/dff4O8LYlTnLPZP85Prxw5JcmOR2SR6Z5MezOs2cLQ7sBj6gqv5BVqfif6i7P77MVJvHeju0qnpYkt/r7svXjw+pu//HMRprk3wpyRlZ3WZuqzOSXLL9xXAN3TPJEw6y/NNZ3Y96PGE2y/WT/O368Xcm+e31D4M3JPml5caaa72b5J3d/ctVdZ2sjmO5c5LLqur7u/u1iw44lPW2I69McvOszvx95dW8ruMs4IP5hSS/tL6e2dvXy+6T5MwkT1tqKPasLyf5hwdZfqdc9ez9kdzEfJZPJLlfVV0vqxuYv369/NT4n+WhPDhf+8f+oVnF7c2z+gf/acuMtBGst8PU3ccduOjz+vGhPkTZQXT3s7M6EPuuSX5+/XHXJGd2t5uYc6S9KslPVdWBq/93Vd02ybOS/PelhtoJx5gNUlX/KquzmS7O6r6Pp3f3lVX1o0n+WXc/cNEBB6qqryS5fXd/qqp+NckXu/vfrP8ivq+7r7/ogENZb7u3Pinn25LcNF//n9vu7l9ZZiogSarqlCS/n+RuWR2jfX5WuzDfmuS7N+GqB3ZlDtLdz6uqc5PcOsnrD5ydmeSvkjx1uclGOz/JXarq01ltBTprvfzkJG7HdGjW2y5U1aOS/Gq+ds3Brf+z7STCDBa0PhHsn6xvzXR6Vv95end3/+Gykx0+YTZEVd0gyd26+y1J3rXt6b9N8oFjP9VG+PUkL09yXpIrsjpNOknundVZrRyc9bY7z0jy7CRPP3B/Vq5qfSbmN66vH3VRruZis87K5EjZ+nO0u9+Q5A1bnrtfVpee+sJiAx4mYTbHlUleW1UP7u4/PrCwqu6R1R+uWy422WDd/fSq+rMkt0nyiu4+cD2zr2Z1TAEHYb3t2ilJXijK/l7/OslF68cbf4scNsae+Dnq4P8huvuirA5afMy2px6V5HXdfcGxn2pjfDnJdyR5fVXdar3sOlkdq8ehWW8797Ik37P0ENN194u6+8A9f/9ZVpH2m+vlX/ex4JjsMXvl56gwm+XFSf55VZ2Q/N2dAH4wyQuXHGqyqnpkklck+WBW13w7Yf3UcfnaNeHYxnrbtScn+e6q+p2q+k9V9R+2fiw93FBfzurfts9U1fOr6v5LD8SetvE/R4XZLK/P6rIY37f+/EFZbcH4vcUmmu8nkzyxu//frHbDHfD2JPdYZqSNYL3tzr9K8l1ZnZX5/Un++ZaPhy8411jrW+DcNKvdm7dM8odV9fGqemZV3XnZ6diDNv7nqDAbZH0W5svytc2wj07y8u52ltyh3SFfuzHyVhdndTwQB2e97c5Tk/yb7r5pd9+lu++65eNuSw83VXdf0t0v7e6HZBVnP5vVD84/XXYy9pq98HPUwf/zvDjJu9bH/Hx/VrXPoZ2X5I5ZXfdtq/tndZkRDs56253jk/zu0kNsqqo6KckDs7pEyx2TfHLZidijNvrnqC1mw3T3+5O8L8lvJPlUd79z4ZGmOyfJL65PhU6SW1XVmVld0sA1pQ7NetudF2R171oOU1UdV1XfWVUvSvKZrP58fTrJd3T37Zadjr1o03+O2mI200uS/Jck/37pQabr7mevr13z+iQnJXljkkuTnN3d7i96CNbbrl03yf9dVQ9O8t5suxhvd//oIlPNdl6SGyR5bZLHJXn1lsuzsAtV9edJ7tDdfoYf2sb+HHVLpoGq6tSsDpR9Xnefv/Q8m6CqrpvkW7LaCvyB7nbJh8Ngve1MVb3xap5ut027qqo6K6tr5f3t0rPsFVX1I0lu1N3/celZptrkn6PCDABgCMeYAQAMIcwAAIYQZoOtj81gh6y3nbPOdsd62x3rbeess93ZxPUmzGbbuD9QQ1hvO2ed7Y71tjvW285ZZ7uzcetNmAEADHGtPyvzOnVin5TrLT3GQV2eS3NCTlx6jI1jve2cdbY71tvuWG87N3md1b7jlx7hkC678iu5znEnLT3GQV341Qsu6O6bbF9+rb843Um5Xu5dG3W3BgCOhuPmBsZkx//DGy49wkZ63eeet/2WeEnsygQAGEOYAQAMIcwAAIYQZgAAQwgzAIAhhBkAwBDCDABgCGEGADCEMAMAGEKYAQAMIcwAAIYQZgAAQwgzAIAhhBkAwBDCDABgCGEGADCEMAMAGEKYAQAMIcwAAIYQZgAAQwgzAIAhhBkAwBDCDABgCGEGADCEMAMAGEKYAQAMIcwAAIYQZgAAQwgzAIAhhBkAwBDCDABgCGEGADDEyDCrqjOqqqvqxtfkNQAAm2REmFXVm6rquTv8srcmOS3J54/CSAAAx9y+pQfYre6+LMn5S88BAHCkLL7FrKpemOTbk/zwetdkJ7nt+um7V9U7quqSqjq3qk7f8nVftyuzqm5QVS+pqs9W1Veq6iNV9WPH+vsBANitxcMsyZOSvC3JC7LaNXlakk+un3tmkn+X5PSsdlm+rKrqEO/z00numuR7k9wpyeOT/PXRGxsA4MhafFdmd3+xqi5Lckl3n58kVXWn9dNP7e43rpc9Pcn/TnLLJJ86yFvdJsmfdPc7159/7FC/Z1WdleSsJDkp1z0S3wYAwDU2YYvZ1XnvlsfnrX+96SFe+ytJHlFVf1pVZ1fVtx/qTbv7nO7e3937T8iJR2pWAIBrZHqYXb7lca9/PejM3f3arLaanZ3kxkleU1UvOLrjAQAcOVPC7LIkx1/TN+nuC7r7Jd392CRPSHJmVdkkBgBshMWPMVv7WJJ7VdVtk1ycXQTj+hi0dyd5f1bf18OSfKS7Lz1iUwIAHEVTtpidndVWsw8k+VySW+/iPS5N8owkf5rkj5NcP8n3HakBAQCOturuv/9Ve9gpdWrfux609BgALO24a3xEzbXS8afecOkRNtLrPve8d3X3/u3Lp2wxAwC41hNmAABDCDMAgCGEGQDAEMIMAGAIYQYAMIQwAwAYQpgBAAwhzAAAhhBmAABDCDMAgCGEGQDAEMIMAGAIYQYAMIQwAwAYQpgBAAwhzAAAhhBmAABDCDMAgCGEGQDAEMIMAGAIYQYAMIQwAwAYQpgBAAwhzAAAhhBmAABDCDMAgCGEGQDAEMIMAGAIYQYAMIQwAwAYYt/SAyyt9h2f42946tJjbJzzHnmnpUfYODd66KeWHmEjnfBd5y09wkbqK65YeoTNc6V1thtXXPD5pUfYU2wxAwAYQpgBAAwhzAAAhhBmAABDCDMAgCGEGQDAEMIMAGAIYQYAMIQwAwAYQpgBAAwhzAAAhhBmAABDCDMAgCGEGQDAEMIMAGAIYQYAMIQwAwAYQpgBAAwhzAAAhhBmAABDCDMAgCGEGQDAEMIMAGAIYQYAMIQwAwAYQpgBAAwhzAAAhhBmAABDCDMAgCGEGQDAEMIMAGAIYQYAMIQwAwAYYqPDrKpeWFWvXnoOAIAjYd/SA1xDT0pSSw8BAHAkbHSYdfcXl54BAOBI2TO7Mqvq/lX19qq6uKq+WFXvqKq7LD0jAMDh2ugtZgdU1b4kr0rya0kemeSEJKcnuWLJuQAAdmJPhFmSU5LcMMnvdfdfrZf9xaFeXFVnJTkrSU467uSjPx0AwGHY6F2ZB3T33yR5YZLXVdVrqurJVXWrq3n9Od29v7v3X+e4k47ZnAAAV2dPhFmSdPfjktw7yZuTPDTJB6vqwctOBQBw+PZMmCVJd/9pdz+ru89I8qYkZy47EQDA4dsTYVZVt6uq/1xV31ZVt6mqByS5W5IPLD0bAMDh2isH/1+S5I5JfivJjZN8JsnLkjxryaEAAHZio8Osux+75dOHLTUHAMCRsCd2ZQIA7AXCDABgCGEGADCEMAMAGEKYAQAMIcwAAIYQZgAAQwgzAIAhhBkAwBDCDABgCGEGADCEMAMAGEKYAQAMIcwAAIYQZgAAQwgzAIAhhBkAwBDCDABgCGEGADCEMAMAGEKYAQAMIcwAAIYQZgAAQwgzAIAhhBkAwBDCDABgCGEGADCEMAMAGEKYAQAMIcwAAIYQZgAAQ+xbeoCl9RVX5soLL156jI1z819+59IjbJ63fNPSE2ykG7/55KVH2Eif+7FbLT3Cxjn+g59YeoSNdMUXL1x6hM3UB19sixkAwBDCDABgCGEGADCEMAMAGEKYAQAMIcwAAIYQZgAAQwgzAIAhhBkAwBDCDABgCGEGADCEMAMAGEKYAQAMIcwAAIYQZgAAQwgzAIAhhBkAwBDCDABgCGEGADCEMAMAGEKYAQAMIcwAAIYQZgAAQwgzAIAhhBkAwBDCDABgCGEGADCEMAMAGEKYAQAMIcwAAIYQZgAAQwgzAIAhhBkAwBDCDABgCGEGADCEMAMAGEKYAQAMMS7MqupNVfUrVfVzVfU3VfW5qnpSVZ1YVb9UVX9bVZ+oqkevX/+Gqnrutvc4paouqaqHLfNdAADs3LgwW3tkkouS3DvJf07yX5L8TpIPJtmf5EVJfrWqbpHk+Ul+sKpO3PL1P5Dk4iS/dyyHBgC4JqaG2fu7+2nd/aEkP5/kgiSXd/dzuvvDSZ6epJJ8W5L/keTKJN+/5esfn+TF3X35wd68qs6qqnOr6tzL+ytH9RsBADhcU8PsvQcedHcn+WyS921ZdnmSLyS5aXdfmuQlWcVYqupbktwrya8f6s27+5zu3t/d+0+ok47OdwAAsEP7lh7gELZv6epDLDsQlr+a5L1VdeskT0jytu7+wNEdEQDgyJq6xWxHuvv9Sd6R5IlJHpWr2VoGADDV1C1mu/H8JP81qy1rL194FgCAHdsTW8zWXp7ksiSv6O6Llh4GAGCnxm0x6+4zDrLsLgdZdvNti26Y5B8k+bWjMxkAwNE1Lsx2qqpOSHJakmck+ZPu/uOFRwIA2JW9sCvzfkk+ntXFaJ+48CwAALu28VvMuvtNWV1sFgBgo+2FLWYAAHuCMAMAGEKYAQAMIcwAAIYQZgAAQwgzAIAhhBkAwBDCDABgCGEGADCEMAMAGEKYAQAMIcwAAIYQZgAAQwgzAIAhhBkAwBDCDABgCGEGADCEMAMAGEKYAQAMIcwAAIYQZgAAQwgzAIAhhBkAwBDCDABgCGEGADCEMAMAGEKYAQAMIcwAAIbYt/QAi+tOX37Z0lNwbfAn7196go30+QeetPQIG+mB/+ftS4+wcX7nZx609Agb6ZSX/5+lR9hTbDEDABhCmAEADCHMAACGEGYAAEMIMwCAIYQZAMAQwgwAYAhhBgAwhDADABhCmAEADCHMAACGEGYAAEMIMwCAIYQZAMAQwgwAYAhhBgAwhDADABhCmAEADCHMAACGEGYAAEMIMwCAIYQZAMAQwgwAYAhhBgAwhDADABhCmAEADCHMAACGEGYAAEMIMwCAIYQZAMAQwgwAYAhhBgAwhDADABhCmAEADDEyzKrqhVX16u2P158fV1XPq6rPV1VX1RmLDQoAcATtW3qAw/CkJLXl84ckeVySM5J8JMnfLDATAMARNz7MuvuL2xbdPsmnu/utS8wDAHC0jNyVudX23ZpJfiHJrde7MT+2Xl5V9ZNV9VdV9eWqel9VPWq5qQEAdm78FrNtnpTk40ken+Rbk1yxXv7TSR6e5IeT/GWS+yZ5flV9obtfs8SgAAA7tVFh1t1frKqLklzR3ecnSVVdL8mTk3xnd79l/dKPVtW9sgq1q4RZVZ2V5KwkOSnXPSazAwD8fTYqzA7hW5KclOQPqqq3LD8hyccO9gXdfU6Sc5LklDq1D/YaAIBjbS+E2YHj5L4vySe2PXf5MZ4FAGDX9kKYfSDJpUlu091vWHoYAIDd2vgw6+6LqursJGdXVSV5c5KTk9wnyZXr3ZYAAONtfJitPTXJZ5L8eJJfSXJhkvckefaSQwEA7MTIMOvuxx7s8frzs5OcvW1ZJ/n/1h8AABtp/AVmAQCuLYQZAMAQwgwAYAhhBgAwhDADABhCmAEADCHMAACGEGYAAEMIMwCAIYQZAMAQwgwAYAhhBgAwhDADABhCmAEADCHMAACGEGYAAEMIMwCAIYQZAMAQwgwAYAhhBgAwhDADABhCmAEADCHMAACGEGYAAEMIMwCAIYQZAMAQwgwAYAhhBgAwhDADABhCmAEADCHMAACG2Lf0AABX58qvfGXpETbS/3rEPZYeYeOc/6Qrlx5hI11w93stPcJm+rcvP+hiW8wAAIYQZgAAQwgzAIAhhBkAwBDCDABgCGEGADCEMAMAGEKYAQAMIcwAAIYQZgAAQwgzAIAhhBkAwBDCDABgCGEGADCEMAMAGEKYAQAMIcwAAIYQZgAAQwgzAIAhhBkAwBDCDABgCGEGADCEMAMAGEKYAQAMIcwAAIYQZgAAQwgzAIAhhBkAwBDCDABgCGEGADCEMAMAGEKYAQAMIcwAAIYQZgAAQwgzAIAhhBkAwBDCDABgCGEGADDEvqUHWEJVnZXkrCQ5KdddeBoAgJVr5Raz7j6nu/d39/4TcuLS4wAAJLmWhhkAwETCDABgiD0bZlX1I1X1F0vPAQBwuPZsmCW5cZJvWnoIAIDDtWfDrLuf1t219BwAAIdrz4YZAMCmEWYAAEMIMwCAIYQZAMAQwgwAYAhhBgAwhDADABhCmAEADCHMAACGEGYAAEMIMwCAIYQZAMAQwgwAYAhhBgAwhDADABhCmAEADCHMAACGEGYAAEMIMwCAIYQZAMAQwgwAYAhhBgAwhDADABhCmAEADCHMAACGEGYAAEMIMwCAIYQZAMAQwgwAYAhhBgAwhDADABiiunvpGRZ1Sp3a964HLT0GwJFVtfQEG2ffN9xy6RE20mve8eqlR9hIx5/24Xd19/7ty20xAwAYQpgBAAwhzAAAhhBmAABDCDMAgCGEGQDAEMIMAGAIYQYAMIQwAwAYQpgBAAwhzAAAhhBmAABDCDMAgCGEGQDAEMIMAGAIYQYAMIQwAwAYQpgBAAwhzAAAhhBmAABDCDMAgCGEGQDAEMIMAGAIYQYAMIQwAwAYQpgBAAwhzAAAhhBmAABDCDMAgCGEGQDAEMIMAGAIYQYAMIQwAwAYQpgBAAyxMWFWVT9eVR9beg4AgKNlY8IMAGCvOyJhVlWnVNUNj8R77eD3vElVnXQsf08AgKNp12FWVcdX1YOr6jeSnJ/k7uvlN6iqc6rqs1V1UVX9r6rav+XrHltVF1fVg6rqz6rqS1X1xqq63bb3/8mqOn/92hcnOXnbCA9Jcv7697rfbr8PAIApdhxmVXXnqnp2kk8keXmSLyX5riRvrqpK8pokt0zyvUn+cZI3J3lDVZ225W1OTPKUJI9Pct8kN0zyX7f8Ho9I8tNJfirJ6Un+MsmTt43ysiQ/mOT6SV5fVR+uqv+wPfAAADbFYYVZVd2oqn60qs5N8idJ7pTkx5LcrLuf2N1v7u5O8oAk90jy8O5+Z3d/uLufmuQjSR695S33Jfnh9Wvem+TsJA+oqgPz/FiSF3X387r7g939jCTv3DpTd3+1u3+/u38gyc2S/Mz69//Qeivd46tq+1a2A9/PWVV1blWde3kuPZxVAABw1B3uFrN/neQ5SS5Ncofufmh3/1Z3b6+aeya5bpLPrXdBXlxVFye5S5J/tOV1l3b3X275/LwkJ2S15SxJvjnJ27a99/bP/053X9Tdv97dD0jyrUlumuTXkjz8EK8/p7v3d/f+E3Li1XzbAADHzr7DfN05SS5P8pgk76+q307ykiR/1N1XbHndcUk+k+SfHuQ9Ltzy+KvbnustX79jVXViku/JaqvcQ5K8P6utbq/azfsBACzhsEKou8/r7md09zcl+Y4kFyf5b0k+VVU/V1X/eP3Sd2e1W/HK9W7MrR+f3cFcf57kPtuWfd3ntfJPqup5WZ188NwkH05yz+4+vbuf091f2MHvCQCwqB1voerut3f3DyU5LatdnHdM8s6q+qdJ/jDJHyd5VVV9d1XdrqruW1X/cf384XpOkjOr6olVdYeqekqSe297zaOS/M8kpyT5gSS36u6f6O4/2+n3BAAwweHuyryK9fFlr0zyyqq6aZIrurur6iFZnVH5/KyO9fpMVrH24h2898ur6huTPCOrY9Z+N8nPJ3nslpf9UZKbd/eFV30HAIDNU6uTKa+9TqlT+971oKXHADiyqpaeYOPs+4ZbLj3CRnrNO1699Agb6fjTPvyu7t6/fblbMgEADCHMAACGEGYAAEMIMwCAIYQZAMAQwgwAYAhhBgAwhDADABhCmAEADCHMAACGEGYAAEMIMwCAIYQZAMAQwgwAYAhhBgAwhDADABhCmAEADCHMAACGEGYAAEMIMwCAIYQZAMAQwgwAYAhhBgAwhDADABhCmAEADCHMAPBWEeQAAAJwSURBVACGEGYAAEMIMwCAIYQZAMAQwgwAYAhhBgAwxL6lBwDgKOheeoKN89VPfmrpETbSg29xj6VH2FAfPuhSW8wAAIYQZgAAQwgzAIAhhBkAwBDCDABgCGEGADCEMAMAGEKYAQAMIcwAAIYQZgAAQwgzAIAhhBkAwBDCDABgCGEGADCEMAMAGEKYAQAMIcwAAIYQZgAAQwgzAIAhhBkAwBDCDABgCGEGADCEMAMAGEKYAQAMIcwAAIYQZgAAQwgzAIAhhBkAwBDCDABgCGEGADCEMAMAGEKYAQAMIcwAAIYQZgAAQwgzAIAhhBkAwBDCDABgCGEGADDEvqUHWEJVnZXkrCQ5KdddeBoAgJVr5Raz7j6nu/d39/4TcuLS4wAAJLmWhhkAwETCDABgCGEGADCEMAMAGEKYAQAMIcwAAIYQZgAAQwgzAIAhhBkAwBDCDABgCGEGADCEMAMAGEKYAQAMIcwAAIYQZgAAQwgzAIAhhBkAwBDCDABgCGEGADCEMAMAGEKYAQAMIcwAAIYQZgAAQwgzAIAhhBkAwBDCDABgCGEGADCEMAMAGEKYAQAMIcwAAIYQZgAAQwgzAIAhhBkAwBDCDABgCGEGADCEMAMAGEKYAQAMUd299AyLqqrPJfn40nMcwo2TXLD0EBvIets562x3rLfdsd52zjrbncnr7TbdfZPtC6/1YTZZVZ3b3fuXnmPTWG87Z53tjvW2O9bbzllnu7OJ682uTACAIYQZAMAQwmy2c5YeYENZbztnne2O9bY71tvOWWe7s3HrzTFmAABD2GIGADCEMAMAGEKYAQAMIcwAAIYQZgAAQ/z/gj3BisIr4qgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "translate(u'esta es mi vida.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A3LLCx3ZE0Ls"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> ¿ todavia estan en casa ? <end>\n",
      "Predicted translation: are you still home ? <end> \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "translate(u'¿todavia estan en casa?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DUQVLVqUE1YW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> trata de averiguarlo . <end>\n",
      "Predicted translation: go of us out . <end> \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# wrong translation\n",
    "translate(u'trata de averiguarlo.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RTe5P5ioMJwN"
   },
   "source": [
    "## [ 과제 ] 다른 나라 언어로 각자가 실험을 한다 !!\n",
    "\n",
    "### 다음 사이트에 가면 여러나라 언어에 대한 데이터가 있다\n",
    "\n",
    "### 다른 언어 데이터로 앞에서 사용한 코드로 실험을 해본다. \n",
    "\n",
    "### (예)  English to Korean, or Korean to English\n",
    "\n",
    "\n",
    "#### 과제 데이터 얻기 : http://www.manythings.org/anki/ \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ======================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

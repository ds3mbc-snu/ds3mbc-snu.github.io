{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Structure\n",
    "\n",
    "    .\n",
    "    ├── data_in                     # 데이터가 존재하는 영역\n",
    "        ├── ChatBotData.csv         # 전체 데이터\n",
    "        ├── ChatBotData.csv_short   # 축소된 데이터 (테스트 용도)\n",
    "        ├── README.md               # 데이터 저자 READMD 파일\n",
    "    ├── data_out                    # 출력 되는 모든 데이터가 모이는 영역\n",
    "        ├── vocabularyData.voc      # 사전 파일\n",
    "        ├── check_point             # check_point 저장 공간\n",
    "    ├── configs.py                  # 모델 설정에 관한 소스\n",
    "    ├── data.py                     # data 전처리 및 모델에 주입되는 data set 만드는 소스\n",
    "    ├── main.py                     # 전체적인 프로그램이 시작되는 소스\n",
    "    ├── model.py                    # 모델이 들어 있는 소스\n",
    "    └── predict.py                  # 학습된 모델로 실행 해보는 소스   \n",
    "    \n",
    "    \n",
    "# Usage\n",
    "\n",
    "python main.py\n",
    "\n",
    "# Predict\n",
    "\n",
    "python predict.py 남자친구가 너무 잘 생겼어\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 패키지 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# 텐서플로우 2.0 사용 \n",
    "\n",
    "\n",
    "# import tensorflow.compat.v1  as tf\n",
    "\n",
    "\n",
    "# import model as ml == 자체 사용 \n",
    "# import data == 자체 시용 \n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "\n",
    "#from configs import DEFINES == 자체 사용 \n",
    "#########################################\n",
    "\n",
    "\n",
    "print( tf.__version__ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-*- coding: utf-8 -*-\n",
    "#import tensorflow as tf\n",
    "#import tensorflow.compat.v1  as tf\n",
    "\n",
    "\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel') \n",
    "# 주피터에서 커널에 전달하기 위한 프레그 방법\n",
    "\n",
    "\n",
    "tf.app.flags.DEFINE_integer('batch_size', 64, 'batch size') # 배치 크기\n",
    "tf.app.flags.DEFINE_integer('train_steps', 20000, 'train steps') # 학습 에포크\n",
    "tf.app.flags.DEFINE_float('dropout_width', 0.5, 'dropout width') # 드롭아웃 크기\n",
    "\n",
    "\n",
    "tf.app.flags.DEFINE_float('learning_rate', 1e-3, 'learning rate') # 학습률\n",
    "tf.app.flags.DEFINE_integer('shuffle_seek', 1000, 'shuffle random seek') # 셔플 시드값\n",
    "tf.app.flags.DEFINE_integer('max_sequence_length', 25, 'max sequence length') # 시퀀스 길이\n",
    "tf.app.flags.DEFINE_integer('embedding_size', 128, 'embedding size') # 임베딩 크기\n",
    "\n",
    "################################################################\n",
    "\n",
    "tf.app.flags.DEFINE_boolean('tokenize_as_morph', False, 'set morph tokenize') \n",
    "# seq2seq 에서는 True : 형태소에 따른 토크나이징 사용 유무\n",
    "\n",
    "#tf.app.flags.DEFINE_boolean('embedding', True, 'Use Embedding flag') # 임베딩 유무 설정\n",
    "#tf.app.flags.DEFINE_boolean('multilayer', True, 'Use Multi RNN Cell') # 멀티 RNN 유무\n",
    "\n",
    "\n",
    "tf.app.flags.DEFINE_integer('model_hidden_size', 128, 'model weights size') # 가중치 크기\n",
    "tf.app.flags.DEFINE_integer('ffn_hidden_size', 512, 'ffn weights size') # 가중치 크기\n",
    "tf.app.flags.DEFINE_integer('attention_head_size', 4, 'attn head size') # 가중치 크기\n",
    "tf.app.flags.DEFINE_integer('layer_size', 2, 'layer size') # 멀티 레이어 크기 (multi rnn)\n",
    "\n",
    "tf.app.flags.DEFINE_string('data_path', './../data_in/ChatBotData.csv', 'data path') #  데이터 위치\n",
    "tf.app.flags.DEFINE_string('vocabulary_path', './data_outformer/vocabularyData.voc', 'vocabulary path') # 사전 위치\n",
    "tf.app.flags.DEFINE_string('check_point_path', './data_outformer/check_point', 'check point path') # 체크 포인트 위치\n",
    "\n",
    "\n",
    "# Define FLAGS\n",
    "\n",
    "\n",
    "DEFINES = tf.app.flags.FLAGS\n",
    "\n",
    "#############################\n",
    "\n",
    "# import tensorflow.compat.v1  as tf  에서만  tf.app\n",
    "\n",
    "# 여러번 실행시키면 에러 \n",
    "# kernel 을 restart 시킴 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./../data_in/ChatBotData.csv\n"
     ]
    }
   ],
   "source": [
    "# tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
    "# 어떤 경우에는 필요 f 가 필요한다고 하는 경우\n",
    "# ./../ 란 이전 디렉토리를 말함 \n",
    "\n",
    "print( DEFINES.check_point_path )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사전 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.py 파일 \n",
    "\n",
    "\n",
    "from konlpy.tag import Okt # Twitter\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "#import tensorflow as tf\n",
    "#import os\n",
    "#from configs import DEFINES\n",
    "\n",
    "import enum\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "#PAD_MASK = 0\n",
    "#NON_PAD_MASK = 1\n",
    "\n",
    "FILTERS = \"([~.,!?\\\"':;)(])\"\n",
    "PAD = \"<PADDING>\"\n",
    "STD = \"<START>\"\n",
    "END = \"<END>\"\n",
    "UNK = \"<UNKNWON>\"\n",
    "\n",
    "PAD_INDEX = 0\n",
    "STD_INDEX = 1\n",
    "END_INDEX = 2\n",
    "UNK_INDEX = 3\n",
    "\n",
    "MARKER = [PAD, STD, END, UNK]\n",
    "CHANGE_FILTER = re.compile(FILTERS)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_vocabulary():\n",
    "    # 사전을 담을 배열 준비한다.\n",
    "    vocabulary_list = []\n",
    "    # 사전을 구성한 후 파일로 저장 진행한다. \n",
    "    # 그 파일의 존재 유무를 확인한다.\n",
    "    if (not (os.path.exists(DEFINES.vocabulary_path))):\n",
    "        # 이미 생성된 사전 파일이 존재하지 않으므로 \n",
    "        # 데이터를 가지고 만들어야 한다.\n",
    "        # 그래서 데이터가 존재 하면 사전을 만들기 위해서 \n",
    "        # 데이터 파일의 존재 유무를 확인한다.\n",
    "        if (os.path.exists(DEFINES.data_path)):\n",
    "            # 데이터가 존재하니 판단스를 통해서 \n",
    "            # 데이터를 불러오자\n",
    "            data_df = pd.read_csv(DEFINES.data_path, encoding='utf-8')\n",
    "            # 판다스의 데이터 프레임을 통해서 \n",
    "            # 질문과 답에 대한 열을 가져 온다.\n",
    "            question, answer = list(data_df['Q']), list(data_df['A'])\n",
    "            if DEFINES.tokenize_as_morph:  # 형태소에 따른 토크나이져 처리\n",
    "                question = prepro_like_morphlized(question)\n",
    "                \n",
    "                #############################################\n",
    "                \n",
    "                answer = prepro_like_morphlized(answer)\n",
    "                \n",
    "                \n",
    "            data = []\n",
    "            # 질문과 답변을 extend을 \n",
    "            # 통해서 구조가 없는 배열로 만든다.\n",
    "            data.extend(question)\n",
    "            data.extend(answer)\n",
    "            # 토큰나이져 처리 하는 부분이다.\n",
    "            \n",
    "            \n",
    "            words = data_tokenizer(data)\n",
    "            #############################\n",
    "            \n",
    "            \n",
    "            # 공통적인 단어에 대해서는 모두 \n",
    "            # 필요 없으므로 한개로 만들어 주기 위해서\n",
    "            # set해주고 이것들을 리스트로 만들어 준다.\n",
    "            words = list(set(words))\n",
    "            # 데이터 없는 내용중에 MARKER를 사전에 \n",
    "            # 추가 하기 위해서 아래와 같이 처리 한다.\n",
    "            # 아래는 MARKER 값이며 리스트의 첫번째 부터 \n",
    "            # 순서대로 넣기 위해서 인덱스 0에 추가한다.\n",
    "            # PAD = \"<PADDING>\"\n",
    "            # STD = \"<START>\"\n",
    "            # END = \"<END>\"\n",
    "            # UNK = \"<UNKNWON>\"     \n",
    "            words[:0] = MARKER\n",
    "        # 사전 리스트를 사전 파일로 만들어 넣는다.\n",
    "        with open(DEFINES.vocabulary_path, 'w', encoding='utf-8') as vocabulary_file:\n",
    "            for word in words:\n",
    "                vocabulary_file.write(word + '\\n')\n",
    "\n",
    "    # 사전 파일이 존재하면 여기에서 \n",
    "    # 그 파일을 불러서 배열에 넣어 준다.\n",
    "    with open(DEFINES.vocabulary_path, 'r', encoding='utf-8') as vocabulary_file:\n",
    "        for line in vocabulary_file:\n",
    "            vocabulary_list.append(line.strip())\n",
    "\n",
    "    # 배열에 내용을 키와 값이 있는 \n",
    "    # 딕셔너리 구조로 만든다.\n",
    "    char2idx, idx2char = make_vocabulary(vocabulary_list)\n",
    "    # 두가지 형태의 키와 값이 있는 형태를 리턴한다. \n",
    "    # (예) 단어: 인덱스 , 인덱스: 단어)\n",
    "    return char2idx, idx2char, len(char2idx)\n",
    "\n",
    "\n",
    "def make_vocabulary(vocabulary_list):\n",
    "    # 리스트를 키가 단어이고 값이 인덱스인 \n",
    "    # 딕셔너리를 만든다.\n",
    "    char2idx = {char: idx for idx, char in enumerate(vocabulary_list)}\n",
    "    # 리스트를 키가 인덱스이고 값이 단어인 \n",
    "    # 딕셔너리를 만든다.\n",
    "    idx2char = {idx: char for idx, char in enumerate(vocabulary_list)}\n",
    "    # 두개의 딕셔너리를 넘겨 준다.\n",
    "    return char2idx, idx2char\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def data_tokenizer(data):\n",
    "    # 토크나이징 해서 담을 배열 생성\n",
    "    words = []\n",
    "    for sentence in data:\n",
    "        # FILTERS = \"([~.,!?\\\"':;)(])\"\n",
    "        # 위 필터와 같은 값들을 정규화 표현식을 \n",
    "        # 통해서 모두 \"\" 으로 변환 해주는 부분이다.\n",
    "        sentence = re.sub(CHANGE_FILTER, \"\", sentence)\n",
    "        for word in sentence.split():\n",
    "            words.append(word)\n",
    "    # 토그나이징과 정규표현식을 통해 만들어진 \n",
    "    # 값들을 넘겨 준다.\n",
    "    return [word for word in words if word]\n",
    "\n",
    "\n",
    "\n",
    "def prepro_like_morphlized(data):\n",
    "    # 형태소 분석 모듈 객체를\n",
    "    # 생성합니다.\n",
    "\n",
    "    morph_analyzer = Okt()\n",
    "    # 형태소 토크나이즈 결과 문장을 받을\n",
    "    #  리스트를 생성합니다.\n",
    "    result_data = list()\n",
    "    # 데이터에 있는 매 문장에 대해 토크나이즈를\n",
    "    # 할 수 있도록 반복문을 선언합니다.\n",
    "    for seq in tqdm(data):\n",
    "        # Twitter.morphs 함수를 통해 토크나이즈 된\n",
    "        # 리스트 객체를 받고 다시 공백문자를 기준으로\n",
    "        # 하여 문자열로 재구성 해줍니다.\n",
    "        morphlized_seq = \" \".join(morph_analyzer.morphs(seq.replace(' ', '')))\n",
    "        result_data.append(morphlized_seq)\n",
    "\n",
    "    return result_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###############################################################\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_OUT_PATH = './data_outformer/'  # 현재 디렉토리에서의 서브디렉토리\n",
    "\n",
    "data_out_path = os.path.join(os.getcwd(), DATA_OUT_PATH)\n",
    "\n",
    "os.makedirs( data_out_path, exist_ok=True )\n",
    "\n",
    "\n",
    "char2idx,  idx2char, vocabulary_length = load_vocabulary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    # 판다스를 통해서 데이터를 불러온다.\n",
    "    data_df = pd.read_csv(DEFINES.data_path, header=0)\n",
    "    # 질문과 답변 열을 가져와 question과 answer에 넣는다.\n",
    "    question, answer = list(data_df['Q']), list(data_df['A'])\n",
    "    # skleran에서 지원하는 함수를 통해서 학습 셋과 \n",
    "    # 테스트 셋을 나눈다.\n",
    "    train_input, eval_input, train_label, eval_label = train_test_split(question, answer, test_size=0.33,\n",
    "                                                                        random_state=42)\n",
    "    # 그 값을 리턴한다.\n",
    "    return train_input, train_label, eval_input, eval_label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input, train_label, eval_input, eval_label = load_data()\n",
    "\n",
    "\n",
    "###################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 함수\n",
    "\n",
    "    def enc_processing(value, dictionary):\n",
    "    \n",
    "    def dec_input_processing(value, dictionary):\n",
    "    \n",
    "    def dec_target_processing(value, dictionary):\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 인덱스화 할 value와 키가 워드이고 \n",
    "# 값이 인덱스인 딕셔너리를 받는다.\n",
    "def enc_processing(value, dictionary):\n",
    "    # 인덱스 값들을 가지고 있는 \n",
    "    # 배열이다.(누적된다.)\n",
    "    sequences_input_index = []\n",
    "    # 하나의 인코딩 되는 문장의 \n",
    "    # 길이를 가지고 있다.(누적된다.)\n",
    "    sequences_length = []\n",
    "    # 형태소 토크나이징 사용 유무\n",
    "    if DEFINES.tokenize_as_morph:\n",
    "        value = prepro_like_morphlized(value)\n",
    "\n",
    "    # 한줄씩 불어온다.\n",
    "    for sequence in value:\n",
    "        # FILTERS = \"([~.,!?\\\"':;)(])\"\n",
    "        # 정규화를 사용하여 필터에 들어 있는 \n",
    "        # 값들을 \"\" 으로 치환 한다.\n",
    "        sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n",
    "        # 하나의 문장을 인코딩 할때 \n",
    "        # 가지고 있기 위한 배열이다.\n",
    "        sequence_index = []\n",
    "        # 문장을 스페이스 단위로 \n",
    "        # 자르고 있다.\n",
    "        for word in sequence.split():\n",
    "            # 잘려진 단어들이 딕셔너리에 존재 하는지 보고 \n",
    "            # 그 값을 가져와 sequence_index에 추가한다.\n",
    "            if dictionary.get(word) is not None:\n",
    "                sequence_index.extend([dictionary[word]])\n",
    "            # 잘려진 단어가 딕셔너리에 존재 하지 않는 \n",
    "            # 경우 이므로 UNK(2)를 넣어 준다.\n",
    "            else:\n",
    "                sequence_index.extend([dictionary[UNK]])\n",
    "        # 문장 제한 길이보다 길어질 경우 뒤에 토큰을 자르고 있다.\n",
    "        if len(sequence_index) > DEFINES.max_sequence_length:\n",
    "            sequence_index = sequence_index[:DEFINES.max_sequence_length]\n",
    "        # 하나의 문장에 길이를 넣어주고 있다.\n",
    "        sequences_length.append(len(sequence_index))\n",
    "        # max_sequence_length보다 문장 길이가 \n",
    "        # 작다면 빈 부분에 PAD(0)를 넣어준다.\n",
    "        sequence_index += (DEFINES.max_sequence_length - len(sequence_index)) * [dictionary[PAD]]\n",
    "        # 인덱스화 되어 있는 값을 \n",
    "        # sequences_input_index에 넣어 준다.\n",
    "        sequences_input_index.append(sequence_index)\n",
    "    # 인덱스화된 일반 배열을 넘파이 배열로 변경한다. \n",
    "    # 이유는 텐서플로우 dataset에 넣어 주기 위한 \n",
    "    # 사전 작업이다.\n",
    "    # 넘파이 배열에 인덱스화된 배열과 \n",
    "    # 그 길이를 넘겨준다.  \n",
    "    return np.asarray(sequences_input_index), sequences_length\n",
    "\n",
    "\n",
    "# 인덱스화 할 value 키가 워드 이고 값이 \n",
    "# 인덱스인 딕셔너리를 받는다.\n",
    "def dec_output_processing(value, dictionary):\n",
    "    # 인덱스 값들을 가지고 있는 \n",
    "    # 배열이다.(누적된다)\n",
    "    sequences_output_index = []\n",
    "    # 하나의 디코딩 입력 되는 문장의 \n",
    "    # 길이를 가지고 있다.(누적된다)\n",
    "    sequences_length = []\n",
    "    # 형태소 토크나이징 사용 유무\n",
    "    if DEFINES.tokenize_as_morph:\n",
    "        value = prepro_like_morphlized(value)\n",
    "    # 한줄씩 불어온다.\n",
    "    for sequence in value:\n",
    "        # FILTERS = \"([~.,!?\\\"':;)(])\"\n",
    "        # 정규화를 사용하여 필터에 들어 있는 \n",
    "        # 값들을 \"\" 으로 치환 한다.\n",
    "        sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n",
    "        # 하나의 문장을 디코딩 할때 가지고 \n",
    "        # 있기 위한 배열이다.\n",
    "        sequence_index = []\n",
    "        # 디코딩 입력의 처음에는 START가 와야 하므로 \n",
    "        # 그 값을 넣어 주고 시작한다.\n",
    "        # 문장에서 스페이스 단위별로 단어를 가져와서 딕셔너리의 \n",
    "        # 값인 인덱스를 넣어 준다.\n",
    "        sequence_index = [dictionary[STD]] + [dictionary[word] for word in sequence.split()]\n",
    "        # 문장 제한 길이보다 길어질 경우 뒤에 토큰을 자르고 있다.\n",
    "        if len(sequence_index) > DEFINES.max_sequence_length:\n",
    "            sequence_index = sequence_index[:DEFINES.max_sequence_length]\n",
    "        # 하나의 문장에 길이를 넣어주고 있다.\n",
    "        sequences_length.append(len(sequence_index))\n",
    "        # max_sequence_length보다 문장 길이가 \n",
    "        # 작다면 빈 부분에 PAD(0)를 넣어준다.\n",
    "        sequence_index += (DEFINES.max_sequence_length - len(sequence_index)) * [dictionary[PAD]]\n",
    "        # 인덱스화 되어 있는 값을 \n",
    "        # sequences_output_index 넣어 준다.\n",
    "        sequences_output_index.append(sequence_index)\n",
    "    # 인덱스화된 일반 배열을 넘파이 배열로 변경한다. \n",
    "    # 이유는 텐서플로우 dataset에 넣어 주기 위한 \n",
    "    # 사전 작업이다.\n",
    "    # 넘파이 배열에 인덱스화된 배열과 그 길이를 넘겨준다.\n",
    "    return np.asarray(sequences_output_index), sequences_length\n",
    "\n",
    "\n",
    "# 인덱스화 할 value와 키가 워드 이고\n",
    "# 값이 인덱스인 딕셔너리를 받는다.\n",
    "def dec_target_processing(value, dictionary):\n",
    "    # 인덱스 값들을 가지고 있는 \n",
    "    # 배열이다.(누적된다)\n",
    "    sequences_target_index = []\n",
    "    # 형태소 토크나이징 사용 유무\n",
    "    if DEFINES.tokenize_as_morph:\n",
    "        value = prepro_like_morphlized(value)\n",
    "    # 한줄씩 불어온다.\n",
    "    for sequence in value:\n",
    "        # FILTERS = \"([~.,!?\\\"':;)(])\"\n",
    "        # 정규화를 사용하여 필터에 들어 있는 \n",
    "        # 값들을 \"\" 으로 치환 한다.\n",
    "        sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n",
    "        # 문장에서 스페이스 단위별로 단어를 가져와서 \n",
    "        # 딕셔너리의 값인 인덱스를 넣어 준다.\n",
    "        # 디코딩 출력의 마지막에 END를 넣어 준다.\n",
    "        sequence_index = [dictionary[word] for word in sequence.split()]\n",
    "        # 문장 제한 길이보다 길어질 경우 뒤에 토큰을 자르고 있다.\n",
    "        # 그리고 END 토큰을 넣어 준다\n",
    "        if len(sequence_index) >= DEFINES.max_sequence_length:\n",
    "            sequence_index = sequence_index[:DEFINES.max_sequence_length - 1] + [dictionary[END]]\n",
    "        else:\n",
    "            sequence_index += [dictionary[END]]\n",
    "        # max_sequence_length보다 문장 길이가 \n",
    "        # 작다면 빈 부분에 PAD(0)를 넣어준다.\n",
    "        sequence_index += (DEFINES.max_sequence_length - len(sequence_index)) * [dictionary[PAD]]\n",
    "        # 인덱스화 되어 있는 값을 \n",
    "        # sequences_target_index에 넣어 준다.\n",
    "        sequences_target_index.append(sequence_index)\n",
    "    # 인덱스화된 일반 배열을 넘파이 배열로 변경한다. \n",
    "    # 이유는 텐서플로우 dataset에 넣어 주기 위한 사전 작업이다.\n",
    "    # 넘파이 배열에 인덱스화된 배열과 그 길이를 넘겨준다.\n",
    "    return np.asarray(sequences_target_index)\n",
    "\n",
    "\n",
    "\n",
    "###########################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 7921/7921 [00:32<00:00, 246.22it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 7921/7921 [00:27<00:00, 288.21it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 7921/7921 [00:27<00:00, 286.16it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 3902/3902 [00:10<00:00, 380.83it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 3902/3902 [00:12<00:00, 303.15it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 3902/3902 [00:13<00:00, 284.62it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# 훈련셋 인코딩 만드는 부분이다.\n",
    "train_input_enc, train_input_enc_length = enc_processing(train_input, char2idx)\n",
    "\n",
    "# 훈련셋 디코딩 입력 부분 만드는 부분이다.\n",
    "train_output_dec, train_output_dec_length = dec_output_processing(train_label, char2idx) \n",
    "#TODO1 실행 안되어 확인 필요(AttributeError: module 'data' has no attribute 'dec_output_processing)\n",
    "\n",
    "# 훈련셋 디코딩 출력 부분 만드는 부분이다.\n",
    "train_target_dec = dec_target_processing(train_label, char2idx)\n",
    "\n",
    "\n",
    "############################################################\n",
    "\n",
    "\n",
    "# 평가셋 인코딩 만드는 부분이다.\n",
    "eval_input_enc, eval_input_enc_length = enc_processing(eval_input,char2idx)\n",
    "\n",
    "# 평가셋 인코딩 만드는 부분이다.\n",
    "eval_output_dec, eval_output_dec_length = dec_output_processing(eval_label, char2idx)\n",
    "\n",
    "# 평가셋 인코딩 만드는 부분이다.\n",
    "eval_target_dec = dec_target_processing(eval_label, char2idx)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 체크 포인트 경로 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 현재 경로'./'에 현재 경로 하부에 \n",
    "# 체크 포인트를 저장한 디렉토리를 설정한다.\n",
    "\n",
    "check_point_path = os.path.join(os.getcwd(), DEFINES.check_point_path)\n",
    "\n",
    "# 디렉토리를 만드는 함수이며 두번째 인자 exist_ok가 \n",
    "# True이면 디렉토리가 이미 존재해도 OSError가 \n",
    "# 발생하지 않는다.\n",
    "# exist_ok가 False이면 이미 존재하면 \n",
    "# OSError가 발생한다.\n",
    "\n",
    "os.makedirs(check_point_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM 네트워크와 에스티메이터 모델 구성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 클래시파이어  classifier = tf.estimator.Estimator("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 2 22222222222222222222222222\n",
    "\n",
    "#-*- coding: utf-8 -*-\n",
    "#import tensorflow as tf\n",
    "#import sys\n",
    "#from configs import DEFINES\n",
    "\n",
    "\n",
    "def layer_norm(inputs, eps=1e-6):\n",
    "    # LayerNorm(x + Sublayer(x))\n",
    "    feature_shape = inputs.get_shape()[-1:]\n",
    "    #  평균과 표준편차을 넘겨 준다.\n",
    "    mean = tf.keras.backend.mean(inputs, [-1], keepdims=True)\n",
    "    std = tf.keras.backend.std(inputs, [-1], keepdims=True)\n",
    "    beta = tf.Variable(tf.zeros(feature_shape), trainable=False)\n",
    "    gamma = tf.Variable(tf.ones(feature_shape), trainable=False)\n",
    "\n",
    "    return gamma * (inputs - mean) / (std + eps) + beta\n",
    "\n",
    "\n",
    "def sublayer_connection(inputs, sublayer, dropout=0.2):\n",
    "    # LayerNorm(x + Sublayer(x))\n",
    "    outputs = layer_norm(inputs + tf.keras.layers.Dropout(dropout)(sublayer))\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def feed_forward(inputs, num_units):\n",
    "    # FFN(x) = max(0, xW1 + b1)W2 + b2\n",
    "    feature_shape = inputs.get_shape()[-1]\n",
    "    inner_layer = tf.keras.layers.Dense(num_units, activation=tf.nn.relu)(inputs)\n",
    "    outputs = tf.keras.layers.Dense(feature_shape)(inner_layer)\n",
    "\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def positional_encoding(dim, sentence_length):\n",
    "    # Positional Encoding\n",
    "    # paper: https://arxiv.org/abs/1706.03762\n",
    "    # P E(pos,2i) = sin(pos/100002i/dmodel)\n",
    "    # P E(pos,2i+1) = cos(pos/100002i/dmodel)\n",
    "    encoded_vec = np.array([pos / np.power(10000, 2 * i / dim)\n",
    "                            for pos in range(sentence_length) for i in range(dim)])\n",
    "    encoded_vec[::2] = np.sin(encoded_vec[::2])\n",
    "    encoded_vec[1::2] = np.cos(encoded_vec[1::2])\n",
    "    return tf.constant(encoded_vec.reshape([sentence_length, dim]), dtype=tf.float32)\n",
    "\n",
    "\n",
    "def scaled_dot_product_attention(query, key, value, masked=False):\n",
    "    # Attention(Q, K, V ) = softmax(QKt / root dk)V\n",
    "    key_dim_size = float(key.get_shape().as_list()[-1])\n",
    "    key = tf.transpose(key, perm=[0, 2, 1])\n",
    "    outputs = tf.matmul(query, key) / tf.sqrt(key_dim_size)\n",
    "\n",
    "    if masked:\n",
    "        diag_vals = tf.ones_like(outputs[0, :, :])\n",
    "        tril = tf.linalg.LinearOperatorLowerTriangular(diag_vals).to_dense()\n",
    "        masks = tf.tile(tf.expand_dims(tril, 0), [tf.shape(outputs)[0], 1, 1])\n",
    "\n",
    "        paddings = tf.ones_like(masks) * (-2 ** 32 + 1)\n",
    "        outputs = tf.where(tf.equal(masks, 0), paddings, outputs)\n",
    "\n",
    "    attention_map = tf.nn.softmax(outputs)\n",
    "\n",
    "    return tf.matmul(attention_map, value)\n",
    "\n",
    "\n",
    "def multi_head_attention(query, key, value, num_units, heads, masked=False):\n",
    "    query = tf.keras.layers.Dense(num_units, activation=tf.nn.relu)(query)\n",
    "    key = tf.keras.layers.Dense(num_units, activation=tf.nn.relu)(key)\n",
    "    value = tf.keras.layers.Dense(num_units, activation=tf.nn.relu)(value)\n",
    "\n",
    "    query = tf.concat(tf.split(query, heads, axis=-1), axis=0)\n",
    "    key = tf.concat(tf.split(key, heads, axis=-1), axis=0)\n",
    "    value = tf.concat(tf.split(value, heads, axis=-1), axis=0)\n",
    "\n",
    "    attention_map = scaled_dot_product_attention(query, key, value, masked)\n",
    "\n",
    "    attn_outputs = tf.concat(tf.split(attention_map, heads, axis=0), axis=-1)\n",
    "    attn_outputs = tf.keras.layers.Dense(num_units, activation=tf.nn.relu)(attn_outputs)\n",
    "\n",
    "    return attn_outputs\n",
    "\n",
    "\n",
    "def encoder_module(inputs, model_dim, ffn_dim, heads):\n",
    "    self_attn = sublayer_connection(inputs, multi_head_attention(inputs, inputs, inputs,\n",
    "                                                                 model_dim, heads))\n",
    "    outputs = sublayer_connection(self_attn, feed_forward(self_attn, ffn_dim))\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def decoder_module(inputs, encoder_outputs, model_dim, ffn_dim, heads):\n",
    "    masked_self_attn = sublayer_connection(inputs, multi_head_attention(inputs, inputs, inputs,\n",
    "                                                                        model_dim, heads, masked=True))\n",
    "    self_attn = sublayer_connection(masked_self_attn, multi_head_attention(masked_self_attn, encoder_outputs,\n",
    "                                                                           encoder_outputs, model_dim, heads))\n",
    "    outputs = sublayer_connection(self_attn, feed_forward(self_attn, ffn_dim))\n",
    "\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def encoder(inputs, model_dim, ffn_dim, heads, num_layers):\n",
    "    outputs = inputs\n",
    "    for i in range(num_layers):\n",
    "        outputs = encoder_module(outputs, model_dim, ffn_dim, heads)\n",
    "\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def decoder(inputs, encoder_outputs, model_dim, ffn_dim, heads, num_layers):\n",
    "    outputs = inputs\n",
    "    for i in range(num_layers):\n",
    "        outputs = decoder_module(outputs, encoder_outputs, model_dim, ffn_dim, heads)\n",
    "\n",
    "    return outputs\n",
    "\n",
    "\n",
    "############################################################\n",
    "\n",
    "\n",
    "\n",
    "def mlmodel(features, labels, mode, params):\n",
    "    TRAIN = mode == tf.estimator.ModeKeys.TRAIN\n",
    "    EVAL = mode == tf.estimator.ModeKeys.EVAL\n",
    "    PREDICT = mode == tf.estimator.ModeKeys.PREDICT\n",
    "\n",
    "    position_encode = positional_encoding(params['embedding_size'], params['max_sequence_length'])\n",
    "\n",
    "    if params['xavier_initializer']:\n",
    "        embedding_initializer = 'glorot_normal'\n",
    "    else:\n",
    "        embedding_initializer = 'uniform'\n",
    "\n",
    "    embedding = tf.keras.layers.Embedding(params['vocabulary_length'],\n",
    "                                          params['embedding_size'],\n",
    "                                          embeddings_initializer=embedding_initializer)\n",
    "\n",
    "    x_embedded_matrix = embedding(features['input']) + position_encode\n",
    "    y_embedded_matrix = embedding(features['output']) + position_encode\n",
    "\n",
    "    encoder_outputs = encoder(x_embedded_matrix, params['model_hidden_size'], params['ffn_hidden_size'],\n",
    "                              params['attention_head_size'], params['layer_size'])\n",
    "    decoder_outputs = decoder(y_embedded_matrix, encoder_outputs, params['model_hidden_size'],\n",
    "                              params['ffn_hidden_size'],\n",
    "                              params['attention_head_size'], params['layer_size'])\n",
    "\n",
    "    logits = tf.keras.layers.Dense(params['vocabulary_length'])(decoder_outputs)\n",
    "\n",
    "    predict = tf.argmax(logits, 2)\n",
    "\n",
    "    if PREDICT:\n",
    "        predictions = {\n",
    "            'indexs': predict,\n",
    "            'logits': logits,\n",
    "        }\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "\n",
    "    # 정답 차원 변경을 한다. [배치 * max_sequence_length * vocabulary_length]  \n",
    "    # logits과 같은 차원을 만들기 위함이다.\n",
    "    labels_ = tf.one_hot(labels, params['vocabulary_length'])\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=labels_))\n",
    "\n",
    "    accuracy = tf.metrics.accuracy(labels=labels, predictions=predict)\n",
    "\n",
    "    metrics = {'accuracy': accuracy}\n",
    "    tf.summary.scalar('accuracy', accuracy[1])\n",
    "\n",
    "    if EVAL:\n",
    "        return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=metrics)\n",
    "\n",
    "    assert TRAIN\n",
    "\n",
    "    # lrate = d−0.5 *  model · min(step_num−0.5, step_num · warmup_steps−1.5)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=params['learning_rate'])\n",
    "    train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
    "\n",
    "    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': './data_out/check_point', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000238915CAE48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "# 에스티메이터 구성한다.\n",
    "\n",
    "# TODO2: 왜 분류모델이라고 적혀있나요???? s2s이 아닌가요?\n",
    "\n",
    "classifier = tf.estimator.Estimator(\n",
    "    \n",
    "        model_fn = mlmodel, # 모델 등록한다.\n",
    "    \n",
    "        model_dir=DEFINES.check_point_path, # 체크포인트 위치 등록한다.\n",
    "    \n",
    "        params={ # 모델 쪽으로 파라메터 전달한다.\n",
    "            \n",
    "            \n",
    "            'embedding_size': DEFINES.embedding_size,\n",
    "            'model_hidden_size': DEFINES.model_hidden_size,  # 가중치 크기 설정한다.\n",
    "            'ffn_hidden_size': DEFINES.ffn_hidden_size,\n",
    "            'attention_head_size': DEFINES.attention_head_size,\n",
    "            'learning_rate': DEFINES.learning_rate,  # 학습율 설정한다.\n",
    "            'vocabulary_length': vocabulary_length,  # 딕셔너리 크기를 설정한다.\n",
    "            'embedding_size': DEFINES.embedding_size,  # 임베딩 크기를 설정한다.\n",
    "            'layer_size': DEFINES.layer_size,\n",
    "            'max_sequence_length': DEFINES.max_sequence_length,\n",
    "            'xavier_initializer': DEFINES.xavier_initializer\n",
    "            \n",
    "\n",
    "            \n",
    "        })\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"            \n",
    "'hidden_size': DEFINES.hidden_size, # 가중치 크기 설정한다.\n",
    "'layer_size': DEFINES.layer_size, # 멀티 레이어 층 개수를 설정한다.\n",
    "'learning_rate': DEFINES.learning_rate, # 학습율 설정한다. \n",
    "'vocabulary_length': vocabulary_length, # 딕셔너리 크기를 설정한다.\n",
    "'embedding_size': DEFINES.embedding_size, # 임베딩 크기를 설정한다.\n",
    "'embedding': DEFINES.embedding, # 임베딩 사용 유무를 설정한다.\n",
    "'multilayer': DEFINES.multilayer, # 멀티 레이어 사용 유무를 설정한다.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 클래시파이어 만들기와 학습 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rearrange(input, output, target):\n",
    "    features = {\"input\": input, \"output\": output}\n",
    "    return features, target\n",
    "\n",
    "\n",
    "# 학습에 들어가 배치 데이터를 만드는 함수이다.\n",
    "def train_input_fn(train_input_enc, train_output_dec, train_target_dec, batch_size):\n",
    "    # Dataset을 생성하는 부분으로써 from_tensor_slices부분은 \n",
    "    # 각각 한 문장으로 자른다고 보면 된다.\n",
    "    # train_input_enc, train_output_dec, train_target_dec \n",
    "    # 3개를 각각 한문장으로 나눈다.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((train_input_enc, train_output_dec, train_target_dec))\n",
    "    # 전체 데이터를 썩는다.\n",
    "    dataset = dataset.shuffle(buffer_size=len(train_input_enc))\n",
    "    # 배치 인자 값이 없다면  에러를 발생 시킨다.\n",
    "    assert batch_size is not None, \"train batchSize must not be None\"\n",
    "    # from_tensor_slices를 통해 나눈것을 \n",
    "    # 배치크기 만큼 묶어 준다.\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "    # 데이터 각 요소에 대해서 rearrange 함수를 \n",
    "    # 통해서 요소를 변환하여 맵으로 구성한다.\n",
    "    dataset = dataset.map(rearrange)\n",
    "    # repeat()함수에 원하는 에포크 수를 넣을수 있으면 \n",
    "    # 아무 인자도 없다면 무한으로 이터레이터 된다.\n",
    "    dataset = dataset.repeat()\n",
    "    # make_one_shot_iterator를 통해 이터레이터를 \n",
    "    # 만들어 준다.\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    # 이터레이터를 통해 다음 항목의 텐서 \n",
    "    # 개체를 넘겨준다.\n",
    "    return iterator.get_next()\n",
    "\n",
    "\n",
    "\n",
    "#######################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./data_out/check_point\\model.ckpt-3615\n",
      "WARNING:tensorflow:From C:\\WinPython37F\\python-3.7.2.amd64\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py:1069: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file utilities to get mtimes.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 3615 into ./data_out/check_point\\model.ckpt.\n",
      "INFO:tensorflow:loss = 6.9968653, step = 3615\n",
      "INFO:tensorflow:global_step/sec: 0.578642\n",
      "INFO:tensorflow:loss = 1.5761786, step = 3715 (172.833 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.586996\n",
      "INFO:tensorflow:loss = 1.469312, step = 3815 (170.349 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.479299\n",
      "INFO:tensorflow:loss = 1.1975775, step = 3915 (208.653 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3938 into ./data_out/check_point\\model.ckpt.\n",
      "WARNING:tensorflow:From C:\\WinPython37F\\python-3.7.2.amd64\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "INFO:tensorflow:global_step/sec: 0.55214\n",
      "INFO:tensorflow:loss = 1.2724494, step = 4015 (181.109 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.624851\n",
      "INFO:tensorflow:loss = 1.1898525, step = 4115 (160.035 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.562282\n",
      "INFO:tensorflow:loss = 1.2668918, step = 4215 (177.850 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4296 into ./data_out/check_point\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.612516\n",
      "INFO:tensorflow:loss = 1.125241, step = 4315 (163.248 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.590304\n",
      "INFO:tensorflow:loss = 1.2866043, step = 4415 (169.418 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.519219\n",
      "INFO:tensorflow:loss = 1.2689333, step = 4515 (192.599 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4610 into ./data_out/check_point\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.461671\n",
      "INFO:tensorflow:loss = 1.0650374, step = 4615 (216.589 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.452405\n",
      "INFO:tensorflow:loss = 1.2109079, step = 4715 (221.056 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.497904\n",
      "INFO:tensorflow:loss = 1.0156404, step = 4815 (200.832 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4894 into ./data_out/check_point\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.467845\n",
      "INFO:tensorflow:loss = 1.1665304, step = 4915 (213.743 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.47954\n",
      "INFO:tensorflow:loss = 1.0462083, step = 5015 (208.548 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.581159\n",
      "INFO:tensorflow:loss = 1.0856804, step = 5115 (172.067 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5211 into ./data_out/check_point\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.551717\n",
      "INFO:tensorflow:loss = 0.99007845, step = 5215 (181.240 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.61621\n",
      "INFO:tensorflow:loss = 1.0928499, step = 5315 (162.296 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.451245\n",
      "INFO:tensorflow:loss = 1.0703632, step = 5415 (221.600 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5512 into ./data_out/check_point\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.458196\n",
      "INFO:tensorflow:loss = 0.95589286, step = 5515 (218.242 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.448123\n",
      "INFO:tensorflow:loss = 1.0077264, step = 5615 (223.170 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.480341\n",
      "INFO:tensorflow:loss = 1.032801, step = 5715 (208.184 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5791 into ./data_out/check_point\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.468999\n",
      "INFO:tensorflow:loss = 1.0583887, step = 5815 (213.205 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.450057\n",
      "INFO:tensorflow:loss = 1.2160499, step = 5915 (222.207 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.448975\n",
      "INFO:tensorflow:loss = 1.0527442, step = 6015 (222.720 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 6060 into ./data_out/check_point\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.437979\n",
      "INFO:tensorflow:loss = 1.0908692, step = 6115 (228.334 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 6135 into ./data_out/check_point\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.128742\n",
      "INFO:tensorflow:loss = 0.96974725, step = 6215 (776.733 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.971272\n",
      "INFO:tensorflow:loss = 0.99610245, step = 6315 (102.971 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.958503\n",
      "INFO:tensorflow:loss = 1.0209559, step = 6415 (104.316 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.86688\n",
      "INFO:tensorflow:loss = 0.8877976, step = 6515 (115.371 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.858878\n",
      "INFO:tensorflow:loss = 0.9465492, step = 6615 (116.416 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 6645 into ./data_out/check_point\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.836218\n",
      "INFO:tensorflow:loss = 1.0348207, step = 6715 (119.586 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.82534\n",
      "INFO:tensorflow:loss = 1.1309884, step = 6815 (121.178 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.791765\n",
      "INFO:tensorflow:loss = 0.9544152, step = 6915 (126.298 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.772635\n",
      "INFO:tensorflow:loss = 0.9440645, step = 7015 (129.417 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 7116 into ./data_out/check_point\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.719555\n",
      "INFO:tensorflow:loss = 1.0463727, step = 7115 (138.970 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.677399\n",
      "INFO:tensorflow:loss = 0.945754, step = 7215 (147.627 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.658431\n",
      "INFO:tensorflow:loss = 1.0470351, step = 7315 (151.889 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.701134\n",
      "INFO:tensorflow:loss = 1.0110312, step = 7415 (142.625 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.663216\n",
      "INFO:tensorflow:loss = 1.0595026, step = 7515 (150.781 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 7520 into ./data_out/check_point\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.600551\n",
      "INFO:tensorflow:loss = 0.9637656, step = 7615 (166.514 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.651859\n",
      "INFO:tensorflow:loss = 0.8471726, step = 7715 (153.407 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.714927\n",
      "INFO:tensorflow:loss = 0.8933839, step = 7815 (139.863 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.726655\n",
      "INFO:tensorflow:loss = 1.0171839, step = 7915 (137.628 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 7923 into ./data_out/check_point\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.720022\n",
      "INFO:tensorflow:loss = 0.9583315, step = 8015 (138.882 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.786019\n",
      "INFO:tensorflow:loss = 0.83530337, step = 8115 (127.226 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.799106\n",
      "INFO:tensorflow:loss = 0.8602477, step = 8215 (125.137 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.779012\n",
      "INFO:tensorflow:loss = 0.9317649, step = 8315 (128.371 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 8388 into ./data_out/check_point\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.789203\n",
      "INFO:tensorflow:loss = 0.8313315, step = 8415 (126.709 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.785055\n",
      "INFO:tensorflow:loss = 0.78208864, step = 8515 (127.368 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.796515\n",
      "INFO:tensorflow:loss = 0.8543802, step = 8615 (125.547 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.757406\n",
      "INFO:tensorflow:loss = 0.82635045, step = 8715 (132.030 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.731729\n",
      "INFO:tensorflow:loss = 0.8110418, step = 8815 (136.674 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 8848 into ./data_out/check_point\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.727122\n",
      "INFO:tensorflow:loss = 0.8791815, step = 8915 (137.527 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.756712\n",
      "INFO:tensorflow:loss = 0.8465323, step = 9015 (132.157 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.760498\n",
      "INFO:tensorflow:loss = 0.7477864, step = 9115 (131.489 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.754871\n",
      "INFO:tensorflow:loss = 0.8377436, step = 9215 (132.464 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 9295 into ./data_out/check_point\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.692022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.89910007, step = 9315 (144.500 sec)\n"
     ]
    }
   ],
   "source": [
    "# 학습 실행\n",
    "################################################\n",
    "\n",
    "# tf.app.flags.DEFINE_integer('train_steps', 20000, 'train steps') # 학습 에포크\n",
    "\n",
    "\n",
    "classifier.train(input_fn=lambda:  train_input_fn(\n",
    "    train_input_enc, train_output_dec, train_target_dec,  DEFINES.batch_size), steps=DEFINES.train_steps)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이미 학습된 곳에서 다시 시작하는 모습\n",
    "\n",
    "\n",
    "    INFO:tensorflow:Calling model_fn.\n",
    "    INFO:tensorflow:Done calling model_fn.\n",
    "    INFO:tensorflow:Create CheckpointSaverHook.\n",
    "    INFO:tensorflow:Graph was finalized.\n",
    "    INFO:tensorflow:Restoring parameters from ./data_out/check_point\\model.ckpt-3615\n",
    "    WARNING:tensorflow:From C:\\WinPython37F\\python-3.7.2.amd64\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py:1069: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
    "    Instructions for updating:\n",
    "    Use standard file utilities to get mtimes.\n",
    "    INFO:tensorflow:Running local_init_op.\n",
    "    INFO:tensorflow:Done running local_init_op.\n",
    "    INFO:tensorflow:Saving checkpoints for 3615 into ./data_out/check_point\\model.ckpt.\n",
    "    INFO:tensorflow:loss = 6.9968653, step = 3615\n",
    "    INFO:tensorflow:global_step/sec: 0.578642\n",
    "    INFO:tensorflow:loss = 1.5761786, step = 3715 (172.833 sec)\n",
    "    INFO:tensorflow:global_step/sec: 0.586996\n",
    "    INFO:tensorflow:loss = 1.469312, step = 3815 (170.349 sec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 평가 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0207 01:42:59.951724  2876 estimator.py:1147] Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0207 01:43:00.458372  2876 estimator.py:1149] Done calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2020-02-07T01:43:00Z\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0207 01:43:00.494273  2876 evaluation.py:255] Starting evaluation at 2020-02-07T01:43:00Z\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0207 01:43:00.605975  2876 monitored_session.py:240] Graph was finalized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./data_out/check_point\\model.ckpt-3615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0207 01:43:00.612955  2876 saver.py:1284] Restoring parameters from ./data_out/check_point\\model.ckpt-3615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0207 01:43:00.789485  2876 session_manager.py:500] Running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0207 01:43:00.825388  2876 session_manager.py:502] Done running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished evaluation at 2020-02-07-01:43:31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0207 01:43:31.492872  2876 evaluation.py:275] Finished evaluation at 2020-02-07-01:43:31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving dict for global step 3615: accuracy = 0.8097078, global_step = 3615, loss = 1.4880902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0207 01:43:31.493871  2876 estimator.py:2049] Saving dict for global step 3615: accuracy = 0.8097078, global_step = 3615, loss = 1.4880902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 3615: ./data_out/check_point\\model.ckpt-3615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0207 01:43:31.649451  2876 estimator.py:2109] Saving 'checkpoint_path' summary for global step 3615: ./data_out/check_point\\model.ckpt-3615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EVAL set accuracy: 0.810\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def rearrange(input, output, target):\n",
    "    features = {\"input\": input, \"output\": output}\n",
    "    return features, target\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 평가에 들어가 배치 데이터를 만드는 함수이다.\n",
    "def eval_input_fn(eval_input_enc, eval_output_dec, eval_target_dec, batch_size):\n",
    "    # Dataset을 생성하는 부분으로써 from_tensor_slices부분은 \n",
    "    # 각각 한 문장으로 자른다고 보면 된다.\n",
    "    # eval_input_enc, eval_output_dec, eval_target_dec \n",
    "    # 3개를 각각 한문장으로 나눈다.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((eval_input_enc, eval_output_dec, eval_target_dec))\n",
    "    # 전체 데이터를 섞는다.\n",
    "    dataset = dataset.shuffle(buffer_size=len(eval_input_enc))\n",
    "    # 배치 인자 값이 없다면  에러를 발생 시킨다.\n",
    "    assert batch_size is not None, \"eval batchSize must not be None\"\n",
    "    # from_tensor_slices를 통해 나눈것을 \n",
    "    # 배치크기 만큼 묶어 준다.\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "    # 데이터 각 요소에 대해서 rearrange 함수를 \n",
    "    # 통해서 요소를 변환하여 맵으로 구성한다.\n",
    "    \n",
    "    dataset = dataset.map(rearrange)\n",
    "    ################################\n",
    "    \n",
    "    # repeat()함수에 원하는 에포크 수를 넣을수 있으면 \n",
    "    # 아무 인자도 없다면 무한으로 이터레이터 된다.\n",
    "    # 평가이므로 1회만 동작 시킨다.\n",
    "    dataset = dataset.repeat(1)\n",
    "    # make_one_shot_iterator를 통해 \n",
    "    # 이터레이터를 만들어 준다.\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    # 이터레이터를 통해 다음 항목의 \n",
    "    # 텐서 개체를 넘겨준다.\n",
    "    return iterator.get_next()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "eval_result = classifier.evaluate(input_fn=lambda: eval_input_fn(\n",
    "    eval_input_enc, eval_output_dec, eval_target_dec,  DEFINES.batch_size))\n",
    "\n",
    "print('\\nEVAL set accuracy: {accuracy:0.3f}\\n'.format(**eval_result))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 테스트 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코딩 데이터를 만드는 함수이며 \n",
    "# 인덱스화 할 value와 키가 단어이고 값이 인덱스인 딕셔너리를 받아\n",
    "# 넘파이 배열에 인덱스화된 배열과 그 길이를 넘겨준다.  \n",
    "\n",
    "\n",
    "# 인덱스화 할 value와 키가 워드이고 \n",
    "# 값이 인덱스인 딕셔너리를 받는다.\n",
    "\n",
    "\n",
    "def enc_processing(value, dictionary):\n",
    "    # 인덱스 값들을 가지고 있는 \n",
    "    # 배열이다.(누적된다.)\n",
    "    sequences_input_index = []\n",
    "    # 하나의 인코딩 되는 문장의 \n",
    "    # 길이를 가지고 있다.(누적된다.)\n",
    "    sequences_length = []\n",
    "    # 형태소 토크나이징 사용 유무\n",
    "    if DEFINES.tokenize_as_morph:\n",
    "        value = prepro_like_morphlized(value)\n",
    "\n",
    "    # 한줄씩 불어온다.\n",
    "    for sequence in value:\n",
    "        # FILTERS = \"([~.,!?\\\"':;)(])\"\n",
    "        # 정규화를 사용하여 필터에 들어 있는 \n",
    "        # 값들을 \"\" 으로 치환 한다.\n",
    "        sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n",
    "        # 하나의 문장을 인코딩 할때 \n",
    "        # 가지고 있기 위한 배열이다.\n",
    "        sequence_index = []\n",
    "        # 문장을 스페이스 단위로 \n",
    "        # 자르고 있다.\n",
    "        for word in sequence.split():\n",
    "            # 잘려진 단어들이 딕셔너리에 존재 하는지 보고 \n",
    "            # 그 값을 가져와 sequence_index에 추가한다.\n",
    "            if dictionary.get(word) is not None:\n",
    "                sequence_index.extend([dictionary[word]])\n",
    "            # 잘려진 단어가 딕셔너리에 존재 하지 않는 \n",
    "            # 경우 이므로 UNK(2)를 넣어 준다.\n",
    "            else:\n",
    "                sequence_index.extend([dictionary[UNK]])\n",
    "        # 문장 제한 길이보다 길어질 경우 뒤에 토큰을 자르고 있다.\n",
    "        if len(sequence_index) > DEFINES.max_sequence_length:\n",
    "            sequence_index = sequence_index[:DEFINES.max_sequence_length]\n",
    "        # 하나의 문장에 길이를 넣어주고 있다.\n",
    "        sequences_length.append(len(sequence_index))\n",
    "        # max_sequence_length보다 문장 길이가 \n",
    "        # 작다면 빈 부분에 PAD(0)를 넣어준다.\n",
    "        sequence_index += (DEFINES.max_sequence_length - len(sequence_index)) * [dictionary[PAD]]\n",
    "        # 인덱스화 되어 있는 값을 \n",
    "        # sequences_input_index에 넣어 준다.\n",
    "        sequences_input_index.append(sequence_index)\n",
    "    # 인덱스화된 일반 배열을 넘파이 배열로 변경한다. \n",
    "    # 이유는 텐서플로우 dataset에 넣어 주기 위한 \n",
    "    # 사전 작업이다.\n",
    "    # 넘파이 배열에 인덱스화된 배열과 \n",
    "    # 그 길이를 넘겨준다.  \n",
    "    return np.asarray(sequences_input_index), sequences_length\n",
    "\n",
    "\n",
    "# 인덱스화 할 value 키가 워드 이고 값이 \n",
    "# 인덱스인 딕셔너리를 받는다.\n",
    "def dec_output_processing(value, dictionary):\n",
    "    # 인덱스 값들을 가지고 있는 \n",
    "    # 배열이다.(누적된다)\n",
    "    sequences_output_index = []\n",
    "    # 하나의 디코딩 입력 되는 문장의 \n",
    "    # 길이를 가지고 있다.(누적된다)\n",
    "    sequences_length = []\n",
    "    # 형태소 토크나이징 사용 유무\n",
    "    if DEFINES.tokenize_as_morph:\n",
    "        value = prepro_like_morphlized(value)\n",
    "    # 한줄씩 불어온다.\n",
    "    for sequence in value:\n",
    "        # FILTERS = \"([~.,!?\\\"':;)(])\"\n",
    "        # 정규화를 사용하여 필터에 들어 있는 \n",
    "        # 값들을 \"\" 으로 치환 한다.\n",
    "        sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n",
    "        # 하나의 문장을 디코딩 할때 가지고 \n",
    "        # 있기 위한 배열이다.\n",
    "        sequence_index = []\n",
    "        # 디코딩 입력의 처음에는 START가 와야 하므로 \n",
    "        # 그 값을 넣어 주고 시작한다.\n",
    "        # 문장에서 스페이스 단위별로 단어를 가져와서 딕셔너리의 \n",
    "        # 값인 인덱스를 넣어 준다.\n",
    "        sequence_index = [dictionary[STD]] + [dictionary[word] for word in sequence.split()]\n",
    "        # 문장 제한 길이보다 길어질 경우 뒤에 토큰을 자르고 있다.\n",
    "        if len(sequence_index) > DEFINES.max_sequence_length:\n",
    "            sequence_index = sequence_index[:DEFINES.max_sequence_length]\n",
    "        # 하나의 문장에 길이를 넣어주고 있다.\n",
    "        sequences_length.append(len(sequence_index))\n",
    "        # max_sequence_length보다 문장 길이가 \n",
    "        # 작다면 빈 부분에 PAD(0)를 넣어준다.\n",
    "        sequence_index += (DEFINES.max_sequence_length - len(sequence_index)) * [dictionary[PAD]]\n",
    "        # 인덱스화 되어 있는 값을 \n",
    "        # sequences_output_index 넣어 준다.\n",
    "        sequences_output_index.append(sequence_index)\n",
    "    # 인덱스화된 일반 배열을 넘파이 배열로 변경한다. \n",
    "    # 이유는 텐서플로우 dataset에 넣어 주기 위한 \n",
    "    # 사전 작업이다.\n",
    "    # 넘파이 배열에 인덱스화된 배열과 그 길이를 넘겨준다.\n",
    "    return np.asarray(sequences_output_index), sequences_length\n",
    "\n",
    "\n",
    "# 인덱스화 할 value와 키가 워드 이고\n",
    "# 값이 인덱스인 딕셔너리를 받는다.\n",
    "def dec_target_processing(value, dictionary):\n",
    "    # 인덱스 값들을 가지고 있는 \n",
    "    # 배열이다.(누적된다)\n",
    "    sequences_target_index = []\n",
    "    # 형태소 토크나이징 사용 유무\n",
    "    if DEFINES.tokenize_as_morph:\n",
    "        value = prepro_like_morphlized(value)\n",
    "    # 한줄씩 불어온다.\n",
    "    for sequence in value:\n",
    "        # FILTERS = \"([~.,!?\\\"':;)(])\"\n",
    "        # 정규화를 사용하여 필터에 들어 있는 \n",
    "        # 값들을 \"\" 으로 치환 한다.\n",
    "        sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n",
    "        # 문장에서 스페이스 단위별로 단어를 가져와서 \n",
    "        # 딕셔너리의 값인 인덱스를 넣어 준다.\n",
    "        # 디코딩 출력의 마지막에 END를 넣어 준다.\n",
    "        sequence_index = [dictionary[word] for word in sequence.split()]\n",
    "        # 문장 제한 길이보다 길어질 경우 뒤에 토큰을 자르고 있다.\n",
    "        # 그리고 END 토큰을 넣어 준다\n",
    "        if len(sequence_index) >= DEFINES.max_sequence_length:\n",
    "            sequence_index = sequence_index[:DEFINES.max_sequence_length - 1] + [dictionary[END]]\n",
    "        else:\n",
    "            sequence_index += [dictionary[END]]\n",
    "        # max_sequence_length보다 문장 길이가 \n",
    "        # 작다면 빈 부분에 PAD(0)를 넣어준다.\n",
    "        sequence_index += (DEFINES.max_sequence_length - len(sequence_index)) * [dictionary[PAD]]\n",
    "        # 인덱스화 되어 있는 값을 \n",
    "        # sequences_target_index에 넣어 준다.\n",
    "        sequences_target_index.append(sequence_index)\n",
    "    # 인덱스화된 일반 배열을 넘파이 배열로 변경한다. \n",
    "    # 이유는 텐서플로우 dataset에 넣어 주기 위한 사전 작업이다.\n",
    "    # 넘파이 배열에 인덱스화된 배열과 그 길이를 넘겨준다.\n",
    "    return np.asarray(sequences_target_index)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######################################################################################\n",
    "\n",
    "\n",
    "\n",
    "# 인덱스를 스트링으로 변경하는 함수이다.\n",
    "# 바꾸고자 하는 인덱스 value와 인덱스를 \n",
    "# 키로 가지고 있고 값으로 단어를 가지고 있는 \n",
    "# 딕셔너리를 받는다.\n",
    "def pred2string(value, dictionary):\n",
    "    # 텍스트 문장을 보관할 배열을 선언한다.\n",
    "    sentence_string = []\n",
    "    print(value)\n",
    "    # 인덱스 배열 하나를 꺼내서 v에 넘겨준다.\n",
    "    for v in value:\n",
    "        # 딕셔너리에 있는 단어로 변경해서 배열에 담는다.\n",
    "        print(v['indexs'])\n",
    "        for index in v['indexs']:\n",
    "            print(index)\n",
    "        sentence_string = [dictionary[index] for index in v['indexs']]\n",
    "\n",
    "    print(\"***********************\")\n",
    "    print(sentence_string)\n",
    "    print(\"***********************\")\n",
    "    answer = \"\"\n",
    "    # 패딩값도 담겨 있으므로 패딩은 모두 스페이스 처리 한다.\n",
    "    for word in sentence_string:\n",
    "        if word not in PAD and word not in END:\n",
    "            answer += word\n",
    "            answer += \" \"\n",
    "    # 결과를 출력한다.\n",
    "    print(answer)\n",
    "    return answer\n",
    "\n",
    "\n",
    "def pred_next_string(value, dictionary):\n",
    "    # 텍스트 문장을 보관할 배열을 선언한다.\n",
    "    sentence_string = []\n",
    "    is_finished = False\n",
    "\n",
    "    # 인덱스 배열 하나를 꺼내서 v에 넘겨준다.\n",
    "    for v in value:\n",
    "        # 딕셔너리에 있는 단어로 변경해서 배열에 담는다.\n",
    "        sentence_string = [dictionary[index] for index in v['indexs']]\n",
    "\n",
    "    answer = \"\"\n",
    "    # 패딩값도 담겨 있으므로 패딩은 모두 스페이스 처리 한다.\n",
    "    for word in sentence_string:\n",
    "        if word == END:\n",
    "            is_finished = True\n",
    "            break\n",
    "\n",
    "        if word != PAD and word != END:\n",
    "            answer += word\n",
    "            answer += \" \"\n",
    "\n",
    "    # 결과를 출력한다.\n",
    "    return answer, is_finished\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 27.10it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 200.58it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 501.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "I0207 01:43:31.795064  2876 estimator.py:1147] Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0207 01:43:32.876172  2876 estimator.py:1149] Done calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0207 01:43:32.997846  2876 monitored_session.py:240] Graph was finalized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./data_out/check_point\\model.ckpt-3615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0207 01:43:33.003829  2876 saver.py:1284] Restoring parameters from ./data_out/check_point\\model.ckpt-3615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0207 01:43:33.111541  2876 session_manager.py:500] Running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0207 01:43:33.131487  2876 session_manager.py:502] Done running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['마음', '이', '이', '<PADDING>', '<PADDING>', '<PADDING>', '<PADDING>', '<PADDING>', '<PADDING>', '<PADDING>', '<PADDING>', '<PADDING>', '<PADDING>', '<PADDING>', '<PADDING>', '<PADDING>', '<PADDING>', '<PADDING>', '<PADDING>', '<PADDING>', '<PADDING>', '<PADDING>', '<PADDING>', '<PADDING>', '<PADDING>']\n",
      "마음 이 이 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'마음 이 이 '"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "input = \"가끔 궁금해\"\n",
    "\n",
    "\n",
    "\n",
    "# 테스트용 데이터 만드는 부분이다.\n",
    "# 인코딩 부분 만든다.\n",
    "predic_input_enc, predic_input_enc_length = enc_processing([input], char2idx)\n",
    "\n",
    "\n",
    "# 학습 과정이 아니므로 디코딩 입력은 \n",
    "# 존재하지 않는다.(구조를 맞추기 위해 넣는다.)\n",
    "\n",
    "predic_output_dec, predic_output_decLength = dec_output_processing([\"\"], char2idx)       \n",
    "# 학습 과정이 아니므로 디코딩 출력 부분도 \n",
    "# 존재하지 않는다.(구조를 맞추기 위해 넣는다.)\n",
    "\n",
    "\n",
    "predic_target_dec = dec_target_processing([\"\"], char2idx)      \n",
    "\n",
    "# 예측을 하는 부분이다.\n",
    "\n",
    "\n",
    "predictions = classifier.predict(\n",
    "    input_fn=lambda:  eval_input_fn(predic_input_enc, predic_output_dec, predic_target_dec, 1 ))\n",
    "\n",
    "#####################################################  predic_target_dec, DEFINES.batch_size ))\n",
    "\n",
    "# 예측한 값을 인지 할 수 있도록 \n",
    "# 텍스트로 변경하는 부분이다.\n",
    "\n",
    "answer, finished = pred_next_string(predictions, idx2char)\n",
    "\n",
    "\n",
    "print(\"answer: \", answer)\n",
    "########################\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

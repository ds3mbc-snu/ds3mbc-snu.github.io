{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 바로 아래를 먼저 실행시키고, 바로 아래의 주소를 클릭 !!\n",
    "\n",
    "# 여기 주소를 클릭  :  http://localhost:5000  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python  web_babi.py \n",
    "\n",
    "\n",
    "# localhost:5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 바로 위는 웹 기반의 바비 환경을 보여준다\n",
    "\n",
    "<p>\n",
    "    \n",
    "# 또한 각 story 지문에 대한 attention 값을 보여준다\n",
    " \n",
    "<p>\n",
    "        \n",
    "# ============================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학기말 과제: QA 질문답변 등등 교육용 챗봇을 설계하자 !!\n",
    "\n",
    "<p> &nbsp;\n",
    "    \n",
    "# ============================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI 융합수학교육 (수학교육 응용 + 데이터 수집 + 선처리 +  AI 코딩) \n",
    "\n",
    "\n",
    "<p> &nbsp;\n",
    "    \n",
    "    \n",
    "# 학기말 과제 힌트 : 0.ipynb 와 여기의 web_babi.ipynb 그리고 \n",
    "    \n",
    "# 4.ipynb 코드를 기반으로,  교육용 데이터가 중심인 챗봇 설계 !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras 코딩의 단계 : 예를 들어,  손글씨 숫자 알아내기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18000 samples, validate on 42000 samples\n",
      "Epoch 1/5\n",
      "18000/18000 [==============================] - 3s 181us/step - loss: 1.2557 - accuracy: 0.6849 - val_loss: 0.7199 - val_accuracy: 0.8213\n",
      "Epoch 2/5\n",
      "18000/18000 [==============================] - 3s 188us/step - loss: 0.5625 - accuracy: 0.8538 - val_loss: 0.5100 - val_accuracy: 0.8621\n",
      "Epoch 3/5\n",
      "18000/18000 [==============================] - 3s 194us/step - loss: 0.4365 - accuracy: 0.8809 - val_loss: 0.4307 - val_accuracy: 0.8814\n",
      "Epoch 4/5\n",
      "18000/18000 [==============================] - 4s 197us/step - loss: 0.3812 - accuracy: 0.8936 - val_loss: 0.3899 - val_accuracy: 0.8899\n",
      "Epoch 5/5\n",
      "18000/18000 [==============================] - 3s 192us/step - loss: 0.3482 - accuracy: 0.9028 - val_loss: 0.3667 - val_accuracy: 0.8963\n",
      "10000/10000 [==============================] - 0s 44us/step\n",
      "\n",
      "loss_and_metrics : [0.33985268929600715, 0.90420001745224]\n"
     ]
    }
   ],
   "source": [
    "# 0. 사용할 패키지 불러오기\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "import numpy as np\n",
    "from numpy import argmax\n",
    "\n",
    "# 1. 데이터셋 생성하기\n",
    "\n",
    "# 훈련셋과 시험셋 불러오기\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# 데이터셋 전처리\n",
    "x_train = x_train.reshape(60000, 784).astype('float32') / 255.0\n",
    "x_test = x_test.reshape(10000, 784).astype('float32') / 255.0\n",
    "\n",
    "# 원핫인코딩 (one-hot encoding) 처리\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "# 훈련셋과 검증셋 분리\n",
    "x_val = x_train[:42000]   # 데이터셋의 70%를 훈련셋/학습셋으로 사용\n",
    "x_train = x_train[42000:] # 데이터셋의 30%를 검증셋으로 사용\n",
    "y_val = y_train[:42000]   # 데이터셋의 70%를 훈련셋/학습셋으로 사용\n",
    "y_train = y_train[42000:] # 데이터셋의 30%를 검증셋으로 사용\n",
    "\n",
    "# 2. 모델 구성하기\n",
    "model = Sequential()\n",
    "model.add(Dense(units=64, input_dim=28*28, activation='relu'))\n",
    "model.add(Dense(units=10, activation='softmax'))\n",
    "\n",
    "# 3. 모델 학습과정 설정하기\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "# 4. 모델 학습시키기\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=32, validation_data=(x_val, y_val))\n",
    "\n",
    "# 5. 모델 평가하기\n",
    "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=32)\n",
    "print('')\n",
    "print('loss_and_metrics : ' + str(loss_and_metrics))\n",
    "\n",
    "# 6. 모델 저장하기\n",
    "from keras.models import load_model\n",
    "model.save('./data_babi/mnist_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras 학습된 모델 불러와 다시 쓰기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "[4 4 4 9 9]\n",
      "Train on 18000 samples, validate on 42000 samples\n",
      "Epoch 1/5\n",
      "18000/18000 [==============================] - 4s 231us/step - loss: 0.2602 - accuracy: 0.9263 - val_loss: 0.2959 - val_accuracy: 0.9174\n",
      "Epoch 2/5\n",
      "18000/18000 [==============================] - 4s 211us/step - loss: 0.2515 - accuracy: 0.9291 - val_loss: 0.2856 - val_accuracy: 0.9195\n",
      "Epoch 3/5\n",
      "18000/18000 [==============================] - 4s 237us/step - loss: 0.2435 - accuracy: 0.9322 - val_loss: 0.2832 - val_accuracy: 0.9203\n",
      "Epoch 4/5\n",
      "18000/18000 [==============================] - 4s 232us/step - loss: 0.2363 - accuracy: 0.9334 - val_loss: 0.2748 - val_accuracy: 0.9220\n",
      "Epoch 5/5\n",
      "18000/18000 [==============================] - 4s 228us/step - loss: 0.2293 - accuracy: 0.9357 - val_loss: 0.2695 - val_accuracy: 0.9242\n",
      "10000/10000 [==============================] - 0s 38us/step\n",
      "\n",
      "loss_and_metrics : [0.2469834449470043, 0.9290000200271606]\n",
      "True : 4, Predict : 4\n",
      "True : 4, Predict : 4\n",
      "True : 4, Predict : 4\n",
      "True : 9, Predict : 9\n",
      "True : 9, Predict : 9\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "import numpy as np\n",
    "from numpy import argmax\n",
    "\n",
    "\n",
    "# 2-0. 실무에 사용할 데이터 준비하기\n",
    "\n",
    "# 훈련셋과 시험셋 불러오기\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# 데이터셋 전처리\n",
    "x_train = x_train.reshape(60000, 784).astype('float32') / 255.0\n",
    "x_test = x_test.reshape(10000, 784).astype('float32') / 255.0\n",
    "\n",
    "# 원핫인코딩 (one-hot encoding) 처리\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "# 훈련셋과 검증셋 분리\n",
    "x_val = x_train[:42000]   # 데이터셋의 70%를 훈련셋/학습셋으로 사용\n",
    "x_train = x_train[42000:] # 데이터셋의 30%를 검증셋으로 사용\n",
    "y_val = y_train[:42000]   # 데이터셋의 70%를 훈련셋/학습셋으로 사용\n",
    "y_train = y_train[42000:] # 데이터셋의 30%를 검증셋으로 사용\n",
    "\n",
    "###########################################\n",
    "\n",
    "\n",
    "# 2-1. 모델 불러오기\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('./data_babi/mnist_model_15.h5')\n",
    "\n",
    "\n",
    "# 2-2. 모델 다시 사용하기\n",
    "\n",
    "xhat_idx = np.random.choice(x_test.shape[0], 5)\n",
    "xhat = x_test[xhat_idx]\n",
    "zhat = y_test[xhat_idx]\n",
    "yhat = model.predict_classes(xhat)\n",
    "print( zhat )\n",
    "print( yhat )\n",
    "\n",
    "#######################\n",
    "\n",
    "\n",
    "# 2-3. 모델 학습과정 설정하기\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "# 2-4. 모델 학습 플러스 알파 더 시키기\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=32, validation_data=(x_val, y_val))\n",
    "\n",
    "\n",
    "\n",
    "# 2-5. 모델 평가하기\n",
    "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=32)\n",
    "print('')\n",
    "print('loss_and_metrics : ' + str(loss_and_metrics))\n",
    "\n",
    "# 2-6. 모델 저장하기\n",
    "from keras.models import load_model\n",
    "model.save('./data_babi/mnist_model.h5')\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    print('True : ' + str(argmax(y_test[xhat_idx[i]])) + ', Predict : ' + str(yhat[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 위의 예제를 바탕으로, Babi 를 탐구하자 !!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==============================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras 코딩 : Babi 프로젝트 20개 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 바비 데이터 github  : https://github.com/andri27-ts/bAbI\n",
    "\n",
    "<p>\n",
    "\n",
    "\n",
    "# 수학교육 탐구과제로 도전가능한 문제 타입은 무엇인가 ??    \n",
    "    \n",
    "### 바비 데이터 : https://github.com/harvardnlp/MemN2N/tree/master/babi_data/en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 아래의 Keras 코드는 같은 모양의 코드로, 각각  바비의 1번 ,\n",
    "<p>\n",
    "    \n",
    "# 6번, 2번 문제를 다룬다. 여기의 같은 모양인 3개의 코드를\n",
    "    \n",
    "<p>\n",
    "    \n",
    "# (사실은 1개) 에 각 단계에 대한 주석을 달아 서로 나누자 !!\n",
    "    \n",
    "<p> \n",
    "    \n",
    "    \n",
    "# =================== 과제 ======================\n",
    "    \n",
    "<p>\n",
    "<font color=blue>\n",
    "    \n",
    "# [[ 과제 공지 ]] 아래 과제에 대해 각자 준비하여 4월 첫주에 제출한다\n",
    "    \n",
    "<p>\n",
    "    \n",
    "# 다른 17개의 타입에 대해 각자 하나씩 잡아 ipynb 코딩으로 탐구하고, \n",
    "    \n",
    "<p>\n",
    "    \n",
    "# 한글화 및 교육에의 적용 가능성을 ipynb 주석으로 작성해서 제출 !! \n",
    "    \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Trains a memory network on the facebook bAbI dataset for Question/Answering System.\n",
    "\n",
    "'''\n",
    "\n",
    "# 단계 1 : 바비 탐구의 라이브러리를 import 한다\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Input, Activation, Dense, Permute, Dropout\n",
    "from keras.layers import add, dot, concatenate\n",
    "from keras.layers import LSTM, GRU\n",
    "\n",
    "#from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import backend as K\n",
    "\n",
    "from functools import reduce\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting stories for the challenge: single_supporting_fact_10k\n",
      "-\n",
      "Vocab size: 22 unique words\n",
      "Story max length: 68 words\n",
      "Query max length: 4 words\n",
      "Number of training stories: 10000\n",
      "Number of test stories: 1000\n",
      "-\n",
      "Here's what a \"story\" tuple looks like (input, query, answer):\n",
      "(['Mary', 'moved', 'to', 'the', 'bathroom', '.', 'John', 'went', 'to', 'the', 'hallway', '.'], ['Where', 'is', 'Mary', '?'], 'bathroom')\n",
      "-\n",
      "Vectorizing the word sequences...\n",
      "-\n",
      "inputs: integer tensor of shape (samples, max_length)\n",
      "inputs_train shape: (10000, 68)\n",
      "inputs_test shape: (1000, 68)\n",
      "-\n",
      "queries: integer tensor of shape (samples, max_length)\n",
      "queries_train shape: (10000, 4)\n",
      "queries_test shape: (1000, 4)\n",
      "-\n",
      "answers: binary (1 or 0) tensor of shape (samples, vocab_size)\n",
      "answers_train shape: (10000, 22)\n",
      "answers_test shape: (1000, 22)\n",
      "-\n",
      "Compiling...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def tokenize(sent):\n",
    "    '''\n",
    "    Return the tokens of a sentence including punctuation.\n",
    "    >>> tokenize('Bob dropped the apple. Where is the apple?')\n",
    "    ['Bob', 'dropped', 'the', 'apple', '.', 'Where', 'is', 'the', 'apple', '?']\n",
    "    '''\n",
    "    # 정규표현식\n",
    "    # \\W: 단어를 만들 수 있는 문자(a-zA-Z,_,0-9)를 제외한 모든 글자.\n",
    "    # +: 1개 이상 등장\n",
    "    # String.strip(): 양 끝의 \\n과 공백 삭제\n",
    "    return [x.strip() for x in re.split('(\\W+)+', sent) if x.strip()]\n",
    "\n",
    "\n",
    "def parse_stories(lines, only_supporting=False):\n",
    "    '''\n",
    "    Parse stories provided in the bAbi tasks format\n",
    "    If only_supporting is true, only the sentences\n",
    "    that support the answer are kept.\n",
    "    '''\n",
    "    data = []\n",
    "    story = []\n",
    "    for line in lines:\n",
    "        line = line.decode('utf-8').strip()\n",
    "        # String.split(separator, maxsplit): maxsplit번 만큼만 자른다\n",
    "        nid, line = line.split(' ', 1)\n",
    "        nid = int(nid)\n",
    "        # whenever new story starts, initialize stored story\n",
    "        if nid == 1:\n",
    "            story = []\n",
    "        \n",
    "        if '\\t' in line:\n",
    "            q, a, supporting = line.split('\\t')\n",
    "            q = tokenize(q)\n",
    "            substory = None\n",
    "            if only_supporting:\n",
    "                # Only select the related substory\n",
    "                supporting = map(int, supporting.split())\n",
    "                substory = [story[i - 1] for i in supporting]\n",
    "            else:\n",
    "                # Provide all the substories\n",
    "                substory = [x for x in story if x]\n",
    "            data.append((substory, q, a))\n",
    "            story.append('')\n",
    "        else:\n",
    "            sent = tokenize(line)\n",
    "            story.append(sent)\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_stories(f, only_supporting=False, max_length=None):\n",
    "    '''Given a file name, read the file,\n",
    "    retrieve the stories,\n",
    "    and then convert the sentences into a single story.\n",
    "    If max_length is supplied,\n",
    "    any stories longer than max_length tokens will be discarded.\n",
    "    '''\n",
    "    data = parse_stories(f.readlines(), only_supporting=only_supporting)\n",
    "    flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
    "    data = [(flatten(story), q, answer) for story, q, answer in data if not max_length or len(flatten(story)) < max_length]\n",
    "    return data\n",
    "\n",
    "\n",
    "def vectorize_stories(data, word_idx, story_maxlen, query_maxlen):\n",
    "    X = []\n",
    "    Xq = []\n",
    "    Y = []\n",
    "    for story, query, answer in data:\n",
    "        x = [word_idx[w] for w in story]\n",
    "        xq = [word_idx[w] for w in query]\n",
    "        # let's not forget that index 0 is reserved\n",
    "        y = np.zeros(len(word_idx) + 1)\n",
    "        y[word_idx[answer]] = 1\n",
    "        X.append(x)\n",
    "        Xq.append(xq)\n",
    "        Y.append(y)\n",
    "    return (pad_sequences(X, maxlen=story_maxlen),\n",
    "            pad_sequences(Xq, maxlen=query_maxlen), np.array(Y))\n",
    "\n",
    "\n",
    "\n",
    "# 단계 2 : 바비 데이터를 불러온다\n",
    "# 이미 data_babi 에 준비된 문제를 불러옴\n",
    "\n",
    "path='./data_babi/babi-tasks-v1-2.tar.gz'\n",
    "\n",
    "tar = tarfile.open(path)\n",
    "\n",
    "challenges = {\n",
    "    # QA1 with 10,000 samples\n",
    "    'single_supporting_fact_10k': 'tasks_1-20_v1-2/en-10k/qa1_single-supporting-fact_{}.txt',\n",
    "    \n",
    "    # QA2 with 10,000 samples\n",
    "    'two_supporting_facts_10k': 'tasks_1-20_v1-2/en-10k/qa2_two-supporting-facts_{}.txt',\n",
    "}\n",
    "\n",
    "challenge_type = 'single_supporting_fact_10k'\n",
    "#  challenge_type = 'two_supporting_facts_10k'\n",
    "# 두 가지 경우에 각각 해본다 ......\n",
    "\n",
    "challenge = challenges[challenge_type]\n",
    "\n",
    "\n",
    "# 단계 3 : 단어장 등의 데이터를 만든다.\n",
    "# 데이터 선처리 \n",
    "\n",
    "print('Extracting stories for the challenge:', challenge_type)\n",
    "train_stories = get_stories(tar.extractfile(challenge.format('train')))\n",
    "test_stories = get_stories(tar.extractfile(challenge.format('test')))\n",
    "\n",
    "vocab = set()\n",
    "\n",
    "vocab = set()\n",
    "for story, q, answer in train_stories + test_stories:\n",
    "    vocab |= set(story + q + [answer])\n",
    " \n",
    "vocab = sorted(vocab)\n",
    "\n",
    "# Reserve 0 for masking via pad_sequences\n",
    "vocab_size = len(vocab) + 1\n",
    "story_maxlen = max(map(len, (x for x, _, _ in train_stories + test_stories)))\n",
    "query_maxlen = max(map(len, (x for _, x, _ in train_stories + test_stories)))\n",
    "\n",
    "print('-')\n",
    "print('Vocab size:', vocab_size, 'unique words')\n",
    "print('Story max length:', story_maxlen, 'words')\n",
    "print('Query max length:', query_maxlen, 'words')\n",
    "print('Number of training stories:', len(train_stories))\n",
    "print('Number of test stories:', len(test_stories))\n",
    "print('-')\n",
    "print('Here\\'s what a \"story\" tuple looks like (input, query, answer):')\n",
    "print(train_stories[0])\n",
    "print('-')\n",
    "print('Vectorizing the word sequences...')\n",
    "\n",
    "word_idx = dict((c, i + 1) for i, c in enumerate(vocab))\n",
    "idx_word = dict((i+1, c) for i,c in enumerate(vocab))\n",
    "\n",
    "inputs_train, queries_train, answers_train = vectorize_stories(train_stories,\n",
    "                                                               word_idx,\n",
    "                                                               story_maxlen,\n",
    "                                                               query_maxlen)\n",
    "\n",
    "inputs_test, queries_test, answers_test = vectorize_stories(test_stories,\n",
    "                                                            word_idx,\n",
    "                                                            story_maxlen,\n",
    "                                                            query_maxlen)\n",
    "\n",
    "print('-')\n",
    "print('inputs: integer tensor of shape (samples, max_length)')\n",
    "print('inputs_train shape:', inputs_train.shape)\n",
    "print('inputs_test shape:', inputs_test.shape)\n",
    "print('-')\n",
    "print('queries: integer tensor of shape (samples, max_length)')\n",
    "print('queries_train shape:', queries_train.shape)\n",
    "print('queries_test shape:', queries_test.shape)\n",
    "print('-')\n",
    "print('answers: binary (1 or 0) tensor of shape (samples, vocab_size)')\n",
    "print('answers_train shape:', answers_train.shape)\n",
    "print('answers_test shape:', answers_test.shape)\n",
    "print('-')\n",
    "print('Compiling...')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence: Tensor(\"input_3:0\", shape=(None, 68), dtype=float32)\n",
      "Question: Tensor(\"input_4:0\", shape=(None, 4), dtype=float32)\n",
      "Input encoded m Tensor(\"sequential_6/dropout_5/cond/Identity:0\", shape=(None, 68, 64), dtype=float32)\n",
      "Input encoded c Tensor(\"sequential_7/dropout_6/cond/Identity:0\", shape=(None, 68, 4), dtype=float32)\n",
      "Question encoded Tensor(\"sequential_8/dropout_7/cond/Identity:0\", shape=(None, 4, 64), dtype=float32)\n",
      "(None, 68, 4)\n",
      "Match shape Tensor(\"activation_3/truediv:0\", shape=(None, 68, 4), dtype=float32)\n",
      "Response shape Tensor(\"permute_2/transpose:0\", shape=(None, 4, 68), dtype=float32)\n",
      "Answer shape Tensor(\"concatenate_2/concat:0\", shape=(None, 4, 132), dtype=float32)\n",
      "-------------Model Summary------------\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 68)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 4)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_6 (Sequential)       multiple             1408        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_8 (Sequential)       (None, 4, 64)        1408        input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_2 (Dot)                     (None, 68, 4)        0           sequential_6[1][0]               \n",
      "                                                                 sequential_8[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 68, 4)        0           dot_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "sequential_7 (Sequential)       multiple             88          input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 68, 4)        0           activation_3[0][0]               \n",
      "                                                                 sequential_7[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "permute_2 (Permute)             (None, 4, 68)        0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 4, 132)       0           permute_2[0][0]                  \n",
      "                                                                 sequential_8[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 64)           50432       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 64)           0           lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 22)           1430        dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 22)           0           dense_6[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 54,766\n",
      "Trainable params: 54,766\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"1180pt\" viewBox=\"0.00 0.00 783.50 885.00\" width=\"1045pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1.33333 1.33333) rotate(0) translate(4 881)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-881 779.5,-881 779.5,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 2474150934232 -->\n",
       "<g class=\"node\" id=\"node1\"><title>2474150934232</title>\n",
       "<polygon fill=\"none\" points=\"109,-830.5 109,-876.5 368,-876.5 368,-830.5 109,-830.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"172\" y=\"-849.8\">input_3: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"235,-830.5 235,-876.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"263\" y=\"-861.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"235,-853.5 291,-853.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"263\" y=\"-838.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"291,-830.5 291,-876.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"329.5\" y=\"-861.3\">(None, 68)</text>\n",
       "<polyline fill=\"none\" points=\"291,-853.5 368,-853.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"329.5\" y=\"-838.3\">(None, 68)</text>\n",
       "</g>\n",
       "<!-- 2473991970768 -->\n",
       "<g class=\"node\" id=\"node3\"><title>2473991970768</title>\n",
       "<polygon fill=\"none\" points=\"191,-747.5 191,-793.5 456,-793.5 456,-747.5 191,-747.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"265.5\" y=\"-766.8\">sequential_6: Sequential</text>\n",
       "<polyline fill=\"none\" points=\"340,-747.5 340,-793.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"368\" y=\"-778.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"340,-770.5 396,-770.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"368\" y=\"-755.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"396,-747.5 396,-793.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"426\" y=\"-778.3\">multiple</text>\n",
       "<polyline fill=\"none\" points=\"396,-770.5 456,-770.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"426\" y=\"-755.3\">multiple</text>\n",
       "</g>\n",
       "<!-- 2474150934232&#45;&gt;2473991970768 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>2474150934232-&gt;2473991970768</title>\n",
       "<path d=\"M261.727,-830.366C271.378,-821.169 282.711,-810.369 293.017,-800.548\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"295.477,-803.039 300.301,-793.607 290.648,-797.972 295.477,-803.039\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2475896430944 -->\n",
       "<g class=\"node\" id=\"node7\"><title>2475896430944</title>\n",
       "<polygon fill=\"none\" points=\"0,-664.5 0,-710.5 265,-710.5 265,-664.5 0,-664.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"74.5\" y=\"-683.8\">sequential_7: Sequential</text>\n",
       "<polyline fill=\"none\" points=\"149,-664.5 149,-710.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"177\" y=\"-695.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"149,-687.5 205,-687.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"177\" y=\"-672.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"205,-664.5 205,-710.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"235\" y=\"-695.3\">multiple</text>\n",
       "<polyline fill=\"none\" points=\"205,-687.5 265,-687.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"235\" y=\"-672.3\">multiple</text>\n",
       "</g>\n",
       "<!-- 2474150934232&#45;&gt;2475896430944 -->\n",
       "<g class=\"edge\" id=\"edge6\"><title>2474150934232-&gt;2475896430944</title>\n",
       "<path d=\"M213.527,-830.481C202.767,-820.136 190.595,-807.192 181.5,-794 165.595,-770.931 152.537,-741.877 143.912,-720.103\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"147.132,-718.727 140.266,-710.658 140.602,-721.248 147.132,-718.727\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2474150932608 -->\n",
       "<g class=\"node\" id=\"node2\"><title>2474150932608</title>\n",
       "<polygon fill=\"none\" points=\"501.5,-830.5 501.5,-876.5 753.5,-876.5 753.5,-830.5 501.5,-830.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"564.5\" y=\"-849.8\">input_4: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"627.5,-830.5 627.5,-876.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"655.5\" y=\"-861.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"627.5,-853.5 683.5,-853.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"655.5\" y=\"-838.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"683.5,-830.5 683.5,-876.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"718.5\" y=\"-861.3\">(None, 4)</text>\n",
       "<polyline fill=\"none\" points=\"683.5,-853.5 753.5,-853.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"718.5\" y=\"-838.3\">(None, 4)</text>\n",
       "</g>\n",
       "<!-- 2474001616064 -->\n",
       "<g class=\"node\" id=\"node4\"><title>2474001616064</title>\n",
       "<polygon fill=\"none\" points=\"479.5,-747.5 479.5,-793.5 775.5,-793.5 775.5,-747.5 479.5,-747.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"554\" y=\"-766.8\">sequential_8: Sequential</text>\n",
       "<polyline fill=\"none\" points=\"628.5,-747.5 628.5,-793.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"656.5\" y=\"-778.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"628.5,-770.5 684.5,-770.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"656.5\" y=\"-755.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"684.5,-747.5 684.5,-793.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"730\" y=\"-778.3\">(None, 4)</text>\n",
       "<polyline fill=\"none\" points=\"684.5,-770.5 775.5,-770.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"730\" y=\"-755.3\">(None, 4, 64)</text>\n",
       "</g>\n",
       "<!-- 2474150932608&#45;&gt;2474001616064 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>2474150932608-&gt;2474001616064</title>\n",
       "<path d=\"M627.5,-830.366C627.5,-822.152 627.5,-812.658 627.5,-803.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"631,-803.607 627.5,-793.607 624,-803.607 631,-803.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2474111867144 -->\n",
       "<g class=\"node\" id=\"node5\"><title>2474111867144</title>\n",
       "<polygon fill=\"none\" points=\"283,-664.5 283,-710.5 604,-710.5 604,-664.5 283,-664.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"321.5\" y=\"-683.8\">dot_2: Dot</text>\n",
       "<polyline fill=\"none\" points=\"360,-664.5 360,-710.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"388\" y=\"-695.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"360,-687.5 416,-687.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"388\" y=\"-672.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"416,-664.5 416,-710.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"510\" y=\"-695.3\">[(None, 68, 64), (None, 4, 64)]</text>\n",
       "<polyline fill=\"none\" points=\"416,-687.5 604,-687.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"510\" y=\"-672.3\">(None, 68, 4)</text>\n",
       "</g>\n",
       "<!-- 2473991970768&#45;&gt;2474111867144 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>2473991970768-&gt;2474111867144</title>\n",
       "<path d=\"M356.291,-747.366C370.445,-737.812 387.162,-726.528 402.154,-716.409\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"404.419,-719.103 410.749,-710.607 400.502,-713.301 404.419,-719.103\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2474001616064&#45;&gt;2474111867144 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>2474001616064-&gt;2474111867144</title>\n",
       "<path d=\"M577.221,-747.366C554.301,-737.277 526.998,-725.257 503.056,-714.718\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"504.281,-711.433 493.718,-710.607 501.46,-717.839 504.281,-711.433\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2474002732424 -->\n",
       "<g class=\"node\" id=\"node10\"><title>2474002732424</title>\n",
       "<polygon fill=\"none\" points=\"338,-332.5 338,-378.5 749,-378.5 749,-332.5 338,-332.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"424.5\" y=\"-351.8\">concatenate_2: Concatenate</text>\n",
       "<polyline fill=\"none\" points=\"511,-332.5 511,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"539\" y=\"-363.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"511,-355.5 567,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"539\" y=\"-340.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"567,-332.5 567,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"658\" y=\"-363.3\">[(None, 4, 68), (None, 4, 64)]</text>\n",
       "<polyline fill=\"none\" points=\"567,-355.5 749,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"658\" y=\"-340.3\">(None, 4, 132)</text>\n",
       "</g>\n",
       "<!-- 2474001616064&#45;&gt;2474002732424 -->\n",
       "<g class=\"edge\" id=\"edge11\"><title>2474001616064-&gt;2474002732424</title>\n",
       "<path d=\"M628.597,-747.489C630.069,-716.019 632.5,-656.347 632.5,-605.5 632.5,-605.5 632.5,-605.5 632.5,-520.5 632.5,-471.752 627.284,-456.977 602.5,-415 596.198,-404.326 587.478,-394.225 578.674,-385.517\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"580.979,-382.879 571.31,-378.547 576.167,-387.963 580.979,-382.879\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2474002981832 -->\n",
       "<g class=\"node\" id=\"node6\"><title>2474002981832</title>\n",
       "<polygon fill=\"none\" points=\"296,-581.5 296,-627.5 591,-627.5 591,-581.5 296,-581.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"370\" y=\"-600.8\">activation_3: Activation</text>\n",
       "<polyline fill=\"none\" points=\"444,-581.5 444,-627.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"472\" y=\"-612.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"444,-604.5 500,-604.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"472\" y=\"-589.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"500,-581.5 500,-627.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"545.5\" y=\"-612.3\">(None, 68, 4)</text>\n",
       "<polyline fill=\"none\" points=\"500,-604.5 591,-604.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"545.5\" y=\"-589.3\">(None, 68, 4)</text>\n",
       "</g>\n",
       "<!-- 2474111867144&#45;&gt;2474002981832 -->\n",
       "<g class=\"edge\" id=\"edge5\"><title>2474111867144-&gt;2474002981832</title>\n",
       "<path d=\"M443.5,-664.366C443.5,-656.152 443.5,-646.658 443.5,-637.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"447,-637.607 443.5,-627.607 440,-637.607 447,-637.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2473985593072 -->\n",
       "<g class=\"node\" id=\"node8\"><title>2473985593072</title>\n",
       "<polygon fill=\"none\" points=\"283,-498.5 283,-544.5 604,-544.5 604,-498.5 283,-498.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"324.5\" y=\"-517.8\">add_2: Add</text>\n",
       "<polyline fill=\"none\" points=\"366,-498.5 366,-544.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"394\" y=\"-529.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"366,-521.5 422,-521.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"394\" y=\"-506.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"422,-498.5 422,-544.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"513\" y=\"-529.3\">[(None, 68, 4), (None, 68, 4)]</text>\n",
       "<polyline fill=\"none\" points=\"422,-521.5 604,-521.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"513\" y=\"-506.3\">(None, 68, 4)</text>\n",
       "</g>\n",
       "<!-- 2474002981832&#45;&gt;2473985593072 -->\n",
       "<g class=\"edge\" id=\"edge7\"><title>2474002981832-&gt;2473985593072</title>\n",
       "<path d=\"M443.5,-581.366C443.5,-573.152 443.5,-563.658 443.5,-554.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"447,-554.607 443.5,-544.607 440,-554.607 447,-554.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2475896430944&#45;&gt;2473985593072 -->\n",
       "<g class=\"edge\" id=\"edge8\"><title>2475896430944-&gt;2473985593072</title>\n",
       "<path d=\"M160.141,-664.479C189.999,-641.449 239.611,-605.331 286.5,-581 310.768,-568.408 338.379,-557.176 363.608,-548.003\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"364.912,-551.254 373.149,-544.59 362.554,-544.663 364.912,-551.254\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2474001315152 -->\n",
       "<g class=\"node\" id=\"node9\"><title>2474001315152</title>\n",
       "<polygon fill=\"none\" points=\"318,-415.5 318,-461.5 593,-461.5 593,-415.5 318,-415.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"382\" y=\"-434.8\">permute_2: Permute</text>\n",
       "<polyline fill=\"none\" points=\"446,-415.5 446,-461.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"474\" y=\"-446.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"446,-438.5 502,-438.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"474\" y=\"-423.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"502,-415.5 502,-461.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"547.5\" y=\"-446.3\">(None, 68, 4)</text>\n",
       "<polyline fill=\"none\" points=\"502,-438.5 593,-438.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"547.5\" y=\"-423.3\">(None, 4, 68)</text>\n",
       "</g>\n",
       "<!-- 2473985593072&#45;&gt;2474001315152 -->\n",
       "<g class=\"edge\" id=\"edge9\"><title>2473985593072-&gt;2474001315152</title>\n",
       "<path d=\"M446.779,-498.366C447.996,-490.152 449.403,-480.658 450.726,-471.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"454.222,-472.012 452.225,-461.607 447.297,-470.986 454.222,-472.012\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2474001315152&#45;&gt;2474002732424 -->\n",
       "<g class=\"edge\" id=\"edge10\"><title>2474001315152-&gt;2474002732424</title>\n",
       "<path d=\"M479.546,-415.366C489.538,-406.169 501.272,-395.369 511.941,-385.548\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"514.495,-387.954 519.483,-378.607 509.755,-382.804 514.495,-387.954\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2474002734496 -->\n",
       "<g class=\"node\" id=\"node11\"><title>2474002734496</title>\n",
       "<polygon fill=\"none\" points=\"417.5,-249.5 417.5,-295.5 669.5,-295.5 669.5,-249.5 417.5,-249.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"466.5\" y=\"-268.8\">lstm_2: LSTM</text>\n",
       "<polyline fill=\"none\" points=\"515.5,-249.5 515.5,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"543.5\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"515.5,-272.5 571.5,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"543.5\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"571.5,-249.5 571.5,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"620.5\" y=\"-280.3\">(None, 4, 132)</text>\n",
       "<polyline fill=\"none\" points=\"571.5,-272.5 669.5,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"620.5\" y=\"-257.3\">(None, 64)</text>\n",
       "</g>\n",
       "<!-- 2474002732424&#45;&gt;2474002734496 -->\n",
       "<g class=\"edge\" id=\"edge12\"><title>2474002732424-&gt;2474002734496</title>\n",
       "<path d=\"M543.5,-332.366C543.5,-324.152 543.5,-314.658 543.5,-305.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"547,-305.607 543.5,-295.607 540,-305.607 547,-305.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2474002734216 -->\n",
       "<g class=\"node\" id=\"node12\"><title>2474002734216</title>\n",
       "<polygon fill=\"none\" points=\"412,-166.5 412,-212.5 675,-212.5 675,-166.5 412,-166.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"477\" y=\"-185.8\">dropout_8: Dropout</text>\n",
       "<polyline fill=\"none\" points=\"542,-166.5 542,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"570\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"542,-189.5 598,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"570\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"598,-166.5 598,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"636.5\" y=\"-197.3\">(None, 64)</text>\n",
       "<polyline fill=\"none\" points=\"598,-189.5 675,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"636.5\" y=\"-174.3\">(None, 64)</text>\n",
       "</g>\n",
       "<!-- 2474002734496&#45;&gt;2474002734216 -->\n",
       "<g class=\"edge\" id=\"edge13\"><title>2474002734496-&gt;2474002734216</title>\n",
       "<path d=\"M543.5,-249.366C543.5,-241.152 543.5,-231.658 543.5,-222.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"547,-222.607 543.5,-212.607 540,-222.607 547,-222.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2474002734944 -->\n",
       "<g class=\"node\" id=\"node13\"><title>2474002734944</title>\n",
       "<polygon fill=\"none\" points=\"425,-83.5 425,-129.5 662,-129.5 662,-83.5 425,-83.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"477\" y=\"-102.8\">dense_6: Dense</text>\n",
       "<polyline fill=\"none\" points=\"529,-83.5 529,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"557\" y=\"-114.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"529,-106.5 585,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"557\" y=\"-91.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"585,-83.5 585,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"623.5\" y=\"-114.3\">(None, 64)</text>\n",
       "<polyline fill=\"none\" points=\"585,-106.5 662,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"623.5\" y=\"-91.3\">(None, 22)</text>\n",
       "</g>\n",
       "<!-- 2474002734216&#45;&gt;2474002734944 -->\n",
       "<g class=\"edge\" id=\"edge14\"><title>2474002734216-&gt;2474002734944</title>\n",
       "<path d=\"M543.5,-166.366C543.5,-158.152 543.5,-148.658 543.5,-139.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"547,-139.607 543.5,-129.607 540,-139.607 547,-139.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2474002733712 -->\n",
       "<g class=\"node\" id=\"node14\"><title>2474002733712</title>\n",
       "<polygon fill=\"none\" points=\"403,-0.5 403,-46.5 684,-46.5 684,-0.5 403,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"477\" y=\"-19.8\">activation_4: Activation</text>\n",
       "<polyline fill=\"none\" points=\"551,-0.5 551,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"579\" y=\"-31.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"551,-23.5 607,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"579\" y=\"-8.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"607,-0.5 607,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"645.5\" y=\"-31.3\">(None, 22)</text>\n",
       "<polyline fill=\"none\" points=\"607,-23.5 684,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"645.5\" y=\"-8.3\">(None, 22)</text>\n",
       "</g>\n",
       "<!-- 2474002734944&#45;&gt;2474002733712 -->\n",
       "<g class=\"edge\" id=\"edge15\"><title>2474002734944-&gt;2474002733712</title>\n",
       "<path d=\"M543.5,-83.3664C543.5,-75.1516 543.5,-65.6579 543.5,-56.7252\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"547,-56.6068 543.5,-46.6068 540,-56.6069 547,-56.6068\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단계 4 : computaitonal graph 구성\n",
    "# 변수와 딥러닝 모델을 만든다 \n",
    "\n",
    "# placeholders\n",
    "\n",
    "input_sequence = Input((story_maxlen,))\n",
    "question = Input((query_maxlen,))\n",
    "\n",
    "print('Input sequence:', input_sequence)\n",
    "print('Question:', question)\n",
    "\n",
    "# encoders\n",
    "# embed the input sequence into a sequence of vectors\n",
    "input_encoder_m = Sequential()\n",
    "input_encoder_m.add(Embedding(input_dim=vocab_size,\n",
    "                              output_dim=64))\n",
    "input_encoder_m.add(Dropout(0.3))\n",
    "# output: (samples, story_maxlen, embedding_dim)\n",
    "\n",
    "# embed the input into a sequence of vectors of size query_maxlen\n",
    "input_encoder_c = Sequential()\n",
    "input_encoder_c.add(Embedding(input_dim=vocab_size,\n",
    "                              output_dim=query_maxlen))\n",
    "input_encoder_c.add(Dropout(0.3))\n",
    "# output: (samples, story_maxlen, query_maxlen)\n",
    "\n",
    "# embed the question into a sequence of vectors\n",
    "question_encoder = Sequential()\n",
    "question_encoder.add(Embedding(input_dim=vocab_size,\n",
    "                               output_dim=64,\n",
    "                               input_length=query_maxlen))\n",
    "question_encoder.add(Dropout(0.3))\n",
    "# output: (samples, query_maxlen, embedding_dim)\n",
    "\n",
    "# encode input sequence and questions (which are indices)\n",
    "# to sequences of dense vectors\n",
    "input_encoded_m = input_encoder_m(input_sequence)\n",
    "print('Input encoded m', input_encoded_m)\n",
    "input_encoded_c = input_encoder_c(input_sequence)\n",
    "print('Input encoded c', input_encoded_c)\n",
    "question_encoded = question_encoder(question)\n",
    "print('Question encoded', question_encoded)\n",
    "\n",
    "\n",
    "# compute a 'match' between the first input vector sequence\n",
    "# and the question vector sequence\n",
    "# shape: `(samples, story_maxlen, query_maxlen)\n",
    "match = dot([input_encoded_m, question_encoded], axes=(2, 2))\n",
    "print(match.shape)\n",
    "match = Activation('softmax')(match)\n",
    "print('Match shape', match)\n",
    "\n",
    "# add the match matrix with the second input vector sequence\n",
    "response = add([match, input_encoded_c])  # (samples, story_maxlen, query_maxlen)\n",
    "response = Permute((2, 1))(response)  # (samples, query_maxlen, story_maxlen)\n",
    "print('Response shape', response)\n",
    "\n",
    "# concatenate the response vector with the question vector sequence\n",
    "answer = concatenate([response, question_encoded])\n",
    "print('Answer shape', answer)\n",
    "\n",
    "#answer = LSTM(lstm_size, return_sequences=True)(answer)  # Generate tensors of shape 32\n",
    "#answer = Dropout(0.3)(answer)\n",
    "answer = LSTM(lstm_size)(answer)  # Generate tensors of shape 32\n",
    "answer = Dropout(0.3)(answer)\n",
    "answer = Dense(vocab_size)(answer)  # (samples, vocab_size)\n",
    "# we output a probability distribution over the vocabulary\n",
    "answer = Activation('softmax')(answer)\n",
    "# build the final model\n",
    "model = Model([input_sequence, question], answer)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(\"-------------Model Summary------------\")\n",
    "print(model.summary())\n",
    "# 모델 구조를 쓰고 그리기 \n",
    "\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig the model\n",
      "Train on 10000 samples, validate on 1000 samples\n",
      "Epoch 1/100\n",
      "10000/10000 [==============================] - 3s 349us/step - loss: 0.0251 - accuracy: 0.9928 - val_loss: 0.0788 - val_accuracy: 0.9810\n",
      "Epoch 2/100\n",
      "10000/10000 [==============================] - 3s 325us/step - loss: 0.0358 - accuracy: 0.9898 - val_loss: 0.0585 - val_accuracy: 0.9820\n",
      "Epoch 3/100\n",
      "10000/10000 [==============================] - 3s 327us/step - loss: 0.0243 - accuracy: 0.9922 - val_loss: 0.0759 - val_accuracy: 0.9800\n",
      "Epoch 4/100\n",
      "10000/10000 [==============================] - 3s 337us/step - loss: 0.0276 - accuracy: 0.9926 - val_loss: 0.0593 - val_accuracy: 0.9820\n",
      "Epoch 5/100\n",
      "10000/10000 [==============================] - 3s 321us/step - loss: 0.0219 - accuracy: 0.9937 - val_loss: 0.0601 - val_accuracy: 0.9820\n",
      "Epoch 6/100\n",
      "10000/10000 [==============================] - 3s 318us/step - loss: 0.0162 - accuracy: 0.9949 - val_loss: 0.0454 - val_accuracy: 0.9880\n",
      "Epoch 7/100\n",
      "10000/10000 [==============================] - 3s 321us/step - loss: 0.0254 - accuracy: 0.9914 - val_loss: 0.0501 - val_accuracy: 0.9860\n",
      "Epoch 8/100\n",
      "10000/10000 [==============================] - 3s 328us/step - loss: 0.0315 - accuracy: 0.9911 - val_loss: 0.0566 - val_accuracy: 0.9830\n",
      "Epoch 9/100\n",
      "10000/10000 [==============================] - 3s 329us/step - loss: 0.0273 - accuracy: 0.9924 - val_loss: 0.0637 - val_accuracy: 0.9820\n",
      "Epoch 10/100\n",
      "10000/10000 [==============================] - 3s 324us/step - loss: 0.0219 - accuracy: 0.9929 - val_loss: 0.0450 - val_accuracy: 0.9870\n",
      "Epoch 11/100\n",
      "10000/10000 [==============================] - 3s 326us/step - loss: 0.0242 - accuracy: 0.9926 - val_loss: 0.0701 - val_accuracy: 0.9830\n",
      "Epoch 12/100\n",
      "10000/10000 [==============================] - 3s 333us/step - loss: 0.0220 - accuracy: 0.9928 - val_loss: 0.0430 - val_accuracy: 0.9900\n",
      "Epoch 13/100\n",
      "10000/10000 [==============================] - 3s 325us/step - loss: 0.0285 - accuracy: 0.9907 - val_loss: 0.0691 - val_accuracy: 0.9820\n",
      "Epoch 14/100\n",
      "10000/10000 [==============================] - 3s 322us/step - loss: 0.0171 - accuracy: 0.9957 - val_loss: 0.0499 - val_accuracy: 0.9860\n",
      "Epoch 15/100\n",
      "10000/10000 [==============================] - 3s 330us/step - loss: 0.0250 - accuracy: 0.9923 - val_loss: 0.0624 - val_accuracy: 0.9840\n",
      "Epoch 16/100\n",
      "10000/10000 [==============================] - 3s 323us/step - loss: 0.0241 - accuracy: 0.9923 - val_loss: 0.0565 - val_accuracy: 0.9820\n",
      "Epoch 17/100\n",
      "10000/10000 [==============================] - 3s 320us/step - loss: 0.0181 - accuracy: 0.9952 - val_loss: 0.0484 - val_accuracy: 0.9890\n",
      "Epoch 18/100\n",
      "10000/10000 [==============================] - 3s 324us/step - loss: 0.0233 - accuracy: 0.9932 - val_loss: 0.0463 - val_accuracy: 0.9880\n",
      "Epoch 19/100\n",
      "10000/10000 [==============================] - 3s 329us/step - loss: 0.0233 - accuracy: 0.9917 - val_loss: 0.0628 - val_accuracy: 0.9840\n",
      "Epoch 20/100\n",
      "10000/10000 [==============================] - 3s 326us/step - loss: 0.0200 - accuracy: 0.9931 - val_loss: 0.0480 - val_accuracy: 0.9870\n",
      "Epoch 21/100\n",
      "10000/10000 [==============================] - 3s 315us/step - loss: 0.0191 - accuracy: 0.9949 - val_loss: 0.0544 - val_accuracy: 0.9850\n",
      "Epoch 22/100\n",
      "10000/10000 [==============================] - 3s 323us/step - loss: 0.0263 - accuracy: 0.9906 - val_loss: 0.0753 - val_accuracy: 0.9830\n",
      "Epoch 23/100\n",
      "10000/10000 [==============================] - 3s 328us/step - loss: 0.0199 - accuracy: 0.9936 - val_loss: 0.0321 - val_accuracy: 0.9870\n",
      "Epoch 24/100\n",
      "10000/10000 [==============================] - 3s 331us/step - loss: 0.0216 - accuracy: 0.9939 - val_loss: 0.0383 - val_accuracy: 0.9880\n",
      "Epoch 25/100\n",
      "10000/10000 [==============================] - 3s 319us/step - loss: 0.0207 - accuracy: 0.9933 - val_loss: 0.0506 - val_accuracy: 0.9850\n",
      "Epoch 26/100\n",
      "10000/10000 [==============================] - 3s 324us/step - loss: 0.0182 - accuracy: 0.9942 - val_loss: 0.0459 - val_accuracy: 0.9870\n",
      "Epoch 27/100\n",
      "10000/10000 [==============================] - 3s 318us/step - loss: 0.0169 - accuracy: 0.9944 - val_loss: 0.0673 - val_accuracy: 0.9830\n",
      "Epoch 28/100\n",
      "10000/10000 [==============================] - 3s 328us/step - loss: 0.0269 - accuracy: 0.9923 - val_loss: 0.0546 - val_accuracy: 0.9830\n",
      "Epoch 29/100\n",
      "10000/10000 [==============================] - 3s 328us/step - loss: 0.0207 - accuracy: 0.9930 - val_loss: 0.0489 - val_accuracy: 0.9870\n",
      "Epoch 30/100\n",
      "10000/10000 [==============================] - 3s 323us/step - loss: 0.0163 - accuracy: 0.9947 - val_loss: 0.0480 - val_accuracy: 0.9870\n",
      "Epoch 31/100\n",
      "10000/10000 [==============================] - 3s 327us/step - loss: 0.0221 - accuracy: 0.9922 - val_loss: 0.0423 - val_accuracy: 0.9890\n",
      "Epoch 32/100\n",
      "10000/10000 [==============================] - 4s 374us/step - loss: 0.0199 - accuracy: 0.9935 - val_loss: 0.0375 - val_accuracy: 0.9890\n",
      "Epoch 33/100\n",
      "10000/10000 [==============================] - 3s 330us/step - loss: 0.0217 - accuracy: 0.9930 - val_loss: 0.0420 - val_accuracy: 0.9880\n",
      "Epoch 34/100\n",
      "10000/10000 [==============================] - 3s 323us/step - loss: 0.0197 - accuracy: 0.9943 - val_loss: 0.0547 - val_accuracy: 0.9860\n",
      "Epoch 35/100\n",
      "10000/10000 [==============================] - 3s 330us/step - loss: 0.0249 - accuracy: 0.9927 - val_loss: 0.0431 - val_accuracy: 0.9900\n",
      "Epoch 36/100\n",
      "10000/10000 [==============================] - 3s 321us/step - loss: 0.0247 - accuracy: 0.9926 - val_loss: 0.0308 - val_accuracy: 0.9900\n",
      "Epoch 37/100\n",
      "10000/10000 [==============================] - 3s 321us/step - loss: 0.0194 - accuracy: 0.9927 - val_loss: 0.0431 - val_accuracy: 0.9850\n",
      "Epoch 38/100\n",
      "10000/10000 [==============================] - 3s 339us/step - loss: 0.0166 - accuracy: 0.9955 - val_loss: 0.0370 - val_accuracy: 0.9900\n",
      "Epoch 39/100\n",
      "10000/10000 [==============================] - 3s 329us/step - loss: 0.0172 - accuracy: 0.9947 - val_loss: 0.0574 - val_accuracy: 0.9860\n",
      "Epoch 40/100\n",
      "10000/10000 [==============================] - 3s 324us/step - loss: 0.0197 - accuracy: 0.9941 - val_loss: 0.0640 - val_accuracy: 0.9860\n",
      "Epoch 41/100\n",
      "10000/10000 [==============================] - 3s 324us/step - loss: 0.0284 - accuracy: 0.9922 - val_loss: 0.0355 - val_accuracy: 0.9840\n",
      "Epoch 42/100\n",
      "10000/10000 [==============================] - 3s 319us/step - loss: 0.0148 - accuracy: 0.9960 - val_loss: 0.0370 - val_accuracy: 0.9870\n",
      "Epoch 43/100\n",
      "10000/10000 [==============================] - 3s 322us/step - loss: 0.0190 - accuracy: 0.9940 - val_loss: 0.0262 - val_accuracy: 0.9910\n",
      "Epoch 44/100\n",
      "10000/10000 [==============================] - 3s 337us/step - loss: 0.0184 - accuracy: 0.9950 - val_loss: 0.0406 - val_accuracy: 0.9890\n",
      "Epoch 45/100\n",
      "10000/10000 [==============================] - 4s 359us/step - loss: 0.0202 - accuracy: 0.9938 - val_loss: 0.0197 - val_accuracy: 0.9940\n",
      "Epoch 46/100\n",
      "10000/10000 [==============================] - 3s 344us/step - loss: 0.0204 - accuracy: 0.9933 - val_loss: 0.0361 - val_accuracy: 0.9900\n",
      "Epoch 47/100\n",
      "10000/10000 [==============================] - 3s 325us/step - loss: 0.0188 - accuracy: 0.9942 - val_loss: 0.0427 - val_accuracy: 0.9860\n",
      "Epoch 48/100\n",
      "10000/10000 [==============================] - 3s 320us/step - loss: 0.0196 - accuracy: 0.9936 - val_loss: 0.0396 - val_accuracy: 0.9860\n",
      "Epoch 49/100\n",
      "10000/10000 [==============================] - 3s 323us/step - loss: 0.0187 - accuracy: 0.9939 - val_loss: 0.0353 - val_accuracy: 0.9900\n",
      "Epoch 50/100\n",
      "10000/10000 [==============================] - 3s 329us/step - loss: 0.0145 - accuracy: 0.9956 - val_loss: 0.0308 - val_accuracy: 0.9910\n",
      "Epoch 51/100\n",
      "10000/10000 [==============================] - 3s 319us/step - loss: 0.0174 - accuracy: 0.9946 - val_loss: 0.0456 - val_accuracy: 0.9890\n",
      "Epoch 52/100\n",
      "10000/10000 [==============================] - 3s 322us/step - loss: 0.0194 - accuracy: 0.9935 - val_loss: 0.0478 - val_accuracy: 0.9840\n",
      "Epoch 53/100\n",
      "10000/10000 [==============================] - 3s 324us/step - loss: 0.0218 - accuracy: 0.9947 - val_loss: 0.0308 - val_accuracy: 0.9900\n",
      "Epoch 54/100\n",
      "10000/10000 [==============================] - 3s 324us/step - loss: 0.0144 - accuracy: 0.9953 - val_loss: 0.0291 - val_accuracy: 0.9910\n",
      "Epoch 55/100\n",
      "10000/10000 [==============================] - 3s 346us/step - loss: 0.0177 - accuracy: 0.9949 - val_loss: 0.0227 - val_accuracy: 0.9930\n",
      "Epoch 56/100\n",
      "10000/10000 [==============================] - 3s 316us/step - loss: 0.0192 - accuracy: 0.9940 - val_loss: 0.0391 - val_accuracy: 0.9880\n",
      "Epoch 57/100\n",
      "10000/10000 [==============================] - 3s 325us/step - loss: 0.0151 - accuracy: 0.9961 - val_loss: 0.0314 - val_accuracy: 0.9900\n",
      "Epoch 58/100\n",
      "10000/10000 [==============================] - 3s 323us/step - loss: 0.0249 - accuracy: 0.9923 - val_loss: 0.0511 - val_accuracy: 0.9830\n",
      "Epoch 59/100\n",
      "10000/10000 [==============================] - 3s 327us/step - loss: 0.0168 - accuracy: 0.9947 - val_loss: 0.0284 - val_accuracy: 0.9930\n",
      "Epoch 60/100\n",
      "10000/10000 [==============================] - 3s 326us/step - loss: 0.0105 - accuracy: 0.9968 - val_loss: 0.0355 - val_accuracy: 0.9900\n",
      "Epoch 61/100\n",
      "10000/10000 [==============================] - 3s 322us/step - loss: 0.0194 - accuracy: 0.9946 - val_loss: 0.0323 - val_accuracy: 0.9910\n",
      "Epoch 62/100\n",
      "10000/10000 [==============================] - 3s 317us/step - loss: 0.0144 - accuracy: 0.9958 - val_loss: 0.0337 - val_accuracy: 0.9900\n",
      "Epoch 63/100\n",
      "10000/10000 [==============================] - 3s 329us/step - loss: 0.0176 - accuracy: 0.9953 - val_loss: 0.0206 - val_accuracy: 0.9910\n",
      "Epoch 64/100\n",
      "10000/10000 [==============================] - 3s 320us/step - loss: 0.0148 - accuracy: 0.9950 - val_loss: 0.0324 - val_accuracy: 0.9900\n",
      "Epoch 65/100\n",
      "10000/10000 [==============================] - 3s 328us/step - loss: 0.0106 - accuracy: 0.9972 - val_loss: 0.0443 - val_accuracy: 0.9890\n",
      "Epoch 66/100\n",
      "10000/10000 [==============================] - 3s 333us/step - loss: 0.0171 - accuracy: 0.9939 - val_loss: 0.0311 - val_accuracy: 0.9910\n",
      "Epoch 67/100\n",
      "10000/10000 [==============================] - 3s 323us/step - loss: 0.0125 - accuracy: 0.9960 - val_loss: 0.0384 - val_accuracy: 0.9900\n",
      "Epoch 68/100\n",
      "10000/10000 [==============================] - 3s 318us/step - loss: 0.0186 - accuracy: 0.9940 - val_loss: 0.0332 - val_accuracy: 0.9890\n",
      "Epoch 69/100\n",
      "10000/10000 [==============================] - 4s 358us/step - loss: 0.0152 - accuracy: 0.9960 - val_loss: 0.0445 - val_accuracy: 0.9900\n",
      "Epoch 70/100\n",
      "10000/10000 [==============================] - 3s 328us/step - loss: 0.0176 - accuracy: 0.9951 - val_loss: 0.0395 - val_accuracy: 0.9880\n",
      "Epoch 71/100\n",
      "10000/10000 [==============================] - 3s 315us/step - loss: 0.0120 - accuracy: 0.9964 - val_loss: 0.0487 - val_accuracy: 0.9840\n",
      "Epoch 72/100\n",
      "10000/10000 [==============================] - 3s 328us/step - loss: 0.0096 - accuracy: 0.9975 - val_loss: 0.0350 - val_accuracy: 0.9900\n",
      "Epoch 73/100\n",
      "10000/10000 [==============================] - 3s 326us/step - loss: 0.0140 - accuracy: 0.9952 - val_loss: 0.0378 - val_accuracy: 0.9870\n",
      "Epoch 74/100\n",
      "10000/10000 [==============================] - 3s 325us/step - loss: 0.0138 - accuracy: 0.9965 - val_loss: 0.0466 - val_accuracy: 0.9880\n",
      "Epoch 75/100\n",
      "10000/10000 [==============================] - 3s 325us/step - loss: 0.0142 - accuracy: 0.9962 - val_loss: 0.0511 - val_accuracy: 0.9870\n",
      "Epoch 76/100\n",
      "10000/10000 [==============================] - 3s 318us/step - loss: 0.0185 - accuracy: 0.9944 - val_loss: 0.0247 - val_accuracy: 0.9940\n",
      "Epoch 77/100\n",
      "10000/10000 [==============================] - 3s 340us/step - loss: 0.0155 - accuracy: 0.9957 - val_loss: 0.0245 - val_accuracy: 0.9950\n",
      "Epoch 78/100\n",
      "10000/10000 [==============================] - 3s 331us/step - loss: 0.0137 - accuracy: 0.9965 - val_loss: 0.0402 - val_accuracy: 0.9880\n",
      "Epoch 79/100\n",
      "10000/10000 [==============================] - 3s 330us/step - loss: 0.0106 - accuracy: 0.9965 - val_loss: 0.0362 - val_accuracy: 0.9910\n",
      "Epoch 80/100\n",
      "10000/10000 [==============================] - 3s 327us/step - loss: 0.0104 - accuracy: 0.9966 - val_loss: 0.0201 - val_accuracy: 0.9950\n",
      "Epoch 81/100\n",
      "10000/10000 [==============================] - 3s 315us/step - loss: 0.0165 - accuracy: 0.9957 - val_loss: 0.0440 - val_accuracy: 0.9860\n",
      "Epoch 82/100\n",
      "10000/10000 [==============================] - 3s 328us/step - loss: 0.0173 - accuracy: 0.9951 - val_loss: 0.0449 - val_accuracy: 0.9890\n",
      "Epoch 83/100\n",
      "10000/10000 [==============================] - 3s 322us/step - loss: 0.0063 - accuracy: 0.9979 - val_loss: 0.0259 - val_accuracy: 0.9910\n",
      "Epoch 84/100\n",
      "10000/10000 [==============================] - 3s 324us/step - loss: 0.0124 - accuracy: 0.9957 - val_loss: 0.0163 - val_accuracy: 0.9950\n",
      "Epoch 85/100\n",
      "10000/10000 [==============================] - 3s 325us/step - loss: 0.0149 - accuracy: 0.9958 - val_loss: 0.0384 - val_accuracy: 0.9900\n",
      "Epoch 86/100\n",
      "10000/10000 [==============================] - 3s 324us/step - loss: 0.0122 - accuracy: 0.9971 - val_loss: 0.0440 - val_accuracy: 0.9890\n",
      "Epoch 87/100\n",
      "10000/10000 [==============================] - 3s 318us/step - loss: 0.0134 - accuracy: 0.9957 - val_loss: 0.0283 - val_accuracy: 0.9910\n",
      "Epoch 88/100\n",
      "10000/10000 [==============================] - 3s 326us/step - loss: 0.0101 - accuracy: 0.9973 - val_loss: 0.0333 - val_accuracy: 0.9920\n",
      "Epoch 89/100\n",
      "10000/10000 [==============================] - 3s 331us/step - loss: 0.0118 - accuracy: 0.9963 - val_loss: 0.0214 - val_accuracy: 0.9920\n",
      "Epoch 90/100\n",
      "10000/10000 [==============================] - 3s 323us/step - loss: 0.0118 - accuracy: 0.9964 - val_loss: 0.0263 - val_accuracy: 0.9930\n",
      "Epoch 91/100\n",
      "10000/10000 [==============================] - 3s 314us/step - loss: 0.0111 - accuracy: 0.9971 - val_loss: 0.0308 - val_accuracy: 0.9920\n",
      "Epoch 92/100\n",
      "10000/10000 [==============================] - 3s 341us/step - loss: 0.0129 - accuracy: 0.9958 - val_loss: 0.0294 - val_accuracy: 0.9950\n",
      "Epoch 93/100\n",
      "10000/10000 [==============================] - 3s 337us/step - loss: 0.0072 - accuracy: 0.9980 - val_loss: 0.0247 - val_accuracy: 0.9930\n",
      "Epoch 94/100\n",
      "10000/10000 [==============================] - 3s 316us/step - loss: 0.0121 - accuracy: 0.9953 - val_loss: 0.0352 - val_accuracy: 0.9910\n",
      "Epoch 95/100\n",
      "10000/10000 [==============================] - 3s 318us/step - loss: 0.0126 - accuracy: 0.9957 - val_loss: 0.0288 - val_accuracy: 0.9910\n",
      "Epoch 96/100\n",
      "10000/10000 [==============================] - 3s 319us/step - loss: 0.0081 - accuracy: 0.9970 - val_loss: 0.0267 - val_accuracy: 0.9930\n",
      "Epoch 97/100\n",
      "10000/10000 [==============================] - 3s 323us/step - loss: 0.0098 - accuracy: 0.9971 - val_loss: 0.0511 - val_accuracy: 0.9860\n",
      "Epoch 98/100\n",
      "10000/10000 [==============================] - 3s 325us/step - loss: 0.0129 - accuracy: 0.9967 - val_loss: 0.0309 - val_accuracy: 0.9910\n",
      "Epoch 99/100\n",
      "10000/10000 [==============================] - 3s 317us/step - loss: 0.0077 - accuracy: 0.9975 - val_loss: 0.0487 - val_accuracy: 0.9900\n",
      "Epoch 100/100\n",
      "10000/10000 [==============================] - 3s 320us/step - loss: 0.0112 - accuracy: 0.9966 - val_loss: 0.0266 - val_accuracy: 0.9900\n",
      "-------------------------------------------------------------------------------------------\n",
      "Qualitative Test Result Analysis\n",
      "John travelled to the hallway . Mary journeyed to the bathroom . Where is John ? | Prediction: hallway | Ground Truth: hallway\n",
      "John travelled to the hallway . Mary journeyed to the bathroom . Daniel went back to the bathroom . John moved to the bedroom . Where is Mary ? | Prediction: bathroom | Ground Truth: bathroom\n",
      "John travelled to the hallway . Mary journeyed to the bathroom . Daniel went back to the bathroom . John moved to the bedroom . John went to the hallway . Sandra journeyed to the kitchen . Where is Sandra ? | Prediction: kitchen | Ground Truth: kitchen\n",
      "John travelled to the hallway . Mary journeyed to the bathroom . Daniel went back to the bathroom . John moved to the bedroom . John went to the hallway . Sandra journeyed to the kitchen . Sandra travelled to the hallway . John went to the garden . Where is Sandra ? | Prediction: hallway | Ground Truth: hallway\n",
      "John travelled to the hallway . Mary journeyed to the bathroom . Daniel went back to the bathroom . John moved to the bedroom . John went to the hallway . Sandra journeyed to the kitchen . Sandra travelled to the hallway . John went to the garden . Sandra went back to the bathroom . Sandra moved to the kitchen . Where is Sandra ? | Prediction: kitchen | Ground Truth: kitchen\n",
      "Sandra travelled to the kitchen . Sandra travelled to the hallway . Where is Sandra ? | Prediction: hallway | Ground Truth: hallway\n",
      "Sandra travelled to the kitchen . Sandra travelled to the hallway . Mary went to the bathroom . Sandra moved to the garden . Where is Sandra ? | Prediction: garden | Ground Truth: garden\n",
      "Sandra travelled to the kitchen . Sandra travelled to the hallway . Mary went to the bathroom . Sandra moved to the garden . Sandra travelled to the office . Daniel journeyed to the hallway . Where is Daniel ? | Prediction: hallway | Ground Truth: hallway\n",
      "Sandra travelled to the kitchen . Sandra travelled to the hallway . Mary went to the bathroom . Sandra moved to the garden . Sandra travelled to the office . Daniel journeyed to the hallway . Daniel journeyed to the office . John moved to the hallway . Where is Sandra ? | Prediction: office | Ground Truth: office\n",
      "Sandra travelled to the kitchen . Sandra travelled to the hallway . Mary went to the bathroom . Sandra moved to the garden . Sandra travelled to the office . Daniel journeyed to the hallway . Daniel journeyed to the office . John moved to the hallway . John travelled to the bathroom . John journeyed to the office . Where is Daniel ? | Prediction: office | Ground Truth: office\n"
     ]
    }
   ],
   "source": [
    "단계 5: epoch 와 batch size 등을 정하고 학습시킨다. \n",
    "    \n",
    "    \n",
    "train_epochs = 100\n",
    "# 에포크\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "lstm_size = 64\n",
    "\n",
    "\n",
    "\n",
    "# train, batch_size = 32 and epochs = 120\n",
    "\n",
    "print(\"Trainig the model\")\n",
    "model.fit([inputs_train, queries_train], answers_train, \n",
    "          batch_size, # 배치 크기\n",
    "          train_epochs,  # 에포크\n",
    "          validation_data=([inputs_test, queries_test], answers_test))\n",
    "\n",
    "\n",
    "model.save('./data_babi/babi_model1.h5')\n",
    "\n",
    "print('-------------------------------------------------------------------------------------------')\n",
    "print('Qualitative Test Result Analysis')\n",
    "\n",
    "for i in range(0,10):\n",
    "    current_inp = test_stories[i]\n",
    "    current_story, current_query, current_answer = vectorize_stories([current_inp], word_idx, story_maxlen, query_maxlen)\n",
    "    current_prediction = model.predict([current_story, current_query])\n",
    "    current_prediction = idx_word[np.argmax(current_prediction)]\n",
    "    print(' '.join(current_inp[0]), ' '.join(current_inp[1]), '| Prediction:', current_prediction, '| Ground Truth:', current_inp[2])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# +++++++++++++++++++++++++++\n",
    "\n",
    "\n",
    "# 바비 문제 20 가지를 교육적 관점에서 탐구한다\n",
    "\n",
    "<p>\n",
    "    \n",
    "# 한글화로, 수학교육용 등으로 설계함을 탐구한다\n",
    "\n",
    "\n",
    "\n",
    "# +++++++++++++++++++++++++++ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.ipynb 에서 다루었던 yes-no 바비 6번 문제타입을 다룬다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting stories for the challenge: qa6_yes_no_ques_10k\n",
      "-\n",
      "Vocab size: 38 unique words\n",
      "Story max length: 156 words\n",
      "Query max length: 6 words\n",
      "Number of training stories: 10000\n",
      "Number of test stories: 1000\n",
      "-\n",
      "Here's what a \"story\" tuple looks like (input, query, answer):\n",
      "(['Mary', 'moved', 'to', 'the', 'bathroom', '.', 'Sandra', 'journeyed', 'to', 'the', 'bedroom', '.', 'Mary', 'went', 'back', 'to', 'the', 'bedroom', '.', 'Daniel', 'went', 'back', 'to', 'the', 'hallway', '.'], ['Is', 'Daniel', 'in', 'the', 'bathroom', '?'], 'no')\n",
      "-\n",
      "Vectorizing the word sequences...\n",
      "-\n",
      "inputs: integer tensor of shape (samples, max_length)\n",
      "inputs_train shape: (10000, 156)\n",
      "inputs_test shape: (1000, 156)\n",
      "-\n",
      "queries: integer tensor of shape (samples, max_length)\n",
      "queries_train shape: (10000, 6)\n",
      "queries_test shape: (1000, 6)\n",
      "-\n",
      "answers: binary (1 or 0) tensor of shape (samples, vocab_size)\n",
      "answers_train shape: (10000, 38)\n",
      "answers_test shape: (1000, 38)\n",
      "-\n",
      "Compiling...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\WinPython37F\\python-3.7.2.amd64\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "10000/10000 [==============================] - 8s 796us/step - loss: 0.8419 - accuracy: 0.4939 - val_loss: 0.6952 - val_accuracy: 0.5030\n",
      "Epoch 2/10\n",
      "10000/10000 [==============================] - 7s 706us/step - loss: 0.7011 - accuracy: 0.5017 - val_loss: 0.6965 - val_accuracy: 0.4970\n",
      "Epoch 3/10\n",
      "10000/10000 [==============================] - 7s 684us/step - loss: 0.6960 - accuracy: 0.5006 - val_loss: 0.6933 - val_accuracy: 0.5030\n",
      "Epoch 4/10\n",
      "10000/10000 [==============================] - 7s 717us/step - loss: 0.6942 - accuracy: 0.5088 - val_loss: 0.6932 - val_accuracy: 0.5030\n",
      "Epoch 5/10\n",
      "10000/10000 [==============================] - 5s 550us/step - loss: 0.6948 - accuracy: 0.4910 - val_loss: 0.6935 - val_accuracy: 0.4990\n",
      "Epoch 6/10\n",
      "10000/10000 [==============================] - 7s 700us/step - loss: 0.6940 - accuracy: 0.4986 - val_loss: 0.6942 - val_accuracy: 0.4970\n",
      "Epoch 7/10\n",
      "10000/10000 [==============================] - 7s 705us/step - loss: 0.6938 - accuracy: 0.4990 - val_loss: 0.6940 - val_accuracy: 0.4940\n",
      "Epoch 8/10\n",
      "10000/10000 [==============================] - 7s 709us/step - loss: 0.6927 - accuracy: 0.5118 - val_loss: 0.6965 - val_accuracy: 0.4920- accura\n",
      "Epoch 9/10\n",
      "10000/10000 [==============================] - 7s 698us/step - loss: 0.6923 - accuracy: 0.5109 - val_loss: 0.6949 - val_accuracy: 0.4940\n",
      "Epoch 10/10\n",
      "10000/10000 [==============================] - 7s 717us/step - loss: 0.6877 - accuracy: 0.5405 - val_loss: 0.6846 - val_accuracy: 0.5480\n",
      "dict_keys(['val_loss', 'val_accuracy', 'loss', 'accuracy'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Story is: Daniel moved to the hallway . Daniel picked up the football there . Daniel travelled to the garden . Daniel journeyed to the hallway .\n",
      "Question is:  Is Daniel in the kitchen ?\n",
      "Actual answer is:  no\n",
      "Machine answer is:  no\n",
      "I am  0.5618641 certain of it\n"
     ]
    }
   ],
   "source": [
    "'''Trains a memory network on the bAbI dataset.\n",
    "References:\n",
    "- Jason Weston, Antoine Bordes, Sumit Chopra, Tomas Mikolov, Alexander M. Rush,\n",
    "  \"Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks\",\n",
    "  http://arxiv.org/abs/1502.05698\n",
    "- Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, Rob Fergus,\n",
    "  \"End-To-End Memory Networks\",\n",
    "  http://arxiv.org/abs/1503.08895\n",
    "Reaches 98.6% accuracy on task 'single_supporting_fact_10k' after 120 epochs.\n",
    "Time per epoch: 3s on CPU (core i7).\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Input, Activation, Dense, Permute, Dropout\n",
    "from keras.layers import add, dot, concatenate\n",
    "from keras.layers import LSTM\n",
    "#from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from functools import reduce\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "\n",
    "def tokenize(sent):\n",
    "    '''Return the tokens of a sentence including punctuation.\n",
    "    >>> tokenize('Bob dropped the apple. Where is the apple?')\n",
    "    ['Bob', 'dropped', 'the', 'apple', '.', 'Where', 'is', 'the', 'apple', '?']\n",
    "    '''\n",
    "    return [x.strip() for x in re.split('(\\W+)+', sent) if x.strip()]\n",
    "\n",
    "\n",
    "def parse_stories(lines, only_supporting=False):\n",
    "    '''Parse stories provided in the bAbi tasks format\n",
    "    If only_supporting is true, only the sentences\n",
    "    that support the answer are kept.\n",
    "    '''\n",
    "    data = []\n",
    "    story = []\n",
    "    for line in lines:\n",
    "        line = line.decode('utf-8').strip()\n",
    "        nid, line = line.split(' ', 1)\n",
    "        nid = int(nid)\n",
    "        if nid == 1:\n",
    "            story = []\n",
    "        if '\\t' in line:\n",
    "            q, a, supporting = line.split('\\t')\n",
    "            q = tokenize(q)\n",
    "            substory = None\n",
    "            if only_supporting:\n",
    "                # Only select the related substory\n",
    "                supporting = map(int, supporting.split())\n",
    "                substory = [story[i - 1] for i in supporting]\n",
    "            else:\n",
    "                # Provide all the substories\n",
    "                substory = [x for x in story if x]\n",
    "            data.append((substory, q, a))\n",
    "            story.append('')\n",
    "        else:\n",
    "            sent = tokenize(line)\n",
    "            story.append(sent)\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_stories(f, only_supporting=False, max_length=None):\n",
    "    '''Given a file name, read the file,\n",
    "    retrieve the stories,\n",
    "    and then convert the sentences into a single story.\n",
    "    If max_length is supplied,\n",
    "    any stories longer than max_length tokens will be discarded.\n",
    "    '''\n",
    "    data = parse_stories(f.readlines(), only_supporting=only_supporting)\n",
    "    flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
    "    data = [(flatten(story), q, answer) for story, q, answer in data if not max_length or len(flatten(story)) < max_length]\n",
    "    return data\n",
    "\n",
    "\n",
    "def vectorize_stories(data, word_idx, story_maxlen, query_maxlen):\n",
    "    X = []\n",
    "    Xq = []\n",
    "    Y = []\n",
    "    for story, query, answer in data:\n",
    "        x = [word_idx[w] for w in story]\n",
    "        xq = [word_idx[w] for w in query]\n",
    "        # let's not forget that index 0 is reserved\n",
    "        y = np.zeros(len(word_idx) + 1)\n",
    "        y[word_idx[answer]] = 1\n",
    "        X.append(x)\n",
    "        Xq.append(xq)\n",
    "        Y.append(y)\n",
    "    return (pad_sequences(X, maxlen=story_maxlen),\n",
    "            pad_sequences(Xq, maxlen=query_maxlen), np.array(Y))\n",
    "\n",
    "\"\"\"\n",
    "try:\n",
    "    path = get_file('babi-tasks-v1-2.tar.gz', origin='https://s3.amazonaws.com/text-datasets/babi_tasks_1-20_v1-2.tar.gz')\n",
    "except:\n",
    "    print('Error downloading dataset, please download it manually:\\n'\n",
    "          '$ wget http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2.tar.gz\\n'\n",
    "          '$ mv tasks_1-20_v1-2.tar.gz ~/.keras/datasets/babi-tasks-v1-2.tar.gz')\n",
    "    raise\n",
    "\"\"\"\n",
    "    \n",
    "path='./data_babi/babi-tasks-v1-2.tar.gz'\n",
    "tar = tarfile.open(path)\n",
    "\n",
    "challenges = {\n",
    "    # QA1 with 10,000 samples\n",
    "    'single_supporting_fact_10k': 'tasks_1-20_v1-2/en-10k/qa1_single-supporting-fact_{}.txt',\n",
    "    # QA2 with 10,000 samples\n",
    "    'two_supporting_facts_10k': 'tasks_1-20_v1-2/en-10k/qa2_two-supporting-facts_{}.txt',\n",
    "\n",
    "    'two_arg_relations_10k': 'tasks_1-20_v1-2/en-10k/qa4_two-arg-relations_{}.txt',\n",
    "\n",
    "    'qa6_yes_no_ques_10k': 'tasks_1-20_v1-2/en-10k/qa6_yes-no-questions_{}.txt',\n",
    "}\n",
    "\n",
    "challenge_type = 'qa6_yes_no_ques_10k'\n",
    "\n",
    "challenge = challenges[challenge_type]\n",
    "\n",
    "print('Extracting stories for the challenge:', challenge_type)\n",
    "train_stories = get_stories(tar.extractfile(challenge.format('train')))\n",
    "test_stories = get_stories(tar.extractfile(challenge.format('test')))\n",
    "\n",
    "vocab = set()\n",
    "for story, q, answer in train_stories + test_stories:\n",
    "    vocab |= set(story + q + [answer])\n",
    "vocab = sorted(vocab)\n",
    "\n",
    "# Reserve 0 for masking via pad_sequences\n",
    "vocab_size = len(vocab) + 1\n",
    "story_maxlen = max(map(len, (x for x, _, _ in train_stories + test_stories)))\n",
    "query_maxlen = max(map(len, (x for _, x, _ in train_stories + test_stories)))\n",
    "\n",
    "print('-')\n",
    "print('Vocab size:', vocab_size, 'unique words')\n",
    "print('Story max length:', story_maxlen, 'words')\n",
    "print('Query max length:', query_maxlen, 'words')\n",
    "print('Number of training stories:', len(train_stories))\n",
    "print('Number of test stories:', len(test_stories))\n",
    "print('-')\n",
    "print('Here\\'s what a \"story\" tuple looks like (input, query, answer):')\n",
    "print(train_stories[1])\n",
    "print('-')\n",
    "print('Vectorizing the word sequences...')\n",
    "\n",
    "word_idx = dict((c, i + 1) for i, c in enumerate(vocab))\n",
    "inputs_train, queries_train, answers_train = vectorize_stories(train_stories,\n",
    "                                                               word_idx,\n",
    "                                                               story_maxlen,\n",
    "                                                               query_maxlen)\n",
    "inputs_test, queries_test, answers_test = vectorize_stories(test_stories,\n",
    "                                                            word_idx,\n",
    "                                                            story_maxlen,\n",
    "                                                            query_maxlen)\n",
    "\n",
    "print('-')\n",
    "print('inputs: integer tensor of shape (samples, max_length)')\n",
    "print('inputs_train shape:', inputs_train.shape)\n",
    "print('inputs_test shape:', inputs_test.shape)\n",
    "print('-')\n",
    "print('queries: integer tensor of shape (samples, max_length)')\n",
    "print('queries_train shape:', queries_train.shape)\n",
    "print('queries_test shape:', queries_test.shape)\n",
    "print('-')\n",
    "print('answers: binary (1 or 0) tensor of shape (samples, vocab_size)')\n",
    "print('answers_train shape:', answers_train.shape)\n",
    "print('answers_test shape:', answers_test.shape)\n",
    "print('-')\n",
    "print('Compiling...')\n",
    "\n",
    "# placeholders\n",
    "input_sequence = Input((story_maxlen,))\n",
    "question = Input((query_maxlen,))\n",
    "\n",
    "# encoders\n",
    "# embed the input sequence into a sequence of vectors\n",
    "input_encoder_m = Sequential()\n",
    "input_encoder_m.add(Embedding(input_dim=vocab_size,\n",
    "                              output_dim=64))\n",
    "input_encoder_m.add(Dropout(0.3))\n",
    "# output: (samples, story_maxlen, embedding_dim)\n",
    "\n",
    "# embed the input into a sequence of vectors of size query_maxlen\n",
    "input_encoder_c = Sequential()\n",
    "input_encoder_c.add(Embedding(input_dim=vocab_size,\n",
    "                              output_dim=query_maxlen))\n",
    "input_encoder_c.add(Dropout(0.3))\n",
    "# output: (samples, story_maxlen, query_maxlen)\n",
    "\n",
    "# embed the question into a sequence of vectors\n",
    "question_encoder = Sequential()\n",
    "question_encoder.add(Embedding(input_dim=vocab_size,\n",
    "                               output_dim=64,\n",
    "                               input_length=query_maxlen))\n",
    "question_encoder.add(Dropout(0.3))\n",
    "# output: (samples, query_maxlen, embedding_dim)\n",
    "\n",
    "# encode input sequence and questions (which are indices)\n",
    "# to sequences of dense vectors\n",
    "input_encoded_m = input_encoder_m(input_sequence)\n",
    "input_encoded_c = input_encoder_c(input_sequence)\n",
    "question_encoded = question_encoder(question)\n",
    "\n",
    "# compute a 'match' between the first input vector sequence\n",
    "# and the question vector sequence\n",
    "# shape: `(samples, story_maxlen, query_maxlen)`\n",
    "match = dot([input_encoded_m, question_encoded], axes=(2, 2))\n",
    "match = Activation('softmax')(match)\n",
    "\n",
    "# add the match matrix with the second input vector sequence\n",
    "response = add([match, input_encoded_c])  # (samples, story_maxlen, query_maxlen)\n",
    "response = Permute((2, 1))(response)  # (samples, query_maxlen, story_maxlen)\n",
    "\n",
    "# concatenate the match matrix with the question vector sequence\n",
    "answer = concatenate([response, question_encoded])\n",
    "\n",
    "# the original paper uses a matrix multiplication for this reduction step.\n",
    "# we choose to use a RNN instead.\n",
    "answer = LSTM(32)(answer)  # (samples, 32)\n",
    "\n",
    "# one regularization layer -- more would probably be needed.\n",
    "answer = Dropout(0.3)(answer)\n",
    "answer = Dense(vocab_size)(answer)  # (samples, vocab_size)\n",
    "# we output a probability distribution over the vocabularypath='./data_babi/babi-tasks-v1-2.tar.gz'\n",
    "answer = Activation('softmax')(answer)\n",
    "\n",
    "# build the final model\n",
    "model = Model([input_sequence, question], answer)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# train\n",
    "history = model.fit([inputs_train, queries_train], answers_train,\n",
    "          batch_size=32,\n",
    "          epochs=10, # 에포크\n",
    "          validation_data=([inputs_test, queries_test], answers_test))\n",
    "\n",
    "# list all data in history\n",
    "import matplotlib.pyplot as plt\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "model_path1 = './data_babi/model6.h5'\n",
    "model.save(model_path1)\n",
    "#model save as pickle file\n",
    "# model load again\n",
    "# write story answer question in the format in a text file\n",
    "\n",
    "model.load_weights(model_path1)\n",
    "pred_results = model.predict(([inputs_test, queries_test]))\n",
    "# Display a selected test story\n",
    "\n",
    "n = np.random.randint(0,1000)\n",
    "story_list = test_stories[n][0]\n",
    "story =' '.join(word for word in story_list)\n",
    "print(\"Story is:\",story)\n",
    "\n",
    "question_list = test_stories[n][1]\n",
    "ques =' '.join(word for word in question_list)\n",
    "print(\"Question is: \",ques)\n",
    "\n",
    "ans = test_stories[n][2]\n",
    "print(\"Actual answer is: \", ans)\n",
    "\n",
    "#Generate prediction from model\n",
    "\n",
    "val_max = np.argmax(pred_results[n])\n",
    "\n",
    "for key, val in word_idx.items():\n",
    "    if val == val_max:\n",
    "        k = key\n",
    "\n",
    "print(\"Machine answer is: \", k)\n",
    "print(\"I am \", pred_results[n][val_max], \"certain of it\")\n",
    "\n",
    "## Read my own file\n",
    "\n",
    "# f = open(r\"C:\\Users\\priya\\Documents\\my_dl\\qachatbot\\my_test_q2.txt\", \"r\")\n",
    "# print(f.readlines())\n",
    "# data = parse_stories(f.readlines(), only_supporting=False)\n",
    "# print(data)\n",
    "# extra_stories = get_stories(f, only_supporting=False, max_length=None)\n",
    "#\n",
    "# print(extra_stories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 바비 2번 문제타입을 다룬다. 다른 문제들도 해본다 !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting stories for the challenge: two_arg_relations_10k\n",
      "-\n",
      "Vocab size: 18 unique words\n",
      "Story max length: 16 words\n",
      "Query max length: 7 words\n",
      "Number of training stories: 10000\n",
      "Number of test stories: 1000\n",
      "-\n",
      "Here's what a \"story\" tuple looks like (input, query, answer):\n",
      "(['The', 'kitchen', 'is', 'west', 'of', 'the', 'garden', '.', 'The', 'hallway', 'is', 'west', 'of', 'the', 'kitchen', '.'], ['What', 'is', 'the', 'garden', 'east', 'of', '?'], 'kitchen')\n",
      "-\n",
      "Vectorizing the word sequences...\n",
      "-\n",
      "inputs: integer tensor of shape (samples, max_length)\n",
      "inputs_train shape: (10000, 16)\n",
      "inputs_test shape: (1000, 16)\n",
      "-\n",
      "queries: integer tensor of shape (samples, max_length)\n",
      "queries_train shape: (10000, 7)\n",
      "queries_test shape: (1000, 7)\n",
      "-\n",
      "answers: binary (1 or 0) tensor of shape (samples, vocab_size)\n",
      "answers_train shape: (10000, 18)\n",
      "answers_test shape: (1000, 18)\n",
      "-\n",
      "Compiling...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\WinPython37F\\python-3.7.2.amd64\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "10000/10000 [==============================] - 3s 254us/step - loss: 1.8960 - accuracy: 0.1970 - val_loss: 1.5719 - val_accuracy: 0.3680\n",
      "Epoch 2/10\n",
      "10000/10000 [==============================] - 2s 180us/step - loss: 1.4860 - accuracy: 0.3826 - val_loss: 1.3210 - val_accuracy: 0.4390\n",
      "Epoch 3/10\n",
      "10000/10000 [==============================] - 2s 168us/step - loss: 1.2945 - accuracy: 0.4501 - val_loss: 1.1011 - val_accuracy: 0.4990\n",
      "Epoch 4/10\n",
      "10000/10000 [==============================] - 2s 170us/step - loss: 1.1859 - accuracy: 0.4966 - val_loss: 0.9924 - val_accuracy: 0.5450\n",
      "Epoch 5/10\n",
      "10000/10000 [==============================] - 2s 188us/step - loss: 1.0907 - accuracy: 0.5402 - val_loss: 0.9191 - val_accuracy: 0.6230\n",
      "Epoch 6/10\n",
      "10000/10000 [==============================] - 2s 205us/step - loss: 1.0000 - accuracy: 0.5730 - val_loss: 0.7928 - val_accuracy: 0.6340\n",
      "Epoch 7/10\n",
      "10000/10000 [==============================] - 2s 204us/step - loss: 0.9615 - accuracy: 0.5895 - val_loss: 0.7473 - val_accuracy: 0.6810\n",
      "Epoch 8/10\n",
      "10000/10000 [==============================] - 2s 199us/step - loss: 0.9219 - accuracy: 0.6061 - val_loss: 0.6982 - val_accuracy: 0.6940\n",
      "Epoch 9/10\n",
      "10000/10000 [==============================] - 2s 202us/step - loss: 0.8665 - accuracy: 0.6304 - val_loss: 0.6637 - val_accuracy: 0.7090\n",
      "Epoch 10/10\n",
      "10000/10000 [==============================] - 2s 205us/step - loss: 0.8167 - accuracy: 0.6526 - val_loss: 0.5715 - val_accuracy: 0.7490\n",
      "Story is: The office is west of the kitchen . The hallway is east of the kitchen .\n",
      "Question is:  What is the kitchen west of ?\n",
      "Actual answer is:  hallway\n",
      "Machine answer is:  office\n",
      "I am  0.5613385 certain of it\n"
     ]
    }
   ],
   "source": [
    "'''Trains a memory network on the bAbI dataset.\n",
    "References:\n",
    "- Jason Weston, Antoine Bordes, Sumit Chopra, Tomas Mikolov, Alexander M. Rush,\n",
    "  \"Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks\",\n",
    "  http://arxiv.org/abs/1502.05698\n",
    "- Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, Rob Fergus,\n",
    "  \"End-To-End Memory Networks\",\n",
    "  http://arxiv.org/abs/1503.08895\n",
    "Reaches 98.6% accuracy on task 'single_supporting_fact_10k' after 120 epochs.\n",
    "Time per epoch: 3s on CPU (core i7).\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Input, Activation, Dense, Permute, Dropout\n",
    "from keras.layers import add, dot, concatenate\n",
    "from keras.layers import LSTM\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from functools import reduce\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "\n",
    "def tokenize(sent):\n",
    "    '''Return the tokens of a sentence including punctuation.\n",
    "    >>> tokenize('Bob dropped the apple. Where is the apple?')\n",
    "    ['Bob', 'dropped', 'the', 'apple', '.', 'Where', 'is', 'the', 'apple', '?']\n",
    "    '''\n",
    "    return [x.strip() for x in re.split('(\\W+)+', sent) if x.strip()]\n",
    "\n",
    "\n",
    "def parse_stories(lines, only_supporting=False):\n",
    "    '''Parse stories provided in the bAbi tasks format\n",
    "    If only_supporting is true, only the sentences\n",
    "    that support the answer are kept.\n",
    "    '''\n",
    "    data = []\n",
    "    story = []\n",
    "    for line in lines:\n",
    "        line = line.decode('utf-8').strip()\n",
    "        nid, line = line.split(' ', 1)\n",
    "        nid = int(nid)\n",
    "        if nid == 1:\n",
    "            story = []\n",
    "        if '\\t' in line:\n",
    "            q, a, supporting = line.split('\\t')\n",
    "            q = tokenize(q)\n",
    "            substory = None\n",
    "            if only_supporting:\n",
    "                # Only select the related substory\n",
    "                supporting = map(int, supporting.split())\n",
    "                substory = [story[i - 1] for i in supporting]\n",
    "            else:\n",
    "                # Provide all the substories\n",
    "                substory = [x for x in story if x]\n",
    "            data.append((substory, q, a))\n",
    "            story.append('')\n",
    "        else:\n",
    "            sent = tokenize(line)\n",
    "            story.append(sent)\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_stories(f, only_supporting=False, max_length=None):\n",
    "    '''Given a file name, read the file,\n",
    "    retrieve the stories,\n",
    "    and then convert the sentences into a single story.\n",
    "    If max_length is supplied,\n",
    "    any stories longer than max_length tokens will be discarded.\n",
    "    '''\n",
    "    data = parse_stories(f.readlines(), only_supporting=only_supporting)\n",
    "    flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
    "    data = [(flatten(story), q, answer) for story, q, answer in data if not max_length or len(flatten(story)) < max_length]\n",
    "    return data\n",
    "\n",
    "\n",
    "def vectorize_stories(data, word_idx, story_maxlen, query_maxlen):\n",
    "    X = []\n",
    "    Xq = []\n",
    "    Y = []\n",
    "    for story, query, answer in data:\n",
    "        x = [word_idx[w] for w in story]\n",
    "        xq = [word_idx[w] for w in query]\n",
    "        # let's not forget that index 0 is reserved\n",
    "        y = np.zeros(len(word_idx) + 1)\n",
    "        y[word_idx[answer]] = 1\n",
    "        X.append(x)\n",
    "        Xq.append(xq)\n",
    "        Y.append(y)\n",
    "    return (pad_sequences(X, maxlen=story_maxlen),\n",
    "            pad_sequences(Xq, maxlen=query_maxlen), np.array(Y))\n",
    "\"\"\"\n",
    "try:\n",
    "    path = get_file('babi-tasks-v1-2.tar.gz', origin='https://s3.amazonaws.com/text-datasets/babi_tasks_1-20_v1-2.tar.gz')\n",
    "except:\n",
    "    print('Error downloading dataset, please download it manually:\\n'\n",
    "          '$ wget http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2.tar.gz\\n'\n",
    "          '$ mv tasks_1-20_v1-2.tar.gz ~/.keras/datasets/babi-tasks-v1-2.tar.gz')\n",
    "    raise\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "path='./data_babi/babi-tasks-v1-2.tar.gz'\n",
    "\n",
    "tar = tarfile.open(path)\n",
    "\n",
    "challenges = {\n",
    "    # QA1 with 10,000 samples\n",
    "    'single_supporting_fact_10k': 'tasks_1-20_v1-2/en-10k/qa1_single-supporting-fact_{}.txt',\n",
    "    # QA2 with 10,000 samples\n",
    "    'two_supporting_facts_10k': 'tasks_1-20_v1-2/en-10k/qa2_two-supporting-facts_{}.txt',\n",
    "\n",
    "    'two_arg_relations_10k': 'tasks_1-20_v1-2/en-10k/qa4_two-arg-relations_{}.txt',\n",
    "\n",
    "    'qa6_yes_no_ques_10k': 'tasks_1-20_v1-2/en-10k/qa6_yes-no-questions_{}.txt',\n",
    "}\n",
    "\n",
    "\n",
    "challenge_type = 'two_arg_relations_10k'\n",
    "challenge = challenges[challenge_type]\n",
    "\n",
    "print('Extracting stories for the challenge:', challenge_type)\n",
    "train_stories = get_stories(tar.extractfile(challenge.format('train')))\n",
    "test_stories = get_stories(tar.extractfile(challenge.format('test')))\n",
    "\n",
    "vocab = set()\n",
    "for story, q, answer in train_stories + test_stories:\n",
    "    vocab |= set(story + q + [answer])\n",
    "vocab = sorted(vocab)\n",
    "\n",
    "# Reserve 0 for masking via pad_sequences\n",
    "vocab_size = len(vocab) + 1\n",
    "story_maxlen = max(map(len, (x for x, _, _ in train_stories + test_stories)))\n",
    "query_maxlen = max(map(len, (x for _, x, _ in train_stories + test_stories)))\n",
    "\n",
    "print('-')\n",
    "print('Vocab size:', vocab_size, 'unique words')\n",
    "print('Story max length:', story_maxlen, 'words')\n",
    "print('Query max length:', query_maxlen, 'words')\n",
    "print('Number of training stories:', len(train_stories))\n",
    "print('Number of test stories:', len(test_stories))\n",
    "print('-')\n",
    "print('Here\\'s what a \"story\" tuple looks like (input, query, answer):')\n",
    "print(train_stories[1])\n",
    "print('-')\n",
    "print('Vectorizing the word sequences...')\n",
    "\n",
    "word_idx = dict((c, i + 1) for i, c in enumerate(vocab))\n",
    "inputs_train, queries_train, answers_train = vectorize_stories(train_stories,\n",
    "                                                               word_idx,\n",
    "                                                               story_maxlen,\n",
    "                                                               query_maxlen)\n",
    "inputs_test, queries_test, answers_test = vectorize_stories(test_stories,\n",
    "                                                            word_idx,\n",
    "                                                            story_maxlen,\n",
    "                                                            query_maxlen)\n",
    "\n",
    "print('-')\n",
    "print('inputs: integer tensor of shape (samples, max_length)')\n",
    "print('inputs_train shape:', inputs_train.shape)\n",
    "print('inputs_test shape:', inputs_test.shape)\n",
    "print('-')\n",
    "print('queries: integer tensor of shape (samples, max_length)')\n",
    "print('queries_train shape:', queries_train.shape)\n",
    "print('queries_test shape:', queries_test.shape)\n",
    "print('-')\n",
    "print('answers: binary (1 or 0) tensor of shape (samples, vocab_size)')\n",
    "print('answers_train shape:', answers_train.shape)\n",
    "print('answers_test shape:', answers_test.shape)\n",
    "print('-')\n",
    "print('Compiling...')\n",
    "\n",
    "# placeholders\n",
    "input_sequence = Input((story_maxlen,))\n",
    "question = Input((query_maxlen,))\n",
    "\n",
    "# encoders\n",
    "# embed the input sequence into a sequence of vectors\n",
    "input_encoder_m = Sequential()\n",
    "input_encoder_m.add(Embedding(input_dim=vocab_size,\n",
    "                              output_dim=64))\n",
    "input_encoder_m.add(Dropout(0.3))\n",
    "# output: (samples, story_maxlen, embedding_dim)\n",
    "\n",
    "# embed the input into a sequence of vectors of size query_maxlen\n",
    "input_encoder_c = Sequential()\n",
    "input_encoder_c.add(Embedding(input_dim=vocab_size,\n",
    "                              output_dim=query_maxlen))\n",
    "input_encoder_c.add(Dropout(0.3))\n",
    "# output: (samples, story_maxlen, query_maxlen)\n",
    "\n",
    "# embed the question into a sequence of vectors\n",
    "question_encoder = Sequential()\n",
    "question_encoder.add(Embedding(input_dim=vocab_size,\n",
    "                               output_dim=64,\n",
    "                               input_length=query_maxlen))\n",
    "question_encoder.add(Dropout(0.3))\n",
    "# output: (samples, query_maxlen, embedding_dim)\n",
    "\n",
    "# encode input sequence and questions (which are indices)\n",
    "# to sequences of dense vectors\n",
    "input_encoded_m = input_encoder_m(input_sequence)\n",
    "input_encoded_c = input_encoder_c(input_sequence)\n",
    "question_encoded = question_encoder(question)\n",
    "\n",
    "# compute a 'match' between the first input vector sequence\n",
    "# and the question vector sequence\n",
    "# shape: `(samples, story_maxlen, query_maxlen)`\n",
    "match = dot([input_encoded_m, question_encoded], axes=(2, 2))\n",
    "match = Activation('softmax')(match)\n",
    "\n",
    "# add the match matrix with the second input vector sequence\n",
    "response = add([match, input_encoded_c])  # (samples, story_maxlen, query_maxlen)\n",
    "response = Permute((2, 1))(response)  # (samples, query_maxlen, story_maxlen)\n",
    "\n",
    "# concatenate the match matrix with the question vector sequence\n",
    "answer = concatenate([response, question_encoded])\n",
    "\n",
    "# the original paper uses a matrix multiplication for this reduction step.\n",
    "# we choose to use a RNN instead.\n",
    "answer = LSTM(32)(answer)  # (samples, 32)\n",
    "\n",
    "# one regularization layer -- more would probably be needed.\n",
    "answer = Dropout(0.3)(answer)\n",
    "answer = Dense(vocab_size)(answer)  # (samples, vocab_size)\n",
    "# we output a probability distribution over the vocabulary\n",
    "answer = Activation('softmax')(answer)\n",
    "\n",
    "# build the final model\n",
    "model = Model([input_sequence, question], answer)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# train\n",
    "history = model.fit([inputs_train, queries_train], answers_train,\n",
    "          batch_size=32,\n",
    "          epochs=10,  # 에포크 \n",
    "          validation_data=([inputs_test, queries_test], answers_test))\n",
    "\n",
    "\n",
    "model_path1 = './data_babi/model4.h5'\n",
    "model.save(model_path1)\n",
    "#model save as pickle file\n",
    "# model load again\n",
    "# write story answer question in the format in a text file\n",
    "\n",
    "model.load_weights(model_path1)\n",
    "pred_results = model.predict(([inputs_test, queries_test]))\n",
    "# Display a selected test story\n",
    "\n",
    "n = np.random.randint(0,1000)\n",
    "story_list = test_stories[n][0]\n",
    "story =' '.join(word for word in story_list)\n",
    "print(\"Story is:\",story)\n",
    "\n",
    "question_list = test_stories[n][1]\n",
    "ques =' '.join(word for word in question_list)\n",
    "print(\"Question is: \",ques)\n",
    "\n",
    "ans = test_stories[n][2]\n",
    "print(\"Actual answer is: \", ans)\n",
    "\n",
    "#Generate prediction from model\n",
    "\n",
    "val_max = np.argmax(pred_results[n])\n",
    "\n",
    "for key, val in word_idx.items():\n",
    "    if val == val_max:\n",
    "        k = key\n",
    "\n",
    "print(\"Machine answer is: \", k)\n",
    "print(\"I am \", pred_results[n][val_max], \"certain of it\")\n",
    "\n",
    "## Read my own file\n",
    "\n",
    "# f = open(r\"C:\\Users\\priya\\Documents\\my_dl\\qachatbot\\my_test_q2.txt\", \"r\")\n",
    "# print(f.readlines())\n",
    "# data = parse_stories(f.readlines(), only_supporting=False)\n",
    "# print(data)\n",
    "# extra_stories = get_stories(f, only_supporting=False, max_length=None)\n",
    "#\n",
    "# print(extra_stories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

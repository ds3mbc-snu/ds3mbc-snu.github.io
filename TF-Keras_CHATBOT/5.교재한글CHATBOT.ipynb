{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6장 .  한글 챗봇 (데이터, seq2seq, 트랜스포머)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =========================\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =========================\n",
    "\n",
    "\n",
    "# 6.3 : 시퀀스 투 시퀀스 모델링\n",
    "\n",
    "\n",
    "\n",
    "# 한글 챗봇 파이썬 코드 (seq2seq 모델)\n",
    "\n",
    "## data_outseq2 디렉토리 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 패키지 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "# 텐서플로우 2.0 사용 \n",
    "\n",
    "\n",
    "import tensorflow.compat.v1  as tf\n",
    "\n",
    "\n",
    "# import model as ml == 자체 사용 \n",
    "# import data == 자체 시용 \n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "\n",
    "#from configs import DEFINES == 자체 사용 \n",
    "#########################################\n",
    "\n",
    "\n",
    "print( tf.__version__ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-*- coding: utf-8 -*-\n",
    "#import tensorflow as tf\n",
    "#import tensorflow.compat.v1  as tf\n",
    "\n",
    "\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel') \n",
    "# 주피터에서 커널에 전달하기 위한 프레그 방법\n",
    "\n",
    "\n",
    "tf.app.flags.DEFINE_integer('batch_size', 64, 'batch size') # 배치 크기\n",
    "tf.app.flags.DEFINE_integer('train_steps', 25000, 'train steps') # 학습 에포크\n",
    "tf.app.flags.DEFINE_float('dropout_width', 0.5, 'dropout width') # 드롭아웃 크기\n",
    "tf.app.flags.DEFINE_integer('layer_size', 3, 'layer size') # 멀티 레이어 크기 (multi rnn)\n",
    "tf.app.flags.DEFINE_integer('hidden_size', 128, 'weights size') # 가중치 크기\n",
    "tf.app.flags.DEFINE_float('learning_rate', 1e-3, 'learning rate') # 학습률\n",
    "tf.app.flags.DEFINE_string('data_path', './data_in/ChatBotData.csv', 'data path') #  데이터 위치\n",
    "tf.app.flags.DEFINE_string('vocabulary_path', './data_outseq2/vocabularyData.voc', 'vocabulary path') # 사전 위치\n",
    "tf.app.flags.DEFINE_string('check_point_path', './data_outseq2/check_point', 'check point path') # 체크 포인트 위치\n",
    "tf.app.flags.DEFINE_integer('shuffle_seek', 1000, 'shuffle random seek') # 셔플 시드값\n",
    "tf.app.flags.DEFINE_integer('max_sequence_length', 25, 'max sequence length') # 시퀀스 길이\n",
    "tf.app.flags.DEFINE_integer('embedding_size', 128, 'embedding size') # 임베딩 크기\n",
    "tf.app.flags.DEFINE_boolean('tokenize_as_morph', True, 'set morph tokenize') # 형태소에 따른 토크나이징 사용 유무\n",
    "tf.app.flags.DEFINE_boolean('embedding', True, 'Use Embedding flag') # 임베딩 유무 설정\n",
    "tf.app.flags.DEFINE_boolean('multilayer', True, 'Use Multi RNN Cell') # 멀티 RNN 유무\n",
    "\n",
    "\n",
    "# Define FLAGS\n",
    "\n",
    "\n",
    "DEFINES = tf.app.flags.FLAGS\n",
    "\n",
    "#############################\n",
    "\n",
    "# import tensorflow.compat.v1  as tf  에서만  tf.app\n",
    "\n",
    "# 여러번 실행시키면 에러 \n",
    "# kernel 을 restart 시킴 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data_in/ChatBotData.csv\n"
     ]
    }
   ],
   "source": [
    "# tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
    "# 어떤 경우에는 필요 f 가 필요한다고 하는 경우\n",
    "# ./../ 란 이전 디렉토리를 말함 \n",
    "# ./../data_in/ChatBotData.csv\n",
    "\n",
    "print( DEFINES.data_path )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 챗봇 데이터와 단어장 연습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Q            A  label\n",
       "0           12시 땡!   하루가 또 가네요.      0\n",
       "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
       "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "4          PPL 심하네   눈살이 찌푸려지죠.      0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "data = pd.read_csv( DEFINES.data_path , encoding='utf-8')\n",
    "\n",
    "data.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0             12시 땡!\n",
      "1        1지망 학교 떨어졌어\n",
      "2       3박4일 놀러가고 싶다\n",
      "3    3박4일 정도 놀러가고 싶다\n",
      "4            PPL 심하네\n",
      "Name: Q, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print( data['Q'][0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0\n",
      "1    0\n",
      "2    0\n",
      "3    0\n",
      "4    0\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print( data['label'][0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 911.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['12시 땡 !', '1 지망 학교 떨어졌어', '3 박 4일 놀러가고싶다', '3 박 4일 정도놀러가고싶다', 'PPL 심하네', '하루 가 또 가네요 .', '위로 해드립니다 .', '여행 은 언제나 좋죠 .', '여행 은 언제나 좋죠 .', '눈살 이 찌푸려지죠 .']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt # Twitter\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "questions, answers  = list( data['Q'][0:5] ) , list( data['A'][0:5] )\n",
    "\n",
    "\n",
    "#if DEFINES.tokenize_as_morph:  # 형태소에 따른 토크나이져 처리\n",
    "#questions = prepro_like_morphlized( questions )\n",
    "###########################################\n",
    "#answers = prepro_like_morphlized( answers )\n",
    "\n",
    "    \n",
    "    \n",
    "morph_analyzer = Okt()  # Twitter()\n",
    "# 형태소 토크나이즈 결과 문장을 받을\n",
    "#  리스트를 생성합니다.\n",
    "\n",
    "result_data = list()\n",
    "# 데이터에 있는 매 문장에 대해 토크나이즈를\n",
    "# 할 수 있도록 반복문을 선언합니다.\n",
    "\n",
    "\n",
    "# Twitter.morphs 함수를 통해 토크나이즈 된\n",
    "# 리스트 객체를 받고 다시 공백문자를 기준으로\n",
    "# 하여 문자열로 재구성 해줍니다.\n",
    "for seq in tqdm( questions + answers ):\n",
    "    morphlized_seq = \" \".join(morph_analyzer.morphs(seq.replace(' ', '')))\n",
    "    result_data.append(morphlized_seq)\n",
    "    \n",
    "    \n",
    "questions = result_data         \n",
    "        \n",
    "        \n",
    "datas = []\n",
    "# 질문과 답변을 extend을 통해서 구조가 없는 배열로 만든다.\n",
    "datas.extend(questions)\n",
    "# datas.extend(answers)\n",
    "\n",
    "print( datas )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "['12시 땡 !', '1 지망 학교 떨어졌어', '3 박 4일 놀러가고싶다', '3 박 4일 정도놀러가고싶다', 'PPL 심하네', '하루 가 또 가네요 .', '위로 해드립니다 .', '여행 은 언제나 좋죠 .', '여행 은 언제나 좋죠 .', '눈살 이 찌푸려지죠 .']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['12시', '땡', '1', '지망', '학교', '떨어졌어', '3', '박', '4일', '놀러가고싶다', '3', '박', '4일', '정도놀러가고싶다', 'PPL', '심하네', '하루', '가', '또', '가네요', '위로', '해드립니다', '여행', '은', '언제나', '좋죠', '여행', '은', '언제나', '좋죠', '눈살', '이', '찌푸려지죠']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "\n",
    "FILTERS = \"([~.,!?\\\"':;)(])\"\n",
    "CHANGE_FILTER = re.compile(FILTERS)\n",
    "\n",
    "\n",
    "vocabwords = []\n",
    "for sentence in datas:\n",
    "    # FILTERS = \"([~.,!?\\\"':;)(])\"\n",
    "    # 위 필터와 같은 값들을 정규화 표현식을 \n",
    "    # 통해서 모두 \"\" 으로 변환 해주는 부분이다.\n",
    "    sentence = re.sub(CHANGE_FILTER, \"\", sentence)\n",
    "    for word in sentence.split():\n",
    "        vocabwords.append(word)\n",
    "            \n",
    "print( vocabwords )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "['12시', '땡', '1', '지망', '학교', '떨어졌어', '3', '박', '4일', '놀러가고싶다', '3', '박', '4일', '정도놀러가고싶다', 'PPL', '심하네', '하루', '가', '또', '가네요', '위로', '해드립니다', '여행', '은', '언제나', '좋죠', '여행', '은', '언제나', '좋죠', '눈살', '이', '찌푸려지죠']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 챗봇 데이터 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    # 판다스를 통해서 데이터를 불러온다.\n",
    "    data_df = pd.read_csv(DEFINES.data_path, header=0)\n",
    "    # 질문과 답변 열을 가져와 question과 answer에 넣는다.\n",
    "    question, answer = list(data_df['Q']), list(data_df['A'])\n",
    "    # skleran에서 지원하는 함수를 통해서 학습 셋과 \n",
    "    # 테스트 셋을 나눈다.\n",
    "    \n",
    "    train_input, eval_input, train_label, eval_label = train_test_split(question, answer, test_size=0.33, random_state=42)\n",
    "    \n",
    "    # 그 값을 리턴한다.\n",
    "    return train_input, train_label, eval_input, eval_label\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_input, train_label, eval_input, eval_label = load_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['짝사랑 했던 여자가 떠나갔네.', '대리 불렀는데 안 오네', '내가 싫은 건 너도 싫어해줘서 고마워', '썸은 왜 타?', '짝녀 프사 자음이 궁금함.']\n"
     ]
    }
   ],
   "source": [
    "print( train_input[0:5] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['여기까지 인연이었나봅니다.', '잘했어요.', '싫은 게 통해야 편하죠.', '사귀기 전에 마음을 확인하는 단계라서 그렇지 않을까요?', '궁금할 수 있어요.']\n"
     ]
    }
   ],
   "source": [
    "print( train_label[0:5] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사전 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.py 파일 \n",
    "#import tensorflow as tf\n",
    "#import os\n",
    "#from configs import DEFINES\n",
    "\n",
    "import enum\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from konlpy.tag import Okt # Twitter\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "PAD_MASK = 0\n",
    "NON_PAD_MASK = 1\n",
    "\n",
    "FILTERS = \"([~.,!?\\\"':;)(])\"\n",
    "PAD = \"<PADDING>\"\n",
    "STD = \"<START>\"\n",
    "END = \"<END>\"\n",
    "UNK = \"<UNKNWON>\"\n",
    "\n",
    "PAD_INDEX = 0\n",
    "STD_INDEX = 1\n",
    "END_INDEX = 2\n",
    "UNK_INDEX = 3\n",
    "\n",
    "MARKER = [PAD, STD, END, UNK]\n",
    "CHANGE_FILTER = re.compile(FILTERS)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_vocabulary():\n",
    "    # 사전을 담을 배열 준비한다.\n",
    "    vocabulary_list = []\n",
    "    # 사전을 구성한 후 파일로 저장 진행한다. \n",
    "    # 그 파일의 존재 유무를 확인한다.\n",
    "    if (not (os.path.exists(DEFINES.vocabulary_path))):\n",
    "        # 이미 생성된 사전 파일이 존재하지 않으므로 \n",
    "        # 데이터를 가지고 만들어야 한다.\n",
    "        # 그래서 데이터가 존재 하면 사전을 만들기 위해서 \n",
    "        # 데이터 파일의 존재 유무를 확인한다.\n",
    "        if (os.path.exists(DEFINES.data_path)):\n",
    "            # 데이터가 존재하니 판단스를 통해서 \n",
    "            # 데이터를 불러오자\n",
    "            data_df = pd.read_csv(DEFINES.data_path, encoding='utf-8')\n",
    "            # 판다스의 데이터 프레임을 통해서 \n",
    "            # 질문과 답에 대한 열을 가져 온다.\n",
    "            question, answer = list(data_df['Q']), list(data_df['A'])\n",
    "            if DEFINES.tokenize_as_morph:  # 형태소에 따른 토크나이져 처리\n",
    "                question = prepro_like_morphlized(question)\n",
    "                ###########################################\n",
    "                answer = prepro_like_morphlized(answer)\n",
    "                \n",
    "            data = []\n",
    "            # 질문과 답변을 extend을 \n",
    "            # 통해서 구조가 없는 배열로 만든다.\n",
    "            data.extend(question)\n",
    "            data.extend(answer)\n",
    "            # 토큰나이져 처리 하는 부분이다.\n",
    "            \n",
    "            words = data_tokenizer(data)\n",
    "            ############################\n",
    "            \n",
    "            \n",
    "            # 공통적인 단어에 대해서는 모두 \n",
    "            # 필요 없으므로 한개로 만들어 주기 위해서\n",
    "            # set해주고 이것들을 리스트로 만들어 준다.\n",
    "            words = list(set(words))\n",
    "            # 데이터 없는 내용중에 MARKER를 사전에 \n",
    "            # 추가 하기 위해서 아래와 같이 처리 한다.\n",
    "            # 아래는 MARKER 값이며 리스트의 첫번째 부터 \n",
    "            # 순서대로 넣기 위해서 인덱스 0에 추가한다.\n",
    "            # PAD = \"<PADDING>\"\n",
    "            # STD = \"<START>\"\n",
    "            # END = \"<END>\"\n",
    "            # UNK = \"<UNKNWON>\"     \n",
    "            words[:0] = MARKER\n",
    "        # 사전을 리스트로 만들었으니 이 내용을 \n",
    "        # 사전 파일을 만들어 넣는다.\n",
    "        with open(DEFINES.vocabulary_path, 'w') as vocabulary_file:\n",
    "            for word in words:\n",
    "                vocabulary_file.write(word + '\\n')\n",
    "\n",
    "    # 사전 파일이 존재하면 여기에서 \n",
    "    # 그 파일을 불러서 배열에 넣어 준다.\n",
    "    with open(DEFINES.vocabulary_path, 'r', encoding='utf-8') as vocabulary_file:\n",
    "        for line in vocabulary_file:\n",
    "            vocabulary_list.append(line.strip())\n",
    "\n",
    "    # 배열에 내용을 키와 값이 있는 \n",
    "    # 딕셔너리 구조로 만든다.\n",
    "    char2idx, idx2char = make_vocabulary(vocabulary_list)\n",
    "    ######################################################\n",
    "    \n",
    "    # 두가지 형태의 키와 값이 있는 형태를 리턴한다. \n",
    "    # (예) 단어: 인덱스 , 인덱스: 단어)\n",
    "    return char2idx, idx2char, len(char2idx)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def make_vocabulary(vocabulary_list):\n",
    "    # 리스트를 키가 단어이고 값이 인덱스인 \n",
    "    # 딕셔너리를 만든다.\n",
    "    char2idx = {char: idx for idx, char in enumerate(vocabulary_list)}\n",
    "    # 리스트를 키가 인덱스이고 값이 단어인 \n",
    "    # 딕셔너리를 만든다.\n",
    "    idx2char = {idx: char for idx, char in enumerate(vocabulary_list)}\n",
    "    # 두개의 딕셔너리를 넘겨 준다.\n",
    "    return char2idx, idx2char\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def prepro_like_morphlized(data):\n",
    "    # 형태소 분석 모듈 객체를\n",
    "    # 생성합니다.\n",
    "\n",
    "    morph_analyzer = Okt()  # Twitter()\n",
    "    # 형태소 토크나이즈 결과 문장을 받을\n",
    "    #  리스트를 생성합니다.\n",
    "    result_data = list()\n",
    "    # 데이터에 있는 매 문장에 대해 토크나이즈를\n",
    "    # 할 수 있도록 반복문을 선언합니다.\n",
    "    for seq in tqdm(data):\n",
    "        # Twitter.morphs 함수를 통해 토크나이즈 된\n",
    "        # 리스트 객체를 받고 다시 공백문자를 기준으로\n",
    "        # 하여 문자열로 재구성 해줍니다.\n",
    "        morphlized_seq = \" \".join(morph_analyzer.morphs(seq.replace(' ', '')))\n",
    "        result_data.append(morphlized_seq)\n",
    "\n",
    "    return result_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def data_tokenizer(data):\n",
    "    # 토크나이징 해서 담을 배열 생성\n",
    "    words = []\n",
    "    for sentence in data:\n",
    "        # FILTERS = \"([~.,!?\\\"':;)(])\"\n",
    "        # 위 필터와 같은 값들을 정규화 표현식을 \n",
    "        # 통해서 모두 \"\" 으로 변환 해주는 부분이다.\n",
    "        sentence = re.sub(CHANGE_FILTER, \"\", sentence)\n",
    "        for word in sentence.split():\n",
    "            words.append(word)\n",
    "    # 토그나이징과 정규표현식을 통해 만들어진 \n",
    "    # 값들을 넘겨 준다.\n",
    "    return [word for word in words if word]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_OUT_PATH = './data_outseq2/'  # 현재 디렉토리에서의 서브디렉토리\n",
    "\n",
    "data_out_path = os.path.join(os.getcwd(), DATA_OUT_PATH)\n",
    "\n",
    "os.makedirs( data_out_path, exist_ok=True )\n",
    "\n",
    "\n",
    "char2idx,  idx2char, vocabulary_length = load_vocabulary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "###################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 함수\n",
    "\n",
    "    def enc_processing(value, dictionary):\n",
    "    \n",
    "    def dec_input_processing(value, dictionary):\n",
    "    \n",
    "    def dec_target_processing(value, dictionary):\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 인덱스화 할 value와 키가 워드이고 \n",
    "# 값이 인덱스인 딕셔너리를 받는다.\n",
    "def enc_processing(value, dictionary):\n",
    "    # 인덱스 값들을 가지고 있는 \n",
    "    # 배열이다.(누적된다.)\n",
    "    sequences_input_index = []\n",
    "    # 하나의 인코딩 되는 문장의 \n",
    "    # 길이를 가지고 있다.(누적된다.)\n",
    "    sequences_length = []\n",
    "    # 형태소 토크나이징 사용 유무\n",
    "    if DEFINES.tokenize_as_morph:\n",
    "        value = prepro_like_morphlized(value)\n",
    "\n",
    "    # 한줄씩 불어온다.\n",
    "    for sequence in value:\n",
    "        # FILTERS = \"([~.,!?\\\"':;)(])\"\n",
    "        # 정규화를 사용하여 필터에 들어 있는 \n",
    "        # 값들을 \"\" 으로 치환 한다.\n",
    "        sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n",
    "        # 하나의 문장을 인코딩 할때 \n",
    "        # 가지고 있기 위한 배열이다.\n",
    "        sequence_index = []\n",
    "        # 문장을 스페이스 단위로 \n",
    "        # 자르고 있다.\n",
    "        for word in sequence.split():\n",
    "            # 잘려진 단어들이 딕셔너리에 존재 하는지 보고 \n",
    "            # 그 값을 가져와 sequence_index에 추가한다.\n",
    "            if dictionary.get(word) is not None:\n",
    "                sequence_index.extend([dictionary[word]])\n",
    "            # 잘려진 단어가 딕셔너리에 존재 하지 않는 \n",
    "            # 경우 이므로 UNK(2)를 넣어 준다.\n",
    "            else:\n",
    "                sequence_index.extend([dictionary[UNK]])\n",
    "        # 문장 제한 길이보다 길어질 경우 뒤에 토큰을 자르고 있다.\n",
    "        if len(sequence_index) > DEFINES.max_sequence_length:\n",
    "            sequence_index = sequence_index[:DEFINES.max_sequence_length]\n",
    "        # 하나의 문장에 길이를 넣어주고 있다.\n",
    "        sequences_length.append(len(sequence_index))\n",
    "        # max_sequence_length보다 문장 길이가 \n",
    "        # 작다면 빈 부분에 PAD(0)를 넣어준다.\n",
    "        sequence_index += (DEFINES.max_sequence_length - len(sequence_index)) * [dictionary[PAD]]\n",
    "        # 뒤로 넣어 준다. \n",
    "        sequence_index.reverse()\n",
    "        # 인덱스화 되어 있는 값을 \n",
    "        # sequences_input_index에 넣어 준다.\n",
    "        sequences_input_index.append(sequence_index)\n",
    "    # 인덱스화된 일반 배열을 넘파이 배열로 변경한다. \n",
    "    # 이유는 텐서플로우 dataset에 넣어 주기 위한 \n",
    "    # 사전 작업이다.\n",
    "    # 넘파이 배열에 인덱스화된 배열과 \n",
    "    # 그 길이를 넘겨준다.  \n",
    "    return np.asarray(sequences_input_index), sequences_length\n",
    "\n",
    "\n",
    "\n",
    "###############################################################\n",
    "\n",
    "# 디코딩 입력 데이터를 만드는 함수이다.\n",
    "def dec_input_processing(value, dictionary):\n",
    "    sequences_output_index = []\n",
    "    sequences_length = []\n",
    "\n",
    "    if DEFINES.tokenize_as_morph:\n",
    "        value = prepro_like_morphlized(value)\n",
    "\n",
    "    for sequence in value:\n",
    "        sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n",
    "        sequence_index = []\n",
    "        # 디코딩 입력의 처음에는 START가 와야 하므로 \n",
    "        # 그 값을 넣어 주고 시작한다.\n",
    "        sequence_index = [dictionary[STD]] + [dictionary[word] for word in sequence.split()]\n",
    "\n",
    "        if len(sequence_index) > DEFINES.max_sequence_length:\n",
    "            sequence_index = sequence_index[:DEFINES.max_sequence_length]\n",
    "        sequences_length.append(len(sequence_index))\n",
    "        sequence_index += (DEFINES.max_sequence_length - len(sequence_index)) * [dictionary[PAD]]\n",
    "        sequences_output_index.append(sequence_index)\n",
    "\n",
    "    return np.asarray(sequences_output_index), sequences_length\n",
    "\n",
    "#######################################################################\n",
    "\n",
    "\n",
    "\n",
    "# 인덱스화 할 value와 키가 워드 이고\n",
    "# 값이 인덱스인 딕셔너리를 받는다.\n",
    "\n",
    "\n",
    "# 디코딩 출력 데이터를 만드는 함수이다.\n",
    "def dec_target_processing(value, dictionary):\n",
    "    sequences_target_index = []\n",
    "\n",
    "    if DEFINES.tokenize_as_morph:\n",
    "        value = prepro_like_morphlized(value)\n",
    "    for sequence in value:\n",
    "        sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n",
    "        # 문장에서 스페이스 단위별로 단어를 가져와서 \n",
    "        # 딕셔너리의 값인 인덱스를 넣어 준다.\n",
    "        # 디코딩 출력의 마지막에 END를 넣어 준다.\n",
    "        sequence_index = [dictionary[word] for word in sequence.split()]\n",
    "        # 문장 제한 길이보다 길어질 경우 뒤에 토큰을 자르고 있다.\n",
    "        # 그리고 END 토큰을 넣어 준다\n",
    "        if len(sequence_index) >= DEFINES.max_sequence_length:\n",
    "            sequence_index = sequence_index[:DEFINES.max_sequence_length-1] + [dictionary[END]]\n",
    "        else:\n",
    "            sequence_index += [dictionary[END]]\n",
    "        # max_sequence_length보다 문장 길이가 \n",
    "\n",
    "        sequence_index += (DEFINES.max_sequence_length - len(sequence_index)) * [dictionary[PAD]]\n",
    "        sequences_target_index.append(sequence_index)\n",
    "\n",
    "    return np.asarray(sequences_target_index)\n",
    "\n",
    "\n",
    "#########################################################\n",
    "\n",
    "\n",
    "\n",
    "def dec_target_processing_two(value, dictionary):\n",
    "    # 인덱스 값들을 가지고 있는 \n",
    "    # 배열이다.(누적된다)\n",
    "    sequences_target_index = []\n",
    "    sequences_length = []\n",
    "    # 형태소 토크나이징 사용 유무\n",
    "    if DEFINES.tokenize_as_morph:\n",
    "        value = prepro_like_morphlized(value)\n",
    "    # 한줄씩 불어온다.\n",
    "    for sequence in value:\n",
    "        # FILTERS = \"([~.,!?\\\"':;)(])\"\n",
    "        # 정규화를 사용하여 필터에 들어 있는 \n",
    "        # 값들을 \"\" 으로 치환 한다.\n",
    "        sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n",
    "        # 문장에서 스페이스 단위별로 단어를 가져와서 \n",
    "        # 딕셔너리의 값인 인덱스를 넣어 준다.\n",
    "        # 디코딩 출력의 마지막에 END를 넣어 준다.\n",
    "        sequence_index = [dictionary[word] for word in sequence.split()]\n",
    "        # 문장 제한 길이보다 길어질 경우 뒤에 토큰을 자르고 있다.\n",
    "        # 그리고 END 토큰을 넣어 준다\n",
    "        if len(sequence_index) >= DEFINES.max_sequence_length:\n",
    "            sequence_index = sequence_index[:DEFINES.max_sequence_length-1] + [dictionary[END]]\n",
    "        else:\n",
    "            sequence_index += [dictionary[END]]\n",
    "        \n",
    "        # 학습시 PAD 마스크를 위한 벡터를 구성한다.                   \n",
    "        sequences_length.append([PAD_MASK if num > len(sequence_index) else NON_PAD_MASK for num in range (DEFINES.max_sequence_length)])\n",
    "        # max_sequence_length보다 문장 길이가 \n",
    "        # 작다면 빈 부분에 PAD(0)를 넣어준다.\n",
    "        sequence_index += (DEFINES.max_sequence_length - len(sequence_index)) * [dictionary[PAD]]\n",
    "        # 인덱스화 되어 있는 값을 \n",
    "        # sequences_target_index에 넣어 준다.\n",
    "        sequences_target_index.append(sequence_index)\n",
    "    # 인덱스화된 일반 배열을 넘파이 배열로 변경한다. \n",
    "    # 이유는 텐서플로우 dataset에 넣어 주기 위한 사전 작업이다.\n",
    "    # 넘파이 배열에 인덱스화된 배열과 그 길이를 넘겨준다.\n",
    "    return np.asarray(sequences_target_index), np.asarray(sequences_length)\n",
    "\n",
    "##############################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 7921/7921 [00:23<00:00, 342.96it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 7921/7921 [00:21<00:00, 367.63it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 7921/7921 [00:21<00:00, 375.82it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 3902/3902 [00:07<00:00, 503.79it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 3902/3902 [00:09<00:00, 398.76it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 3902/3902 [00:09<00:00, 393.08it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#TODO3: 실행 안됨, 실행 후 지속적으로 리뷰하겠음\n",
    "\n",
    "# 훈련셋 인코딩 만드는 부분이다.\n",
    "train_input_enc, train_input_enc_length = enc_processing(train_input, char2idx)\n",
    "\n",
    "# 훈련셋 디코딩 입력 부분 만드는 부분이다.\n",
    "train_output_dec, train_output_dec_length = dec_input_processing(train_label, char2idx) \n",
    "#TODO1 실행 안되어 확인 필요(AttributeError: module 'data' has no attribute 'dec_output_processing)\n",
    "\n",
    "# 훈련셋 디코딩 출력 부분 만드는 부분이다.\n",
    "train_target_dec = dec_target_processing(train_label, char2idx)\n",
    "\n",
    "\n",
    "\n",
    "############################################################\n",
    "\n",
    "\n",
    "# 평가셋 인코딩 만드는 부분이다.\n",
    "eval_input_enc, eval_input_enc_length = enc_processing(eval_input,char2idx)\n",
    "\n",
    "# 평가셋 인코딩 만드는 부분이다.\n",
    "eval_output_dec, eval_output_dec_length = dec_input_processing(eval_label, char2idx)\n",
    "\n",
    "# 평가셋 인코딩 만드는 부분이다.\n",
    "eval_target_dec = dec_target_processing(eval_label, char2idx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 체크 포인트 경로 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 현재 경로'./'에 현재 경로 하부에 \n",
    "# 체크 포인트를 저장한 디렉토리를 설정한다.\n",
    "\n",
    "\n",
    "\n",
    "check_point_path = os.path.join(os.getcwd(), DEFINES.check_point_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 디렉토리를 만드는 함수이며 두번째 인자 exist_ok가 \n",
    "# True이면 디렉토리가 이미 존재해도 OSError가 \n",
    "# 발생하지 않는다.\n",
    "# exist_ok가 False이면 이미 존재하면 \n",
    "# OSError가 발생한다.\n",
    "\n",
    "\n",
    "os.makedirs(check_point_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM 네트워크와 에스티메이터 모델 구성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 클래시파이어  classifier = tf.estimator.Estimator("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 2 22222222222222222222222222\n",
    "\n",
    "#-*- coding: utf-8 -*-\n",
    "#import tensorflow as tf\n",
    "#import sys\n",
    "#from configs import DEFINES\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 엘에스티엠(LSTM) 단층 네트워크 구성하는 부분\n",
    "def make_lstm_cell(mode, hiddenSize, index):\n",
    "    cell = tf.nn.rnn_cell.BasicLSTMCell(hiddenSize, name = \"lstm\"+str(index))\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=DEFINES.dropout_width)\n",
    "        # tensor 2.0  tf.contrib.rnn.\n",
    "        #############################\n",
    "    return cell\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 에스티메이터 모델 부분이다.\n",
    "def mlmodel(features, labels, mode, params):\n",
    "    TRAIN = mode == tf.estimator.ModeKeys.TRAIN\n",
    "    EVAL = mode == tf.estimator.ModeKeys.EVAL\n",
    "    PREDICT = mode == tf.estimator.ModeKeys.PREDICT\n",
    "    \n",
    "    \n",
    "    # 인코딩 부분 (미리 정의된 임베딩 벡터 사용 유무)\n",
    "    if params['embedding'] == True:\n",
    "        # 가중치 행렬에 대한 초기화 함수이다.\n",
    "        # xavier (Xavier Glorot와 Yoshua Bengio (2010)\n",
    "        # URL : http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf\n",
    "        #initializer = tf.contrib.layers.xavier_initializer()\n",
    "        initializer = tf.glorot_uniform_initializer()\n",
    "        \n",
    "        embedding = tf.get_variable(name = \"embedding\", # 이름\n",
    "                                 \t  shape=[params['vocabulary_length'], params['embedding_size']], #  모양\n",
    "                                 \t  dtype=tf.float32, # 타입\n",
    "                                 \t  initializer=initializer, # 초기화 값\n",
    "                                 \t  trainable=True) # 학습 유무\n",
    "    else:   \n",
    "        # tf.eye를 통해서 사전의 크기 만큼의 단위행렬 \n",
    "        # 구조를 만든다.\n",
    "        embedding = tf.eye(num_rows = params['vocabulary_length'], dtype = tf.float32)\n",
    "        embedding = tf.get_variable(name = \"embedding\", # 이름\n",
    "                                            initializer = embedding, # 초기화 값\n",
    "                                            trainable = False) # 학습 유무\n",
    "\n",
    "    # 임베딩된 인코딩 배치를 만든다.\n",
    "    embedding_encoder = tf.nn.embedding_lookup(params = embedding, ids = features['input'])\n",
    "\n",
    "    # 임베딩된 디코딩 배치를 만든다.\n",
    "    embedding_decoder = tf.nn.embedding_lookup(params = embedding, ids = features['output'])\n",
    "\n",
    "    with tf.variable_scope('encoder_scope', reuse=tf.AUTO_REUSE):\n",
    "        # 값이 True이면 멀티레이어로 모델을 구성하고 False이면 \n",
    "        # 단일레이어로 모델을 구성 한다.\n",
    "        if params['multilayer'] == True:\n",
    "            encoder_cell_list = [make_lstm_cell(mode, params['hidden_size'], i) for i in range(params['layer_size'])]\n",
    "            #rnn_cell = tf.contrib.rnn.MultiRNNCell(encoder_cell_list)\n",
    "            rnn_cell = tf.nn.rnn_cell.MultiRNNCell(encoder_cell_list)\n",
    "        else:\n",
    "            rnn_cell = make_lstm_cell(mode, params['hidden_size'], \"\")\n",
    "        \n",
    "        # rnn_cell에 의해 지정된 dynamic_rnn 반복적인 신경망을 만든다. \n",
    "        # encoder_states 최종 상태  [batch_size, cell.state_size]\n",
    "        encoder_outputs, encoder_states = tf.nn.dynamic_rnn(cell=rnn_cell, # RNN 셀\n",
    "                                                                inputs=embedding_encoder, # 입력 값\n",
    "                                                                dtype=tf.float32) # 타입\n",
    "\n",
    "    with tf.variable_scope('decoder_scope', reuse=tf.AUTO_REUSE):\n",
    "        if params['multilayer'] == True:\n",
    "            decoder_cell_list = [make_lstm_cell(mode, params['hidden_size'], i) for i in range(params['layer_size'])]\n",
    "            #rnn_cell = tf.contrib.rnn.MultiRNNCell(decoder_cell_list)\n",
    "            rnn_cell = tf.nn.rnn_cell.MultiRNNCell(encoder_cell_list)\n",
    "        else:\n",
    "            rnn_cell = make_lstm_cell(mode, params['hidden_size'], \"\")\n",
    "\n",
    "        decoder_initial_state = encoder_states\n",
    "        decoder_outputs, decoder_states = tf.nn.dynamic_rnn(cell=rnn_cell, # RNN 셀\n",
    "                       inputs=embedding_decoder, # 입력 값\n",
    "                       initial_state=decoder_initial_state, # 인코딩의 마지막 값으로 초기화\n",
    "                       dtype=tf.float32) # 타입\n",
    "\n",
    "\n",
    "    # logits는 마지막 히든레이어를 통과한 결과값이다.\n",
    "    logits = tf.layers.dense(decoder_outputs, params['vocabulary_length'], activation=None)\n",
    "\n",
    "\t# argmax를 통해서 최대 값을 가져 온다.\n",
    "    predict = tf.argmax(logits, 2)\n",
    "\n",
    "    if PREDICT:\n",
    "        predictions = { # 예측 값들이 여기에 딕셔너리 형태로 담긴다.\n",
    "            'indexs': predict, # 시퀀스 마다 예측한 값\n",
    "        }\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "    \n",
    "    #  \n",
    "    # logits과 같은 차원을 만들어 마지막 결과 값과 정답 값을 비교하여 에러를 구한다.\n",
    "    labels_ = tf.one_hot(labels, params['vocabulary_length'])\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=labels_))\n",
    "    # 라벨과 결과가 일치하는지 빈도 계산을 통해 정확도를 측정하는 방법이다.\n",
    "    accuracy = tf.metrics.accuracy(labels=labels, predictions=predict,name='accOp')\n",
    "\n",
    "    # accuracy를 전체 값으로 나눠 확률 값으로 한다.\n",
    "    metrics = {'accuracy': accuracy}\n",
    "    tf.summary.scalar('accuracy', accuracy[1])\n",
    "    \n",
    "    if EVAL:\n",
    "        # 에러 값(loss)과 정확도 값(eval_metric_ops) 전달\n",
    "        return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=metrics)\n",
    "\n",
    "    # 수행 mode(tf.estimator.ModeKeys.TRAIN)가 \n",
    "    # 아닌 경우는 여기 까지 오면 안되도록 방어적 코드를 넣은것이다.\n",
    "    assert TRAIN\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=DEFINES.learning_rate)\n",
    "    train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())  \n",
    "\n",
    "    # 에러 값(loss)과 그라디언트 반환값 (train_op) 전달\n",
    "    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': './data_outseq2/check_point', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000021E72EC81D0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "# 에스티메이터 구성한다.\n",
    "\n",
    "# TODO2: 왜 분류모델이라고 적혀있나요???? s2s이 아닌가요?\n",
    "\n",
    "classifier = tf.estimator.Estimator(\n",
    "    \n",
    "        model_fn = mlmodel, # 모델 등록한다.\n",
    "    \n",
    "        model_dir=DEFINES.check_point_path, # 체크포인트 위치 등록한다.\n",
    "    \n",
    "        params={ # 모델 쪽으로 파라메터 전달한다.\n",
    "            'hidden_size': DEFINES.hidden_size, # 가중치 크기 설정한다.\n",
    "            'layer_size': DEFINES.layer_size, # 멀티 레이어 층 개수를 설정한다.\n",
    "            'learning_rate': DEFINES.learning_rate, # 학습율 설정한다. \n",
    "            'vocabulary_length': vocabulary_length, # 딕셔너리 크기를 설정한다.\n",
    "            'embedding_size': DEFINES.embedding_size, # 임베딩 크기를 설정한다.\n",
    "            'embedding': DEFINES.embedding, # 임베딩 사용 유무를 설정한다.\n",
    "            'multilayer': DEFINES.multilayer, # 멀티 레이어 사용 유무를 설정한다.\n",
    "            \n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 클래시파이어 만들기와 학습 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def rearrange(input, output, target):\n",
    "    features = {\"input\": input, \"output\": output}\n",
    "    return features, target\n",
    "\n",
    "\n",
    "\n",
    "# 학습에 들어가 배치 데이터를 만드는 함수이다.\n",
    "def train_input_fn(train_input_enc, train_target_dec_length, train_target_dec, batch_size):\n",
    "    # Dataset을 생성하는 부분으로써 from_tensor_slices부분은 \n",
    "    # 각각 한 문장으로 자른다고 보면 된다.\n",
    "    # train_input_enc, train_target_dec_length, train_target_dec \n",
    "    # 3개를 각각 한문장으로 나눈다.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((train_input_enc, train_target_dec_length, train_target_dec))\n",
    "    # 전체 데이터를 썩는다.\n",
    "    dataset = dataset.shuffle(buffer_size=len(train_input_enc))\n",
    "    # 배치 인자 값이 없다면  에러를 발생 시킨다.\n",
    "    assert batch_size is not None, \"train batchSize must not be None\"\n",
    "    # from_tensor_slices를 통해 나눈것을 \n",
    "    # 배치크기 만큼 묶어 준다.\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    # 데이터 각 요소에 대해서 train_rearrange 함수를 \n",
    "    # 통해서 요소를 변환하여 맵으로 구성한다.\n",
    "    \n",
    "    dataset = dataset.map(rearrange)\n",
    "    #######################################\n",
    "    # train_rearrange 에서 그냥 rearrange 로 \n",
    "    \n",
    "    # repeat()함수에 원하는 에포크 수를 넣을수 있으면 \n",
    "    # 아무 인자도 없다면 무한으로 이터레이터 된다.\n",
    "    dataset = dataset.repeat()\n",
    "    # make_one_shot_iterator를 통해 이터레이터를 \n",
    "    # 만들어 준다.\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    # 이터레이터를 통해 다음 항목의 텐서 \n",
    "    # 개체를 넘겨준다.\n",
    "    return iterator.get_next()\n",
    "\n",
    "\n",
    "# 평가에 들어가 배치 데이터를 만드는 함수이다.\n",
    "def dataeval_input_fn(eval_input_enc, eval_target_dec, batch_size):\n",
    "    # Dataset을 생성하는 부분으로써 from_tensor_slices부분은 \n",
    "    # 각각 한 문장으로 자른다고 보면 된다.\n",
    "    # eval_input_enc, eval_target_dec, batch_size \n",
    "    # 3개를 각각 한문장으로 나눈다.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((eval_input_enc, eval_target_dec))\n",
    "    # 전체 데이터를 섞는다.\n",
    "    dataset = dataset.shuffle(buffer_size=len(eval_input_enc))\n",
    "    # 배치 인자 값이 없다면  에러를 발생 시킨다.\n",
    "    assert batch_size is not None, \"eval batchSize must not be None\"\n",
    "    # from_tensor_slices를 통해 나눈것을 \n",
    "    # 배치크기 만큼 묶어 준다.\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    # 데이터 각 요소에 대해서 rearrange 함수를 \n",
    "    # 통해서 요소를 변환하여 맵으로 구성한다.\n",
    "    dataset = dataset.map(rearrange)\n",
    "    #################################\n",
    "    \n",
    "    # repeat()함수에 원하는 에포크 수를 넣을수 있으면 \n",
    "    # 아무 인자도 없다면 무한으로 이터레이터 된다.\n",
    "    # 평가이므로 1회만 동작 시킨다.\n",
    "    dataset = dataset.repeat(1)\n",
    "    # make_one_shot_iterator를 통해 \n",
    "    # 이터레이터를 만들어 준다.\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    # 이터레이터를 통해 다음 항목의 \n",
    "    # 텐서 개체를 넘겨준다.\n",
    "    return iterator.get_next()\n",
    "\n",
    "\n",
    "\n",
    "#######################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\WinPython37F\\python-3.7.2.amd64\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From C:\\WinPython37F\\python-3.7.2.amd64\\lib\\site-packages\\tensorflow_core\\python\\training\\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "WARNING:tensorflow:From <ipython-input-13-b16dfc159ac5>:33: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:From <ipython-input-11-4f032418809e>:13: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-11-4f032418809e>:64: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-11-4f032418809e>:72: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From C:\\WinPython37F\\python-3.7.2.amd64\\lib\\site-packages\\tensorflow_core\\python\\ops\\rnn_cell_impl.py:735: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From C:\\WinPython37F\\python-3.7.2.amd64\\lib\\site-packages\\tensorflow_core\\python\\ops\\rnn_cell_impl.py:739: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From <ipython-input-11-4f032418809e>:90: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From C:\\WinPython37F\\python-3.7.2.amd64\\lib\\site-packages\\tensorflow_core\\python\\layers\\core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./data_outseq2/check_point\\model.ckpt-21904\n",
      "WARNING:tensorflow:From C:\\WinPython37F\\python-3.7.2.amd64\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py:1069: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file utilities to get mtimes.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 21904 into ./data_outseq2/check_point\\model.ckpt.\n",
      "INFO:tensorflow:loss = 0.45223314, step = 21904\n",
      "INFO:tensorflow:global_step/sec: 1.08541\n",
      "INFO:tensorflow:loss = 0.5523502, step = 22004 (92.133 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.889942\n",
      "INFO:tensorflow:loss = 0.52124745, step = 22104 (112.383 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.881234\n",
      "INFO:tensorflow:loss = 0.47083762, step = 22204 (113.461 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.870336\n",
      "INFO:tensorflow:loss = 0.44187862, step = 22304 (114.912 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.806159\n",
      "INFO:tensorflow:loss = 0.4488974, step = 22404 (124.048 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 22438 into ./data_outseq2/check_point\\model.ckpt.\n",
      "WARNING:tensorflow:From C:\\WinPython37F\\python-3.7.2.amd64\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "INFO:tensorflow:global_step/sec: 0.765989\n",
      "INFO:tensorflow:loss = 0.4977456, step = 22504 (130.549 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.747085\n",
      "INFO:tensorflow:loss = 0.4343311, step = 22604 (133.854 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.745948\n",
      "INFO:tensorflow:loss = 0.518637, step = 22704 (134.057 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.719057\n",
      "INFO:tensorflow:loss = 0.47262642, step = 22804 (139.072 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 22880 into ./data_outseq2/check_point\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.69807\n",
      "INFO:tensorflow:loss = 0.51194936, step = 22904 (143.241 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.670207\n",
      "INFO:tensorflow:loss = 0.46665937, step = 23004 (149.207 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.682238\n",
      "INFO:tensorflow:loss = 0.5301347, step = 23104 (146.574 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.706294\n",
      "INFO:tensorflow:loss = 0.4891078, step = 23204 (141.596 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 23296 into ./data_outseq2/check_point\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.715992\n",
      "INFO:tensorflow:loss = 0.47111374, step = 23304 (139.653 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.746157\n",
      "INFO:tensorflow:loss = 0.48302445, step = 23404 (134.036 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.76232\n",
      "INFO:tensorflow:loss = 0.57751197, step = 23504 (131.178 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.721894\n",
      "INFO:tensorflow:loss = 0.4624485, step = 23604 (138.529 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.780528\n",
      "INFO:tensorflow:loss = 0.48580897, step = 23704 (128.114 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 23748 into ./data_outseq2/check_point\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.777903\n",
      "INFO:tensorflow:loss = 0.5019127, step = 23804 (128.550 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.775868\n",
      "INFO:tensorflow:loss = 0.45642576, step = 23904 (128.889 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.750482\n",
      "INFO:tensorflow:loss = 0.52839434, step = 24004 (133.235 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.768306\n",
      "INFO:tensorflow:loss = 0.48965895, step = 24104 (130.168 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.774113\n",
      "INFO:tensorflow:loss = 0.50785995, step = 24204 (129.180 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 24209 into ./data_outseq2/check_point\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.765916\n",
      "INFO:tensorflow:loss = 0.487687, step = 24304 (130.562 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.781954\n",
      "INFO:tensorflow:loss = 0.46152183, step = 24404 (127.870 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.777807\n",
      "INFO:tensorflow:loss = 0.51131326, step = 24504 (128.582 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.783246\n",
      "INFO:tensorflow:loss = 0.4560382, step = 24604 (127.674 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 24676 into ./data_outseq2/check_point\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.752965\n",
      "INFO:tensorflow:loss = 0.47449294, step = 24704 (132.796 sec)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-e1942850288d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m classifier.train(input_fn=lambda:  train_input_fn(\n\u001b[1;32m----> 8\u001b[1;33m     train_input_enc, train_output_dec, train_target_dec,  DEFINES.batch_size), steps=25000 ) #steps=DEFINES.train_steps)\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\WinPython37F\\python-3.7.2.amd64\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    369\u001b[0m       \u001b[0msaving_listeners\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 370\u001b[1;33m       \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    371\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Loss for final step: %s.'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    372\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\WinPython37F\\python-3.7.2.amd64\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[1;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[0;32m   1158\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_model_distributed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1159\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1160\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_model_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1162\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_train_model_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\WinPython37F\\python-3.7.2.amd64\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\u001b[0m in \u001b[0;36m_train_model_default\u001b[1;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[0;32m   1192\u001b[0m       return self._train_with_estimator_spec(estimator_spec, worker_hooks,\n\u001b[0;32m   1193\u001b[0m                                              \u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step_tensor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1194\u001b[1;33m                                              saving_listeners)\n\u001b[0m\u001b[0;32m   1195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_train_model_distributed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\WinPython37F\\python-3.7.2.amd64\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\u001b[0m in \u001b[0;36m_train_with_estimator_spec\u001b[1;34m(self, estimator_spec, worker_hooks, hooks, global_step_tensor, saving_listeners)\u001b[0m\n\u001b[0;32m   1491\u001b[0m       \u001b[0many_step_done\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1492\u001b[0m       \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_stop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1493\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mestimator_spec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mestimator_spec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1494\u001b[0m         \u001b[0many_step_done\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1495\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0many_step_done\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\WinPython37F\\python-3.7.2.amd64\\lib\\site-packages\\tensorflow_core\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    752\u001b[0m         \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m         \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 754\u001b[1;33m         run_metadata=run_metadata)\n\u001b[0m\u001b[0;32m    755\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\WinPython37F\\python-3.7.2.amd64\\lib\\site-packages\\tensorflow_core\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1257\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1258\u001b[0m             \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1259\u001b[1;33m             run_metadata=run_metadata)\n\u001b[0m\u001b[0;32m   1260\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1261\u001b[0m         logging.info(\n",
      "\u001b[1;32mC:\\WinPython37F\\python-3.7.2.amd64\\lib\\site-packages\\tensorflow_core\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1343\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1344\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1345\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1346\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1347\u001b[0m       \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\WinPython37F\\python-3.7.2.amd64\\lib\\site-packages\\tensorflow_core\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1416\u001b[0m         \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m         \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1418\u001b[1;33m         run_metadata=run_metadata)\n\u001b[0m\u001b[0;32m   1419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1420\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_hooks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\WinPython37F\\python-3.7.2.amd64\\lib\\site-packages\\tensorflow_core\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1175\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1176\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1178\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_with_hooks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\WinPython37F\\python-3.7.2.amd64\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    954\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 956\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    957\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\WinPython37F\\python-3.7.2.amd64\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1180\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1181\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\WinPython37F\\python-3.7.2.amd64\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1357\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1359\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1360\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\WinPython37F\\python-3.7.2.amd64\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1363\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1365\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1366\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\WinPython37F\\python-3.7.2.amd64\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[1;32m-> 1350\u001b[1;33m                                       target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\WinPython37F\\python-3.7.2.amd64\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[0;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1443\u001b[1;33m                                             run_metadata)\n\u001b[0m\u001b[0;32m   1444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1445\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 학습 실행\n",
    "################################################\n",
    "\n",
    "# tf.app.flags.DEFINE_integer('train_steps', 25000, 'train steps') # 학습 에포크\n",
    "\n",
    "\n",
    "classifier.train(input_fn=lambda:  train_input_fn(\n",
    "    train_input_enc, train_output_dec, train_target_dec,  DEFINES.batch_size), steps=DEFINES.train_steps)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이미 학습된 곳에서 다시 시작하는 모습\n",
    "\n",
    "\n",
    "    INFO:tensorflow:Calling model_fn.\n",
    "    INFO:tensorflow:Done calling model_fn.\n",
    "    INFO:tensorflow:Create CheckpointSaverHook.\n",
    "    INFO:tensorflow:Graph was finalized.\n",
    "    INFO:tensorflow:Restoring parameters from ./data_outseq2/check_point\\model.ckpt-3615\n",
    "    \n",
    "    WARNING:tensorflow:From C:\\WinPython37F\\python-3.7.2.amd64\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py:1069: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
    "    Instructions for updating:\n",
    "    Use standard file utilities to get mtimes.\n",
    "    INFO:tensorflow:Running local_init_op.\n",
    "    INFO:tensorflow:Done running local_init_op.\n",
    "    INFO:tensorflow:Saving checkpoints for 3615 into ./data_outseq2/check_point\\model.ckpt.\n",
    "    INFO:tensorflow:loss = 6.9968653, step = 3615\n",
    "    INFO:tensorflow:global_step/sec: 0.578642\n",
    "    INFO:tensorflow:loss = 1.5761786, step = 3715 (172.833 sec)\n",
    "    INFO:tensorflow:global_step/sec: 0.586996\n",
    "    INFO:tensorflow:loss = 1.469312, step = 3815 (170.349 sec)\n",
    "    \n",
    "    ...\n",
    "    ...\n",
    "    \n",
    "    INFO:tensorflow:loss = 0.47262642, step = 22804 (139.072 sec)\n",
    "    INFO:tensorflow:Saving checkpoints for 22880 into ./data_outseq2/check_point\\model.ckpt.\n",
    "    INFO:tensorflow:global_step/sec: 0.69807\n",
    "    INFO:tensorflow:loss = 0.51194936, step = 22904 (143.241 sec)\n",
    "    INFO:tensorflow:global_step/sec: 0.670207\n",
    "    INFO:tensorflow:loss = 0.46665937, step = 23004 (149.207 sec)\n",
    "    INFO:tensorflow:global_step/sec: 0.682238\n",
    "    INFO:tensorflow:loss = 0.5301347, step = 23104 (146.574 sec)\n",
    "    INFO:tensorflow:loss = 0.4891078, step = 23204 (141.596 sec)\n",
    "    INFO:tensorflow:Saving checkpoints for 23296 into ./data_outseq2/check_point\\model.ckpt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 평가 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2020-03-02T13:13:09Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./data_outseq2/check_point\\model.ckpt-24676\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2020-03-02-13:13:51\n",
      "INFO:tensorflow:Saving dict for global step 24676: accuracy = 0.81581753, global_step = 24676, loss = 2.1726224\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 24676: ./data_outseq2/check_point\\model.ckpt-24676\n",
      "\n",
      "EVAL set accuracy: 0.816\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def rearrange(input, output, target):\n",
    "    features = {\"input\": input, \"output\": output}\n",
    "    return features, target\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 평가에 들어가 배치 데이터를 만드는 함수이다.\n",
    "def eval_input_fn(eval_input_enc, eval_output_dec, eval_target_dec, batch_size):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((eval_input_enc, eval_output_dec, eval_target_dec))\n",
    "    # 전체 데이터를 섞는다.\n",
    "    dataset = dataset.shuffle(buffer_size=len(eval_input_enc))\n",
    "    assert batch_size is not None, \"eval batchSize must not be None\"\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(rearrange)\n",
    "    #################################\n",
    "    \n",
    "    # 평가이므로 1회만 동작 시킨다.\n",
    "    dataset = dataset.repeat(1)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    return iterator.get_next()\n",
    "\n",
    "\n",
    "\n",
    "eval_result = classifier.evaluate(input_fn=lambda: eval_input_fn(\n",
    "    eval_input_enc, eval_output_dec, eval_target_dec,  DEFINES.batch_size))\n",
    "\n",
    "print('\\nEVAL set accuracy: {accuracy:0.3f}\\n'.format(**eval_result))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    INFO:tensorflow:Calling model_fn.\n",
    "    INFO:tensorflow:Done calling model_fn.\n",
    "    INFO:tensorflow:Starting evaluation at 2020-03-02T13:13:09Z\n",
    "    INFO:tensorflow:Graph was finalized.\n",
    "    INFO:tensorflow:Restoring parameters from ./data_outseq2/check_point\\model.ckpt-24676\n",
    "    INFO:tensorflow:Running local_init_op.\n",
    "    INFO:tensorflow:Done running local_init_op.\n",
    "    INFO:tensorflow:Finished evaluation at 2020-03-02-13:13:51\n",
    "    INFO:tensorflow:Saving dict for global step 24676: accuracy = 0.81581753, global_step = 24676, loss = 2.1726224\n",
    "    INFO:tensorflow:Saving 'checkpoint_path' summary for global step 24676: ./data_outseq2/check_point\\model.ckpt-24676\n",
    "\n",
    "    EVAL set accuracy: 0.816"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 테스트 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코딩 데이터를 만드는 함수이며 \n",
    "# 인덱스화 할 value와 키가 단어이고 값이 인덱스인 딕셔너리를 받아\n",
    "# 넘파이 배열에 인덱스화된 배열과 그 길이를 넘겨준다.  \n",
    "\n",
    "\n",
    "def enc_processing(value, dictionary):\n",
    "    sequences_input_index = []\n",
    "    sequences_length = []\n",
    "    # 형태소 토크나이징 사용 유무\n",
    "    if DEFINES.tokenize_as_morph:\n",
    "        value = prepro_like_morphlized(value)\n",
    "\n",
    "    for sequence in value:\n",
    "        sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n",
    "        sequence_index = []\n",
    "        # 문장을 스페이스 단위로 자르고 있다.\n",
    "        for word in sequence.split():\n",
    "            # 잘려진 단어들이 딕셔너리에 존재 하는지 보고 \n",
    "            # 그 값을 가져와 sequence_index에 추가한다.\n",
    "            if dictionary.get(word) is not None:\n",
    "                sequence_index.extend([dictionary[word]])\n",
    "            # 잘려진 단어가 딕셔너리에 존재 하지 않는 \n",
    "            # 경우 이므로 UNK(2)를 넣어 준다.\n",
    "            else:\n",
    "                sequence_index.extend([dictionary[UNK]])\n",
    "        \n",
    "        # 문장 제한 길이보다 길어질 경우 뒤에 토큰을 자르고 있다.\n",
    "        if len(sequence_index) > DEFINES.max_sequence_length:\n",
    "            sequence_index = sequence_index[:DEFINES.max_sequence_length]\n",
    "\n",
    "        sequences_length.append(len(sequence_index))\n",
    "        # max_sequence_length보다 문장 길이가 작다면 빈 부분에 PAD(0)를 넣어준다.\n",
    "        sequence_index += (DEFINES.max_sequence_length - len(sequence_index)) * [dictionary[PAD]]\n",
    "        sequences_input_index.append(sequence_index)\n",
    "    # 인덱스화된 일반 배열을 넘파이 배열로 변경한다. \n",
    "    # 이유는 텐서플로우 dataset에 넣어 주기 위한 사전 작업이다.\n",
    "    return np.asarray(sequences_input_index), sequences_length\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 디코딩 입력 데이터를 만드는 함수이다.\n",
    "def dec_input_processing(value, dictionary):\n",
    "    sequences_output_index = []\n",
    "    sequences_length = []\n",
    "\n",
    "    if DEFINES.tokenize_as_morph:\n",
    "        value = prepro_like_morphlized(value)\n",
    "\n",
    "    for sequence in value:\n",
    "        sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n",
    "        sequence_index = []\n",
    "        # 디코딩 입력의 처음에는 START가 와야 하므로 \n",
    "        # 그 값을 넣어 주고 시작한다.\n",
    "        sequence_index = [dictionary[STD]] + [dictionary[word] for word in sequence.split()]\n",
    "\n",
    "        if len(sequence_index) > DEFINES.max_sequence_length:\n",
    "            sequence_index = sequence_index[:DEFINES.max_sequence_length]\n",
    "        sequences_length.append(len(sequence_index))\n",
    "        sequence_index += (DEFINES.max_sequence_length - len(sequence_index)) * [dictionary[PAD]]\n",
    "        sequences_output_index.append(sequence_index)\n",
    "\n",
    "    return np.asarray(sequences_output_index), sequences_length\n",
    "\n",
    "\n",
    "\n",
    "# 디코딩 출력 데이터를 만드는 함수이다.\n",
    "def dec_target_processing(value, dictionary):\n",
    "    sequences_target_index = []\n",
    "\n",
    "    if DEFINES.tokenize_as_morph:\n",
    "        value = prepro_like_morphlized(value)\n",
    "    for sequence in value:\n",
    "        sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n",
    "        # 문장에서 스페이스 단위별로 단어를 가져와서 \n",
    "        # 딕셔너리의 값인 인덱스를 넣어 준다.\n",
    "        # 디코딩 출력의 마지막에 END를 넣어 준다.\n",
    "        sequence_index = [dictionary[word] for word in sequence.split()]\n",
    "        # 문장 제한 길이보다 길어질 경우 뒤에 토큰을 자르고 있다.\n",
    "        # 그리고 END 토큰을 넣어 준다\n",
    "        if len(sequence_index) >= DEFINES.max_sequence_length:\n",
    "            sequence_index = sequence_index[:DEFINES.max_sequence_length-1] + [dictionary[END]]\n",
    "        else:\n",
    "            sequence_index += [dictionary[END]]\n",
    "        # max_sequence_length보다 문장 길이가 \n",
    "\n",
    "        sequence_index += (DEFINES.max_sequence_length - len(sequence_index)) * [dictionary[PAD]]\n",
    "        sequences_target_index.append(sequence_index)\n",
    "\n",
    "    return np.asarray(sequences_target_index)\n",
    "\n",
    "\n",
    "######################################################################################\n",
    "\n",
    "\n",
    "# 인덱스를 스트링으로 변경하는 함수이다.\n",
    "def pred2string(value, dictionary):\n",
    "    sentence_string = []\n",
    "    for v in value:\n",
    "        # 딕셔너리에 있는 단어로 변경해서 배열에 담는다.\n",
    "        sentence_string = [dictionary[index] for index in v['indexs']]\n",
    "\n",
    "    print(sentence_string)\n",
    "    answer = \"\"\n",
    "    # 패딩값과 엔드값이 담겨 있으므로 패딩은 모두 스페이스 처리 한다.\n",
    "    for word in sentence_string:\n",
    "        if word not in PAD and word not in END:\n",
    "            answer += word\n",
    "            answer += \" \"\n",
    "\n",
    "    print(answer)\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 챗봇하기 (input - output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 34.55it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 250.66it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 501.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./data_outseq2/check_point\\model.ckpt-24676\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "['사랑', '사람', '사람', '에', '의', '의', '에', '에', '에', '에', '의', '과', '과', '과', '생각', '생각', '<PADDING>', '<PADDING>', '<PADDING>', '<PADDING>', '<PADDING>', '<PADDING>', '<PADDING>', '<PADDING>', '<PADDING>']\n",
      "사랑 사람 사람 에 의 의 에 에 에 에 의 과 과 과 생각 생각 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'사랑 사람 사람 에 의 의 에 에 에 에 의 과 과 과 생각 생각 '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "input = \"가끔 궁금해\"\n",
    "\n",
    "#######################\n",
    "\n",
    "# 테스트용 데이터 만드는 부분이다.\n",
    "# 인코딩 부분 만든다.\n",
    "predic_input_enc, predic_input_enc_length = enc_processing([input], char2idx)\n",
    "\n",
    "\n",
    "# 학습 과정이 아니므로 디코딩 입력은 \n",
    "# 존재하지 않는다.(구조를 맞추기 위해 넣는다.)\n",
    "\n",
    "predic_output_dec, predic_output_decLength = dec_input_processing([\"\"], char2idx)       \n",
    "# 학습 과정이 아니므로 디코딩 출력 부분도 \n",
    "# 존재하지 않는다.(구조를 맞추기 위해 넣는다.)\n",
    "\n",
    "\n",
    "predic_target_dec = dec_target_processing([\"\"], char2idx)      \n",
    "\n",
    "# 예측을 하는 부분이다.\n",
    "\n",
    "\n",
    "predictions = classifier.predict(\n",
    "    input_fn=lambda:  eval_input_fn(predic_input_enc, predic_output_dec, predic_target_dec, DEFINES.batch_size))\n",
    "\n",
    "# 예측한 값을 인지 할 수 있도록 \n",
    "# 텍스트로 변경하는 부분이다.\n",
    "\n",
    "\n",
    "pred2string(predictions, idx2char)\n",
    "##################################\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    INFO:tensorflow:Calling model_fn.\n",
    "\n",
    "    INFO:tensorflow:Done calling model_fn.\n",
    "    INFO:tensorflow:Graph was finalized.\n",
    "    INFO:tensorflow:Restoring parameters from ./data_outseq2/check_point\\model.ckpt-24676\n",
    "    INFO:tensorflow:Running local_init_op.\n",
    "    INFO:tensorflow:Done running local_init_op.\n",
    "    ['사랑', '사람', '사람', '에', '의', '의', '에', '에', '에', '에', '의', '과', '과', '과', '생각', '생각', '<PADDING>', '<PADDING>', '<PADDING>', '<PADDING>', '<PADDING>', '<PADDING>', '<PADDING>', '<PADDING>', '<PADDING>']\n",
    "    사랑 사람 사람 에 의 의 에 에 에 에 의 과 과 과 생각 생각 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.4 :   Transformer 모델 챗봇"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
